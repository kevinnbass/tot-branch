# Full Codebase Snapshot â€” generated 2025-06-22T12:37:51
====================================================================================================

## 0001. config.yaml
----------------------------------------------------------------------------------------------------
file_paths:
  file_patterns:
    model_majority_output: '{phase}_model_labels.csv'
logging:
  level: INFO
runtime_flags:
  enable_regex: true


## 0002. multi_coder_analysis\config\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Configuration loading utilities (Phase 6)."""

import warnings
from functools import lru_cache
from pathlib import Path
import yaml

from .settings import Settings

_CFG_PATH = Path.cwd() / "config.yaml"


@lru_cache(maxsize=1)
def load_settings(path: Path | None = None) -> Settings:  # noqa: D401
    """Load settings from *path* or environment overrides.
    
    If *config.yaml* is detected, it is parsed and **deprecated** â€“ values are
    fed into the new Pydantic Settings model and a warning is issued.
    """
    cfg_path = Path(path) if path else _CFG_PATH

    if cfg_path.exists():
        warnings.warn(
            "Reading legacy config.yaml is deprecated; migrate to environment variables or TOML config.",
            DeprecationWarning,
            stacklevel=2,
        )
        try:
            data = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
        except Exception as e:
            warnings.warn(f"Could not parse {cfg_path}: {e}")
            data = {}
    else:
        data = {}

    return Settings(**data) 

## 0003. multi_coder_analysis\config\run_config.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from pathlib import Path
from pydantic import BaseModel, Field, validator

__all__ = ["RunConfig"]


class RunConfig(BaseModel):
    """Central runtime configuration for ToT execution."""

    phase: str = Field(
        "pipeline",
        pattern="^(legacy|pipeline)$",
        description="Execution mode: 'legacy' = old monolithic runner, 'pipeline' = new modular ToT stack",
    )
    dimension: str | None = Field(
        None,
        description="(deprecated) reserved for backward compatibility; ignored by pipeline",
    )
    input_csv: Path = Field(..., description="Path to input CSV of statements")
    output_dir: Path = Field(..., description="Directory to write outputs")
    provider: str = Field("gemini", pattern="^(gemini|openrouter)$", description="LLM provider to use")
    model: str = Field("models/gemini-2.5-flash-preview-04-17", description="Model identifier")
    batch_size: int = Field(1, ge=1, description="Batch size for LLM calls")
    concurrency: int = Field(1, ge=1, description="Thread pool size for batch mode")
    regex_mode: str = Field("live", pattern="^(live|shadow|off)$", description="Regex layer mode")
    shuffle_batches: bool = Field(False, description="Randomise batch order for load spreading")
    consensus_mode: str = Field(
        "final",
        pattern="^(hop|final)$",
        description="Consensus strategy: 'hop' = per-hop majority, 'final' = legacy end-of-tree vote",
    )

    # ---------------- Self-consistency decoding ----------------
    decode_mode: str = Field(
        "normal",
        pattern="^(normal|self-consistency)$",
        description="Decoding mode: normal = single path, self-consistency = multi-path sampling + voting",
    )

    sc_votes: int = Field(1, ge=1, le=200, description="Number of sampled paths for self-consistency")
    sc_rule: str = Field(
        "majority",
        pattern="^(majority|ranked|ranked-raw)$",
        description="Voting aggregation rule for self-consistency",
    )
    sc_top_k: int = Field(40, ge=0, description="top-k sampling cutoff (0 disables)")
    sc_top_p: float = Field(0.95, ge=0.0, le=1.0, description="nucleus sampling p-value")
    sc_temperature: float = Field(0.7, ge=0.0, description="Sampling temperature for self-consistency")

    @validator("output_dir", pre=True)
    def _expand_output_dir(cls, v):  # noqa: D401
        return Path(v).expanduser().absolute()

    @validator("input_csv", pre=True)
    def _expand_input_csv(cls, v):  # noqa: D401
        return Path(v).expanduser().absolute() 

## 0004. multi_coder_analysis\config\settings.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Pydantic Settings model for configuration (Phase 6)."""

from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional

__all__ = ["Settings"]


class Settings(BaseSettings):
    """Application settings with environment variable overrides.
    
    Environment variables are prefixed with MCA_ (e.g., MCA_PROVIDER=openrouter).
    """
    
    # Core execution settings
    phase: str = Field("pipeline", description="Pipeline phase label")
    provider: str = Field("gemini", description="LLM provider name")
    model: str = Field("models/gemini-2.5-flash-preview-04-17", description="Model identifier")
    
    # Performance settings
    batch_size: int = Field(1, ge=1, description="Batch size for LLM calls")
    concurrency: int = Field(1, ge=1, description="Thread pool size")
    
    # Feature flags
    enable_regex: bool = Field(True, description="Enable regex short-circuiting")
    regex_mode: str = Field("live", description="Regex mode: live|shadow|off")
    shuffle_batches: bool = Field(False, description="Randomise batch order")
    
    # API keys (optional - can be set via environment)
    google_api_key: Optional[str] = Field(None, description="Google Gemini API key")
    openrouter_api_key: Optional[str] = Field(None, description="OpenRouter API key")
    
    # Observability
    log_level: str = Field("INFO", description="Log level")
    json_logs: bool = Field(False, description="Emit JSON-formatted logs")
    
    class Config:
        env_prefix = "MCA_"
        env_file = ".env"
        case_sensitive = False 

## 0005. multi_coder_analysis\core\__init__.py
----------------------------------------------------------------------------------------------------
from importlib import import_module as _imp

# ---------------------------------------------------------------------------
# Backward-compat shim: early notebooks did
#   from multi_coder_analysis.core import Engine
# After the package re-org that path vanished.  Re-export the default
# implementation so existing user code keeps working without edits.
# ---------------------------------------------------------------------------

Engine = _imp("multi_coder_analysis.core.regex").Engine  # type: ignore[attr-defined]

__all__: list[str] = ["Engine"]

# Export consensus utilities for external reuse
from .consensus import ConsensusStep  # type: ignore[E402,F401]
__all__.append("ConsensusStep") 

## 0006. multi_coder_analysis\core\consensus.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Per-hop consensus reduction utilities."""

from collections import defaultdict
from typing import Dict, List, Tuple, Iterable

from multi_coder_analysis.models import HopContext
from multi_coder_analysis.core.pipeline import Step
from multi_coder_analysis.core.tiebreaker import conservative_tiebreak
from multi_coder_analysis.utils.tie import is_perfect_tie
from multi_coder_analysis import run_multi_coder_tot as _legacy

__all__ = ["ConsensusStep", "HopVariability"]

# hop -> list[(statement_id, distribution)]
HopVariability = Dict[int, List[Tuple[str, Dict[str, int]]]]


class ConsensusStep(Step[List[HopContext]]):  # type: ignore[misc]
    """Collapse *k* permutations into (optionally) a single survivor.

    The step expects **all** permutations for the same segment at a given hop
    to be present in the incoming list.
    """

    def __init__(self, hop_idx: int, variability_log: HopVariability):
        self.hop_idx = hop_idx
        self._var = variability_log

    def run(self, ctxs: List[HopContext]) -> List[HopContext]:  # type: ignore[override]
        # Group by segment / statement_id
        buckets: Dict[str, List[HopContext]] = defaultdict(list)
        for c in ctxs:
            buckets[c.statement_id].append(c)

        survivors: List[HopContext] = []
        for sid, perms in buckets.items():
            # Collect votes from permutations ("yes" / "no" / "uncertain")
            votes: List[str] = [
                (p.raw_llm_responses[-1].get("answer", "") if p.raw_llm_responses else "uncertain")
                for p in perms
            ]

            decided, winner = conservative_tiebreak(votes)

            # Record distribution regardless of outcome
            dist = {v: votes.count(v) for v in set(votes)}
            self._var.setdefault(self.hop_idx, []).append((sid, dist))

            if decided and winner == "yes":
                # Select representative permutation (first) to continue
                rep = perms[0]
                rep.is_concluded = True
                if not rep.final_frame:
                    from multi_coder_analysis import run_multi_coder_tot as _legacy
                    rep.final_frame = _legacy.Q_TO_FRAME.get(self.hop_idx)
                survivors.append(rep)
            elif decided and winner == "no":
                # No conclusion â€“ all permutations progress
                survivors.extend(perms)
            else:
                # Tie / no majority â€“ mark concluded as tie (filtered out)
                for p in perms:
                    p.is_concluded = True
                    p.final_frame = "tie"
                # Not forwarded further
        return survivors 

## 0007. multi_coder_analysis\core\pipeline\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Lightweight functional pipeline primitives used by the Tree-of-Thought refactor.

The goal is to decouple algorithmic steps from I/O and orchestration while
remaining extremely small and dependency-free.  Each `Step[T]` receives and
returns the same context object enabling natural chaining and testability.
"""

from typing import Generic, TypeVar, Protocol, Callable, List

T_co = TypeVar("T_co", covariant=True)
T = TypeVar("T")


class Step(Generic[T], Protocol):
    """A pure-function processing step.

    Sub-classes implement :py:meth:`run` and **MUST NOT** mutate global state or
    perform side-effects outside the provided context object.
    """

    def run(self, ctx: T) -> T:  # noqa: D401
        """Transform *ctx* and return it (or a *new* instance).
        
        The default Tree-of-Thought implementation mutates the context in-place
        and returns the same object for convenience.
        """
        raise NotImplementedError


class FunctionStep(Generic[T]):
    """Adapter turning a plain function into a :class:`Step`."""

    def __init__(self, fn: Callable[[T], T]):
        self._fn = fn

    def run(self, ctx: T) -> T:  # type: ignore[override]
        return self._fn(ctx)


class Pipeline(Generic[T]):
    """Composable list of :class:`Step` objects executed sequentially."""

    def __init__(self, steps: List[Step[T]]):
        self._steps = steps

    def run(self, ctx: T) -> T:
        for step in self._steps:
            # Allow steps (e.g., answer evaluators) to signal early termination
            if (
                isinstance(ctx, list)
                and ctx
                and all(getattr(c, "is_concluded", False) for c in ctx)
            ) or getattr(ctx, "is_concluded", False):
                break
            ctx = step.run(ctx)
        return ctx


__all__ = [
    "Step",
    "FunctionStep",
    "Pipeline",
] 

## 0008. multi_coder_analysis\core\pipeline\consensus_tot.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Consensus-aware Tree-of-Thought pipeline builder."""

from typing import List, Tuple

from multi_coder_analysis.models import HopContext
from multi_coder_analysis.core.pipeline import Pipeline, Step
from multi_coder_analysis.core.pipeline.tot import _HopStep  # type: ignore
from multi_coder_analysis.core.consensus import ConsensusStep, HopVariability
from multi_coder_analysis.providers import ProviderProtocol

__all__ = ["build_consensus_pipeline"]


class _ParallelHopStep(Step[List[HopContext]]):  # type: ignore[misc]
    """Map a single-hop step across all permutations in the list."""

    def __init__(
        self,
        hop_idx: int,
        provider: ProviderProtocol,
        model: str,
        *,
        temperature: float,
        top_k: int | None = None,
        top_p: float | None = None,
    ):
        self._inner = _HopStep(
            hop_idx,
            provider,
            model,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
        )

    def run(self, ctxs: List[HopContext]) -> List[HopContext]:  # type: ignore[override]
        results: List[HopContext] = []
        for ctx in ctxs:
            if ctx.is_concluded:
                # Preserve without re-processing
                results.append(ctx)
            else:
                results.append(self._inner.run(ctx))
        return results


def build_consensus_pipeline(
    provider: ProviderProtocol,
    model: str,
    temperature: float = 0.0,
    top_k: int | None = None,
    top_p: float | None = None,
    variability_log: HopVariability | None = None,
) -> Tuple[Pipeline[List[HopContext]], HopVariability]:
    """Return a pipeline operating on *lists* of HopContext objects."""

    var: HopVariability = variability_log or {}

    steps: List[Step[List[HopContext]]] = []
    for h in range(1, 13):
        steps.append(
            _ParallelHopStep(
                h,
                provider,
                model,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
            )
        )
        steps.append(ConsensusStep(h, var))

    return Pipeline(steps), var 

## 0009. multi_coder_analysis\core\pipeline\tot.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Pure-function Tree-of-Thought pipeline built on :class:`Step`. (Phase 5)"""

from typing import List

from multi_coder_analysis.models import HopContext
from multi_coder_analysis.core.pipeline import Step, Pipeline
from multi_coder_analysis.core.regex import Engine
from multi_coder_analysis.providers import ProviderProtocol

# Re-use existing helper from legacy implementation to avoid code duplication
from multi_coder_analysis import run_multi_coder_tot as _legacy

__all__ = ["build_tot_pipeline"]


class _HopStep(Step[HopContext]):
    """Single-hop processing step.

    The step first tries the regex engine; if inconclusive it delegates to the
    provider using the _legacy._call_llm_single_hop helper to preserve existing
    behaviour.  The class is internal â€“ use :func:`build_tot_pipeline` instead.
    """

    _rx = Engine.default()

    def __init__(
        self,
        hop_idx: int,
        provider: ProviderProtocol,
        model: str,
        *,
        temperature: float,
        top_k: int | None = None,
        top_p: float | None = None,
    ):
        self.hop_idx = hop_idx
        self._provider = provider
        self._model = model
        self._temperature = temperature
        self._top_k = top_k
        self._top_p = top_p

    # ------------------------------------------------------------------
    # The heavy lifting is delegated to code already battle-tested in the
    # legacy module.  This guarantees behavioural parity while moving the
    # orchestration into the new pipeline layer.
    # ------------------------------------------------------------------
    def run(self, ctx: HopContext) -> HopContext:  # noqa: D401
        ctx.q_idx = self.hop_idx

        regex_ans = self._rx.match(ctx)
        if regex_ans:
            ctx.raw_llm_responses.append(regex_ans)
            if regex_ans.get("answer") == "yes":
                ctx.final_frame = regex_ans.get("frame") or _legacy.Q_TO_FRAME[self.hop_idx]
                ctx.final_justification = regex_ans.get("rationale")
                ctx.is_concluded = True
            return ctx

        # Fall-through to LLM
        llm_resp = _legacy._call_llm_single_hop(
            ctx,
            self._provider,
            self._model,
            temperature=self._temperature,
            top_k=self._top_k,
            top_p=self._top_p,
        )  # type: ignore[arg-type]
        ctx.raw_llm_responses.append(llm_resp)
        if llm_resp.get("answer") == "yes":
            ctx.final_frame = _legacy.Q_TO_FRAME[self.hop_idx]
            ctx.final_justification = llm_resp.get("rationale", "").strip()
            ctx.is_concluded = True
        return ctx


def build_tot_pipeline(
    provider: ProviderProtocol,
    model: str,
    *,
    temperature: float = 0.0,
    top_k: int | None = None,
    top_p: float | None = None,
) -> Pipeline[HopContext]:
    """Return a :class:`Pipeline` implementing the 12-hop deterministic chain."""

    steps: List[Step[HopContext]] = [
        _HopStep(
            h,
            provider,
            model,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
        )
        for h in range(1, 13)
    ]
    return Pipeline(steps) 

## 0010. multi_coder_analysis\core\prompt.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from functools import lru_cache
from pathlib import Path
from typing import Tuple, Dict, Any, TypedDict
import re

import yaml

__all__ = ["parse_prompt", "PromptMeta"]


class PromptMeta(TypedDict, total=False):
    hop: int
    short_name: str
    description: str
    # Extend with other known keys as needed.


# Regex to match YAML front-matter at top of file
_FM_RE = re.compile(r"^---\s*\n(.*?)\n---\s*\n?", re.DOTALL)


@lru_cache(maxsize=128)
def parse_prompt(path: Path) -> Tuple[str, PromptMeta]:
    """Return (prompt_body, front_matter) for *path*.

    The result is cached for the lifetime of the process to avoid unnecessary
    disk I/O during batch processing.
    """
    text = path.read_text(encoding="utf-8")

    m = _FM_RE.match(text)
    if not m:
        return text, {}  # type: ignore[return-value]

    meta_yaml = m.group(1)
    try:
        meta: PromptMeta = yaml.safe_load(meta_yaml) or {}
    except Exception:
        meta = {}  # type: ignore[assignment]

    body = text[m.end() :]
    # Normalise â€“ drop the *single* blank line often left between '---' and
    # the actual prompt content so downstream tokenisers don't pay for it.
    if body.startswith("\n"):
        body = body.lstrip("\n")

    return body, meta 

## 0011. multi_coder_analysis\core\regex\__init__.py
----------------------------------------------------------------------------------------------------
from .engine import Engine
from . import stats
from . import loader

__all__ = ["Engine"] 

## 0012. multi_coder_analysis\core\regex\engine.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Class-based regex engine for the 12-hop Tree-of-Thought pipeline.

The Engine class encapsulates regex matching logic as a stateless, first-class
object that can be instantiated with different rule sets.
"""

# Prefer the "regex" engine if available (supports variable-width look-behinds).
try:
    import regex as re  # type: ignore
except ImportError:  # pragma: no cover
    import re  # type: ignore

import logging
from typing import Optional, TypedDict, Callable
from collections import Counter
import threading
import importlib

# Import rules - handle both package and script contexts
try:
    from ...regex_rules import COMPILED_RULES, PatternInfo, RAW_RULES  # type: ignore
except ImportError:  # pragma: no cover
    try:
        from multi_coder_analysis.regex_rules import COMPILED_RULES, PatternInfo, RAW_RULES  # type: ignore
    except ImportError:
        # Final fallback for script execution
        from regex_rules import COMPILED_RULES, PatternInfo, RAW_RULES  # type: ignore

__all__ = ["Engine", "Answer"]


class Answer(TypedDict):
    """Typed structure returned when a regex rule fires."""
    answer: str
    rationale: str
    frame: Optional[str]
    regex: dict  # detailed match information


class Engine:
    """Stateless regex matching engine with configurable rule sets.
    
    Each Engine instance encapsulates its own rule statistics and configuration,
    enabling multiple engines with different behaviors to coexist.
    """
    
    # Class-level singleton for the default engine
    _DEFAULT: Optional["Engine"] = None
    
    def __init__(
        self,
        rules: Optional[dict[int, list[PatternInfo]]] = None,
        global_enabled: bool = True,
        force_shadow: bool = False,
        hit_logger: Optional[Callable[[dict], None]] = None,
    ):
        """Initialize a new Engine instance.
        
        Args:
            rules: Hop -> PatternInfo mapping. If None, uses COMPILED_RULES.
            global_enabled: Whether regex matching is enabled.
            force_shadow: If True, regex runs but never short-circuits.
            hit_logger: Optional callback for successful matches.
        """
        self._rules = rules if rules is not None else COMPILED_RULES
        self._global_enabled = global_enabled
        self._force_shadow = force_shadow
        self._hit_logger = hit_logger
        self._rule_stats: dict[str, Counter] = {}
        self._stat_lock = threading.Lock()
    
    @classmethod
    def default(cls) -> "Engine":
        """Get the default singleton Engine instance.
        
        This provides backward compatibility with the module-level API.
        """
        if cls._DEFAULT is None:
            cls._DEFAULT = cls()
        return cls._DEFAULT
    
    def set_global_enabled(self, flag: bool) -> None:
        """Enable or disable regex matching globally."""
        self._global_enabled = flag
    
    def set_force_shadow(self, flag: bool) -> None:
        """When True, regex runs but never short-circuits (shadow mode)."""
        self._force_shadow = flag
    
    def set_hit_logger(self, fn: Optional[Callable[[dict], None]]) -> None:
        """Register a callback to receive detailed match information."""
        self._hit_logger = fn
    
    def get_rule_stats(self) -> dict[str, Counter]:
        """Get per-rule statistics for this engine instance."""
        return dict(self._rule_stats)
    
    def _rule_fires(self, rule: PatternInfo, text: str) -> bool:
        """Return True iff rule matches positively and is not vetoed."""
        if not isinstance(rule.yes_regex, re.Pattern):
            logging.error("COMPILED_RULES must contain compiled patterns")
            return False

        positive = bool(rule.yes_regex.search(text))
        if not positive:
            return False

        if rule.veto_regex and isinstance(rule.veto_regex, re.Pattern):
            if rule.veto_regex.search(text):
                return False
        return True
    
    def match(self, ctx) -> Optional[Answer]:  # noqa: ANN001
        """Attempt to answer the current hop deterministically.

        Parameters
        ----------
        ctx : HopContext
            The current hop context (expects attributes: `q_idx`, `segment_text`).

        Returns
        -------
        Optional[Answer]
            â€¢ Dict with keys {answer, rationale, frame} when a single live rule
              fires with certainty.
            â€¢ None when no rule (or >1 rules) fire, or hop not covered, or rule is
              in shadow mode.
        """
        hop: int = getattr(ctx, "q_idx")
        text: str = getattr(ctx, "segment_text")

        if not self._global_enabled:
            return None

        rules = self._rules.get(hop, [])
        if not rules:
            # Try lazy reload for robustness
            try:
                from ... import regex_rules as _rr  # type: ignore
                importlib.reload(_rr)
                rules = _rr.COMPILED_RULES.get(hop, [])
            except Exception:  # pragma: no cover
                rules = []

        if not rules:
            return None

        # Safety net: merge missing canonical rules
        try:
            from ... import regex_rules as _rr  # package context
        except ImportError:  # script context
            try:
                from multi_coder_analysis import regex_rules as _rr  # type: ignore
            except ImportError:
                import regex_rules as _rr  # type: ignore

        _master_rules = _rr.COMPILED_RULES.get(hop, [])
        if _master_rules:
            existing_names = {r.name for r in rules}
            for _r in _master_rules:
                if _r.name not in existing_names:
                    rules.append(_r)

        winning_rule: Optional[PatternInfo] = None
        first_hit_rule: Optional[PatternInfo] = None

        # Evaluate every rule for statistics
        for rule in rules:
            fired = self._rule_fires(rule, text)

            # Thread-safe stats update
            with self._stat_lock:
                ctr = self._rule_stats.setdefault(rule.name, Counter())
                ctr["total"] += 1
                if fired:
                    ctr["hit"] += 1

            # Record first hit for shadow-mode logging
            if fired and first_hit_rule is None:
                first_hit_rule = rule

            if (
                fired
                and not self._force_shadow
                and (rule.mode == "live" or rule.mode == "shadow")
            ):
                if winning_rule is not None:
                    # Tolerate multiple hits if they agree on frame
                    if rule.yes_frame == winning_rule.yes_frame:
                        continue

                    # Conflicting frames â†’ fall-through to LLM
                    logging.debug(
                        "Regex engine ambiguity: >1 conflicting rule fired for hop %s (%s vs %s)",
                        hop,
                        winning_rule.name,
                        rule.name,
                    )
                    return None
                winning_rule = rule

        # Shadow-mode logging
        if winning_rule is None:
            logging.debug("No regex rule matched for hop %s", hop)

            # shadow mode accounting
            from multi_coder_analysis.providers.base import _USAGE_ACCUMULATOR as _ACC  # avoid cycle
            _ACC["total_hops"] = _ACC.get("total_hops", 0) + 1
            if self._force_shadow:
                _ACC["regex_hit_shadow"] = _ACC.get("regex_hit_shadow", 0) + 1

            return None

        # ---- counters ----
        from multi_coder_analysis.providers.base import _USAGE_ACCUMULATOR as _ACC  # local import
        _ACC["regex_yes"] = _ACC.get("regex_yes", 0) + 1
        _ACC["total_hops"] = _ACC.get("total_hops", 0) + 1

        # Compute match details
        m = winning_rule.yes_regex.search(text)  # type: ignore[arg-type]
        span = [m.start(), m.end()] if m else None
        captures = list(m.groups()) if m else []

        rationale = f"regex:{winning_rule.name} matched"

        # Emit hit record
        if self._hit_logger is not None:
            try:
                self._hit_logger({
                    "statement_id": getattr(ctx, "statement_id", None),
                    "hop": hop,
                    "segment": text,
                    "rule": winning_rule.name,
                    "frame": winning_rule.yes_frame,
                    "mode": winning_rule.mode,
                    "span": span,
                })
            except Exception as e:  # pragma: no cover
                logging.debug("Regex hit logger raised error: %s", e, exc_info=True)

        return {
            "answer": "yes",
            "rationale": rationale,
            "frame": winning_rule.yes_frame,
            "regex": {
                "rule": winning_rule.name,
                "span": span,
                "captures": captures,
            },
        }


# Backward compatibility: module-level functions delegate to default engine
def match(ctx) -> Optional[Answer]:  # noqa: ANN001
    """Backward compatibility function - delegates to Engine.default().match()."""
    return Engine.default().match(ctx)


def set_global_enabled(flag: bool) -> None:
    """Backward compatibility function."""
    Engine.default().set_global_enabled(flag)


def set_force_shadow(flag: bool) -> None:
    """Backward compatibility function."""
    Engine.default().set_force_shadow(flag)


def get_rule_stats() -> dict[str, Counter]:
    """Backward compatibility function."""
    return Engine.default().get_rule_stats()


def set_hit_logger(fn: Callable[[dict], None]) -> None:
    """Backward compatibility function."""
    Engine.default().set_hit_logger(fn)


# Expose module-level globals for backward compatibility
_GLOBAL_ENABLE = True
_FORCE_SHADOW = False
_HIT_LOG_FN: Optional[Callable[[dict], None]] = None 

## 0013. multi_coder_analysis\core\regex\loader.py
----------------------------------------------------------------------------------------------------
"""Regex rule loader with plugin support (Phase 3)."""

from __future__ import annotations

import re
import yaml
from importlib import resources
from typing import List, Pattern

__all__ = ["load_rules"]


def load_rules() -> List[List[Pattern[str]]]:
    """Load regex rules from YAML files.
    
    Returns:
        List of rule lists, indexed by hop number (1-12)
    """
    # --------------------------------------------------
    # Locate the bundled YAML via importlib.resources â€“ this works regardless
    # of whether the package is executed from an unpacked directory tree *or*
    # an installed, zipped wheel.
    # --------------------------------------------------
    try:
        # Fallback to package *root* then sub-path because 'multi_coder_analysis.regex'
        # is not a Python package (no __init__.py).
        rules_text = resources.files("multi_coder_analysis").joinpath("regex", "hop_patterns.yml").read_text("utf-8")
    except (FileNotFoundError, ModuleNotFoundError):
        # Package wasn't shipped with rules â€“ treat as "no-op" engine.
        return [[] for _ in range(13)]  # 0-12, using 1-12

    try:
        data = yaml.safe_load(rules_text)
        
        # Convert to compiled patterns
        rules = [[] for _ in range(13)]  # 0-12, using 1-12
        
        if data and isinstance(data, dict):
            for hop_key, patterns in data.items():
                try:
                    hop_num = int(str(hop_key).lstrip("Qq"))
                    if 1 <= hop_num <= 12 and isinstance(patterns, list):
                        rules[hop_num] = [
                            re.compile(pattern, re.IGNORECASE)
                            for pattern in patterns
                            if isinstance(pattern, str)
                        ]
                except (ValueError, IndexError):
                    continue
        
        return rules
        
    except Exception:
        # Fallback to empty rules on any error
        return [[] for _ in range(13)] 

## 0014. multi_coder_analysis\core\regex\stats.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Regex engine statistics and reporting utilities."""

from collections import Counter
from typing import Dict, Any
import json
from pathlib import Path


def format_rule_stats(stats: Dict[str, Counter]) -> Dict[str, Any]:
    """Format rule statistics for human-readable output.
    
    Args:
        stats: Dictionary mapping rule names to hit/total counters.
        
    Returns:
        Formatted statistics with coverage percentages.
    """
    formatted = {}
    total_evaluations = 0
    total_hits = 0
    
    for rule_name, counter in stats.items():
        hits = counter.get("hit", 0)
        total = counter.get("total", 0)
        coverage = (hits / total * 100) if total > 0 else 0.0
        
        formatted[rule_name] = {
            "hits": hits,
            "total_evaluations": total,
            "coverage_percent": round(coverage, 2)
        }
        
        total_evaluations += total
        total_hits += hits
    
    # Add overall summary
    overall_coverage = (total_hits / total_evaluations * 100) if total_evaluations > 0 else 0.0
    formatted["_summary"] = {
        "total_rules": len(stats),
        "total_hits": total_hits,
        "total_evaluations": total_evaluations,
        "overall_coverage_percent": round(overall_coverage, 2)
    }
    
    return formatted


def export_stats_to_json(stats: Dict[str, Counter], output_path: Path) -> None:
    """Export rule statistics to a JSON file.
    
    Args:
        stats: Rule statistics from Engine.get_rule_stats().
        output_path: Path where to write the JSON file.
    """
    formatted = format_rule_stats(stats)
    
    with output_path.open('w', encoding='utf-8') as f:
        json.dump(formatted, f, indent=2, ensure_ascii=False)


def print_stats_summary(stats: Dict[str, Counter]) -> None:
    """Print a human-readable summary of rule statistics.
    
    Args:
        stats: Rule statistics from Engine.get_rule_stats().
    """
    formatted = format_rule_stats(stats)
    summary = formatted.pop("_summary")
    
    print(f"\nðŸ“Š Regex Engine Statistics Summary")
    print(f"{'='*50}")
    print(f"Total rules: {summary['total_rules']}")
    print(f"Total evaluations: {summary['total_evaluations']}")
    print(f"Total hits: {summary['total_hits']}")
    print(f"Overall coverage: {summary['overall_coverage_percent']:.2f}%")
    print()
    
    if formatted:
        print("Per-rule breakdown:")
        print(f"{'Rule Name':<30} {'Hits':<8} {'Total':<8} {'Coverage':<10}")
        print("-" * 60)
        
        # Sort by coverage descending
        sorted_rules = sorted(
            formatted.items(),
            key=lambda x: x[1]["coverage_percent"],
            reverse=True
        )
        
        for rule_name, data in sorted_rules:
            print(f"{rule_name:<30} {data['hits']:<8} {data['total_evaluations']:<8} {data['coverage_percent']:<10.2f}%")


__all__ = ["format_rule_stats", "export_stats_to_json", "print_stats_summary"] 

## 0015. multi_coder_analysis\core\self_consistency.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Self-consistency decoding helpers.

This module implements the multi-path sampling + voting strategy popularised
by the *Self-Consistency* paper.  The public surface consists of two helpers:

1. decode_paths() â†’ runs *N* independent passes through the deterministic
   Tree-of-Thought pipeline using stochastic decoding.
2. aggregate()    â†’ collapses the list of answers into a single prediction
   according to one of three voting rules.

The implementation is intentionally dependency-free â€“ we rely on the existing
`build_tot_pipeline` to perform a single ToT pass and use provider-supplied
usage metadata as a crude log-prob proxy when length-normalisation is needed.
"""

from collections import Counter, defaultdict
from typing import Dict, List, Tuple

from multi_coder_analysis.core.pipeline.tot import build_tot_pipeline
from multi_coder_analysis.models import HopContext
from multi_coder_analysis.providers import ProviderProtocol

__all__ = [
    "decode_paths",
    "aggregate",
]


# ---------------------------------------------------------------------------
# Multi-path decoding
# ---------------------------------------------------------------------------

def decode_paths(
    base_ctx: HopContext,
    provider: ProviderProtocol,
    model: str,
    *,
    votes: int,
    temperature: float,
    top_k: int,
    top_p: float,
) -> List[Tuple[str, float]]:
    """Run the ToT pipeline *votes* times with stochastic sampling.

    Parameters
    ----------
    base_ctx
        Template context holding the statement text + IDs.
    provider
        Provider implementing the generate() interface.
    model
        Model identifier.
    votes
        Number of independent samples.
    temperature, top_k, top_p
        Sampling hyper-parameters forwarded to the provider.  Note that the
        internal ToT pipeline currently passes only *temperature*.  Providers
        expose *top_k*/*top_p* anyway, so we call them directly when ToT falls
        through to the LLM.

    Returns
    -------
    list of tuples
        Each tuple is (answer, score) where *answer* is the final frame label
        and *score* is a rough heuristic of likelihood (negative length-
        normalised token count when probability not available).
    """

    # Build a *fresh* pipeline so state does not leak across samples.
    pipeline = build_tot_pipeline(
        provider,
        model,
        temperature=temperature,
        top_k=(None if top_k == 0 else top_k),
        top_p=top_p,
    )

    samples: List[Tuple[str, float]] = []
    for _ in range(votes):
        # Create a shallow copy of the base context
        ctx = HopContext(
            statement_id=base_ctx.statement_id,
            segment_text=base_ctx.segment_text,
            article_id=base_ctx.article_id,
        )
        # Run ToT
        pipeline.run(ctx)
        ans = ctx.final_frame or "âˆ…"

        # Use token count as crude negative log-prob if real logprobs missing
        usage = provider.get_last_usage() or {}
        score = -float(usage.get("total_tokens", 0))
        samples.append((ans, score))

    return samples


# ---------------------------------------------------------------------------
# Voting aggregation
# ---------------------------------------------------------------------------

def _majority(pairs: List[Tuple[str, float]]) -> Tuple[str, float]:
    counts = Counter(a for a, _ in pairs)
    ans, freq = counts.most_common(1)[0]
    return ans, freq / len(pairs)


def _ranked(pairs: List[Tuple[str, float]], normalise: bool) -> Tuple[str, float]:
    buckets: Dict[str, float] = defaultdict(float)
    counts: Dict[str, int] = defaultdict(int)
    for ans, score in pairs:
        buckets[ans] += score
        counts[ans] += 1

    if normalise:
        # Divide by occurrence count â†’ mean score
        for ans in buckets:
            buckets[ans] /= max(counts[ans], 1)

    # Select maximum score (note score is negative token count, so higher is better)
    ans = max(buckets, key=buckets.get)
    return ans, buckets[ans]


def aggregate(pairs: List[Tuple[str, float]], rule: str = "majority") -> Tuple[str, float]:
    """Collapse *pairs* into (answer, confidence) according to *rule*."""
    rule = rule.lower()
    if rule == "majority":
        return _majority(pairs)
    if rule == "ranked":
        return _ranked(pairs, normalise=True)
    if rule == "ranked-raw":
        return _ranked(pairs, normalise=False)
    raise ValueError(f"Unknown rule: {rule}") 

## 0016. multi_coder_analysis\core\tiebreaker.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Generic tie-breaking utilities used by consensus layers."""

from collections import Counter
from typing import List, Optional, Tuple

__all__ = ["conservative_tiebreak"]


def conservative_tiebreak(votes: List[str]) -> Tuple[bool, Optional[str]]:
    """Return (has_consensus, winning_value).

    The function implements a conservative rule: a value only wins if it
    secures *strict* (>50 %) majority.  Otherwise, it reports no consensus.
    """
    ctr = Counter(votes)
    if not ctr:
        return False, None
    winner, n = ctr.most_common(1)[0]
    if n > len(votes) / 2:
        return True, winner
    return False, None 

## 0017. multi_coder_analysis\models\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from .hop import HopContext, BatchHopContext  # noqa: F401

__all__ = [
    "HopContext",
    "BatchHopContext",
] 

## 0018. multi_coder_analysis\models\hop.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, TypedDict

__all__ = [
    "HopContext",
    "BatchHopContext",
]

# ---------------------------------------------------------------------------
# Typed aliases help downstream static analysis without dict[str, Any] noise.
# ---------------------------------------------------------------------------
AnalysisHistory = List[str]
ReasoningTrace = List[Dict[str, Any]]
RawLLMResponses = List[Dict[str, Any]]


@dataclass
class HopContext:
    """State container for a single segment as it progresses through the 12-hop ToT chain."""

    # -------------- Static Data --------------
    statement_id: str
    segment_text: str
    # Optional article identifier (source document) â€“ used for trace exports
    article_id: Optional[str] = None

    # -------------- Dynamic State --------------
    q_idx: int = 0
    is_concluded: bool = False
    final_frame: Optional[str] = None           # The definitive frame once concluded (e.g., Alarmist, Neutral)
    final_justification: Optional[str] = None   # The rationale for the final decision

    # Track consecutive "uncertain" responses to support early termination.
    uncertain_count: int = 0

    # -------------- Logging & Audit Trails --------------
    analysis_history: AnalysisHistory = field(default_factory=list)
    reasoning_trace: ReasoningTrace = field(default_factory=list)
    raw_llm_responses: RawLLMResponses = field(default_factory=list)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Batch Positional Meta â”€â”€â”€â”€â”€â”€â”€â”€â”€
    batch_pos: Optional[int] = None  # 1-based index within API call
    batch_size: Optional[int] = None  # total number of segments in API call

    # -------------- Parsed prompt metadata --------------
    prompt_meta: Dict[str, Any] = field(default_factory=dict, repr=False)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Permutation bookkeeping â”€â”€â”€â”€â”€â”€â”€â”€â”€
    permutation_idx: int | None = None

    # -------------- Convenience Properties --------------
    @property
    def dim1_frame(self) -> Optional[str]:
        """Alias retained for backward compatibility with downstream scripts."""
        return self.final_frame


@dataclass
class BatchHopContext:
    """Container for a batch of segments processed together at a single hop."""

    batch_id: str
    hop_idx: int
    segments: List[HopContext]

    # Raw LLM I/O for debugging
    raw_prompt: str = ""
    raw_response: str = ""
    thoughts: Optional[str] = None 

## 0019. multi_coder_analysis\providers\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from importlib import import_module
from typing import TYPE_CHECKING

from .base import ProviderProtocol

if TYPE_CHECKING:
    # Avoid circular imports during type checking
    from .gemini import GeminiProvider
    from .openrouter import OpenRouterProvider

__all__ = ["ProviderProtocol", "get_provider", "GeminiProvider", "OpenRouterProvider"]


def get_provider(name: str, **kwargs) -> ProviderProtocol:
    """Factory function to create provider instances.
    
    Args:
        name: Provider name ('gemini' or 'openrouter').
        **kwargs: Additional arguments passed to provider constructor.
        
    Returns:
        Provider instance implementing ProviderProtocol.
        
    Raises:
        ValueError: If provider name is not recognized.
        ImportError: If provider module cannot be imported.
    """
    provider_map = {
        "gemini": "multi_coder_analysis.providers.gemini",
        "openrouter": "multi_coder_analysis.providers.openrouter",
    }
    
    if name not in provider_map:
        raise ValueError(f"Unknown provider: {name}. Available: {list(provider_map.keys())}")
    
    try:
        module = import_module(provider_map[name])

        from inspect import isclass

        # Collect *all* candidate classes that look like providers.
        candidates = [
            getattr(module, attr_name)
            for attr_name in dir(module)
            if attr_name.endswith("Provider") and isclass(getattr(module, attr_name))
        ]

        # Return the first class that satisfies the minimal interface.
        for cls in candidates:
            if all(hasattr(cls, m) for m in ("generate", "get_last_thoughts", "get_last_usage")):
                return cls(**kwargs)

        raise ImportError(f"No provider class found in {provider_map[name]}")
        
    except ImportError as e:
        raise ImportError(f"Could not import provider {name}: {e}") from e


# Re-export provider classes for direct import
def __getattr__(name: str):
    """Lazy import of provider classes."""
    if name == "GeminiProvider":
        from .gemini import GeminiProvider
        return GeminiProvider
    elif name == "OpenRouterProvider":
        from .openrouter import OpenRouterProvider
        return OpenRouterProvider
    else:
        raise AttributeError(f"module {__name__!r} has no attribute {name!r}") 

## 0020. multi_coder_analysis\providers\base.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from typing import Protocol, runtime_checkable, Callable, Dict
import threading

__all__ = ["ProviderProtocol"]


@runtime_checkable
class ProviderProtocol(Protocol):
    """Protocol defining the interface for LLM providers.
    
    This uses PEP 544 structural subtyping, allowing any class with the
    required methods to be used as a provider without explicit inheritance.
    """
    
    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        model: str,
        temperature: float = 0.0,
        *,
        top_k: int | None = None,
        top_p: float | None = None,
    ) -> str:
        """Generate a response from the LLM.
        
        The **public** Tree-of-Thought API passes two separate prompt strings â€“
        a high-level *system/developer* instruction and the actual *user*
        content.  Providers must therefore accept both arguments explicitly so
        the call-site ordering is unambiguous.
        
        Args:
            system_prompt: Instruction that sets assistant behaviour.
            user_prompt: The user-visible prompt or question.
            model: Model identifier (provider-specific).
            temperature: Sampling temperature (``0.0`` = deterministic).
            
        Returns:
            The generated response text.
        """
        ...
    
    def get_last_thoughts(self) -> str:
        """Get thinking/reasoning traces from the last generate() call.
        
        Returns:
            Thinking traces as a string, or empty string if not available.
        """
        ...
    
    def get_last_usage(self) -> dict:
        """Get token usage statistics from the last generate() call.
        
        Returns:
            Dictionary with keys like 'prompt_tokens', 'response_tokens', 
            'total_tokens', etc. Empty dict if not available.
        """
        ... 

# ---------------------------------------------------------------------------
# Global usage accumulator â€“ lightweight telemetry across providers and threads
# ---------------------------------------------------------------------------

_USAGE_ACCUMULATOR: Dict[str, int] = {
    "prompt_tokens": 0,
    "response_tokens": 0,
    "thought_tokens": 0,
    "total_tokens": 0,
    "llm_calls": 0,
    "regex_yes": 0,
    "regex_hit_shadow": 0,
    "total_hops": 0,
}


def get_usage_accumulator() -> Dict[str, int]:  # noqa: D401
    """Return the live, process-wide token usage counter (mutable)."""

    return _USAGE_ACCUMULATOR


def track_usage(fn: Callable):  # type: ignore[type-arg]
    """Decorator: after *fn* executes, merge provider.get_last_usage() into global counter."""

    def _wrap(self, *args, **kwargs):  # type: ignore[no-self-use]
        out = fn(self, *args, **kwargs)
        if hasattr(self, "get_last_usage"):
            usage = self.get_last_usage() or {}
            _USAGE_ACCUMULATOR["prompt_tokens"] += int(usage.get("prompt_tokens", 0))
            _USAGE_ACCUMULATOR["response_tokens"] += int(usage.get("response_tokens", 0))
            _USAGE_ACCUMULATOR["thought_tokens"] += int(usage.get("thought_tokens", 0))
            _USAGE_ACCUMULATOR["total_tokens"] += int(usage.get("total_tokens", 0))
            _USAGE_ACCUMULATOR["llm_calls"] += 1
        return out

    return _wrap 

## 0021. multi_coder_analysis\providers\gemini.py
----------------------------------------------------------------------------------------------------
import os
# NOTE: Google Gemini SDK is optional in many test environments. We therefore
# *attempt* to import it lazily and only raise a helpful error message at
# instantiation time, not at module import time (which would break unrelated
# unit-tests that merely import the parent package).
import logging
from typing import Optional, TYPE_CHECKING

# Defer heavy / optional dependency import â€“ set sentinels instead.  We rely
# on run-time checks within `__init__` to raise when the SDK is truly needed.
if TYPE_CHECKING:
    # Type-hints only (ignored at run-time when type checkers are disabled)
    from google import genai as _genai  # pragma: no cover
    from google.genai import types as _types  # pragma: no cover

genai = None  # will attempt to import lazily in __init__
types = None
# Import typing protocol *only* for static type-checkers â€“ we do **not** need to
# subclass it at runtime.
from .base import ProviderProtocol
from .base import track_usage

class GeminiProvider:  # implements ProviderProtocol via duck-typing
    def __init__(self, api_key: Optional[str] = None):
        global genai, types  # noqa: PLW0603 â€“ we intentionally mutate module globals

        # Lazy-load the Google SDK only when the provider is actually
        # instantiated (i.e. during *production* runs, not unit-tests that
        # only touch auxiliary helpers like _assemble_prompt).
        if genai is None or types is None:
            try:
                from google import genai as _genai  # type: ignore
                from google.genai import types as _types  # type: ignore

                genai = _genai  # promote to module-level for reuse
                types = _types
            except ImportError as e:
                # Surface a clear, actionable message **only** when the class
                # is actually used.  This keeps import-time side effects
                # minimal and avoids breaking unrelated tests.
                raise ImportError(
                    "The google-genai SDK is required to use GeminiProvider."
                    "\n   âžœ  pip install google-genai"
                ) from e

        key = api_key or os.getenv("GOOGLE_API_KEY")
        if not key:
            raise ValueError("GOOGLE_API_KEY not set")

        # Initialize client directly with the API key (newer SDK style)
        self._client = genai.Client(api_key=key)

    @track_usage
    def generate(self, system_prompt: str, user_prompt: str, model: str, temperature: float = 0.0, *, top_k: int | None = None, top_p: float | None = None) -> str:
        cfg = {"temperature": temperature}
        # Use provided sampling params falling back to defaults
        if top_p is not None:
            cfg["top_p"] = float(top_p)
        else:
            cfg["top_p"] = 0.1
        if top_k is not None:
            cfg["top_k"] = int(top_k)
        else:
            cfg["top_k"] = 1
        cfg["system_instruction"] = system_prompt

        if "2.5" in model:
            cfg["thinking_config"] = types.ThinkingConfig(include_thoughts=True)
        
        resp = self._client.models.generate_content(
            model=model,
            contents=user_prompt,
            config=types.GenerateContentConfig(**cfg)
        )
        
        # Capture usage metadata if available
        usage_meta = getattr(resp, 'usage_metadata', None)
        if usage_meta:
            def _safe_int(val):
                try:
                    return int(val or 0)
                except Exception:
                    return 0

            prompt_toks = _safe_int(getattr(usage_meta, 'prompt_token_count', 0))
            # SDK renamed field from response_token_count â†’ candidates_token_count
            resp_toks = _safe_int(getattr(usage_meta, 'response_token_count', getattr(usage_meta, 'candidates_token_count', 0)))
            thought_toks = _safe_int(getattr(usage_meta, 'thoughts_token_count', 0))
            total_toks = _safe_int(getattr(usage_meta, 'total_token_count', 0))
            self._last_usage = {
                'prompt_tokens': prompt_toks,
                'response_tokens': resp_toks,
                'thought_tokens': thought_toks,
                'total_tokens': total_toks or (prompt_toks + resp_toks + thought_toks)
            }
        else:
            self._last_usage = {'prompt_tokens': 0, 'response_tokens': 0, 'thought_tokens': 0, 'total_tokens': 0}
        
        # Stitch together parts (Gemini returns listâ€‘ofâ€‘parts)
        thoughts = ""
        content = ""
        
        for part in resp.candidates[0].content.parts:
            if not part.text:
                continue
            if hasattr(part, 'thought') and part.thought:
                thoughts += part.text
            else:
                content += part.text
        
        # Store thoughts for potential retrieval
        self._last_thoughts = thoughts
        
        return content.strip()
    
    def get_last_thoughts(self) -> str:
        """Retrieve thinking traces from the last generate() call."""
        return getattr(self, '_last_thoughts', '')

    def get_last_usage(self) -> dict:
        return getattr(self, '_last_usage', {'prompt_tokens': 0, 'response_tokens': 0, 'total_tokens': 0}) 

## 0022. multi_coder_analysis\providers\openrouter.py
----------------------------------------------------------------------------------------------------
"""HTTP-level OpenRouter provider.

Eliminates the hard dependency on the *openai* Python package so the CLI works
out-of-the-box in minimal environments (CI, Docker).  The implementation
conforms to ``ProviderProtocol`` via duck-typing only â€“ no inheritance needed.
"""

from __future__ import annotations

import os
import logging
from typing import Optional

import requests

from .base import ProviderProtocol
from .base import track_usage

_LOGGER = logging.getLogger(__name__)

__all__ = ["OpenRouterProvider"]


class OpenRouterProvider:
    """Lightweight provider that talks to https://openrouter.ai via REST."""

    _ENDPOINT = "https://openrouter.ai/api/v1/chat/completions"

    def __init__(self, api_key: Optional[str] = None):
        self._api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        if not self._api_key:
            raise ValueError("OPENROUTER_API_KEY not set")

        self._last_usage: dict = {}
        self._last_thoughts: str = ""

    # ------------------------------------------------------------------
    # ProviderProtocol interface
    # ------------------------------------------------------------------
    @track_usage
    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        model: str,
        temperature: float = 0.0,
        *,
        top_k: int | None = None,
        top_p: float | None = None,
    ) -> str:
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": model,
            "temperature": temperature,
            **({"top_p": float(top_p)} if top_p is not None else {}),
            **({"top_k": int(top_k)} if top_k is not None else {}),
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        }

        resp = requests.post(self._ENDPOINT, json=payload, headers=headers, timeout=30)
        try:
            resp.raise_for_status()
        except Exception as exc:  # pragma: no cover
            _LOGGER.error("OpenRouter request failed: %s", exc)
            raise

        data = resp.json()

        # Save usage metadata if available (OpenAI style)
        self._last_usage = data.get("usage", {}) if isinstance(data, dict) else {}

        choice = data["choices"][0]
        self._last_thoughts = choice.get("thoughts", "")  # rarely provided

        return choice["message"]["content"].strip()

    # ------------------------------------------------------------------
    # Introspection helpers
    # ------------------------------------------------------------------
    def get_last_thoughts(self) -> str:
        return self._last_thoughts

    def get_last_usage(self) -> dict:  # noqa: D401 â€“ simple struct
        # Ensure numeric fields are ints for downstream math
        return {k: int(v) for k, v in self._last_usage.items()} if self._last_usage else {
            "prompt_tokens": 0,
            "response_tokens": 0,
            "total_tokens": 0,
        } 

## 0023. multi_coder_analysis\regex\hop_patterns.yml
----------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------
# Canonical regex catalogue for 12â€‘hop pipeline
# Fileâ€‘name:  hop_patterns.yml        (UTFâ€‘8, no tabs)
# ------------------------------------------------------------------
# Schema
#   <hop_number> (int) :
#     - name          : CamelCase identifier (unique within hop)
#       mode          : live | shadow           # default = live
#       frame         : Alarmist | Reassuring | null
#       pattern       : |-                      # block scalar, preserves NL
#           <raw regex, unchanged>
#       # veto_pattern: |-                      # optional
#           <regex that cancels a positive hit>
#
# Notes
# â€¢ Newlines are significant for readabilityâ€”do **not** reâ€‘wrap patterns.
# â€¢ Indent the block scalar exactly two spaces so YAML treats the regex
#   as literal text (nothing is escaped).
# â€¢ Keep ordering by hop â†’ pattern name; the engine preserves this order.
# ------------------------------------------------------------------

1:
- name: IntensifierRiskAdjV2
  mode: live
  frame: Alarmist
  veto_pattern: |-
    # TECH_TERM_GUARD (2025-06-20)
    (?ix)
      \b(?:
           highly\s+(?:pathogenic|contagious|transmissible|virulent) |
           high[-\s]?pathogenic |
           high[-\s]?path\B
        )\b
      (?=\s+(?:strain|bird\s+flu|avian(?:\s+flu|\s+influenza)?|h5n1|virus))
  # Added negative lookâ€‘behind to exclude "deadly toll", "deadly cost"
  pattern: |-
    # block idioms unrelated to epidemiological danger
    # add new intensifiers; block 'soâ€‘called' hedge
    (?<!toll\s|cost\s|sins\s|silence\s|so\scalled\s)
    \b(?:(?:highly(?!\s+(?:pathogenic|susceptible)\b))
        |very|deadlier|more|extremely|severely|so|alarmingly|unusually
        |particularly|frighteningly|definitely|certainly|ever[-\s]*more|progressively)  
        (?:\s+\w+){0,3}\s+
    (deadly|lethal|dangerous|severe|catastrophic|brutal|contagious|virulent|destructive|infectious|transmissible)\b|
    \bdeadly\s+from\s+(?:the\s+)?(?:start|outset)\b|
    \bmost\s+(?:\w+\s+){0,2}?(?:deadly|destructive|dangerous|severe|catastrophic|devastating|virulent|contagious|lethal)\b
    \b(?:incredibly|unbelievably|increasingly)\s+(?:\w+\s+){0,2}?(?:deadly|dangerous|severe|lethal|catastrophic|virulent|contagious)\b

- name: H1.AutoIntensifierRiskAdj
  mode: shadow
  frame: Alarmist
  veto_pattern: |-
    # TECH_TERM_GUARD (2025-06-20)
    (?ix)
      \b(?:
           highly\s+(?:pathogenic|contagious|transmissible|virulent) |
           high[-\s]?pathogenic |
           high[-\s]?path\B
        )\b
      (?=\s+(?:strain|bird\s+flu|avian(?:\s+flu|\s+influenza)?|h5n1|virus))
  pattern: 
    (?:highly|particularly)\s+(?:contagious|dangerous|deadly|infectious|lethal|transmissible)\b
2:
- name: HighPotencyVerbMetaphor
  mode: live
  frame: Alarmist
  veto_pattern: |-
    # (a) keep price-trend guard
    \btrending\s+sharply\s+(?:higher|lower)\b
    |
    # (c) macro-context guard â€“ price/temperature/inflation backdrop â‰  impact
    \bsoar(?:ed|ing)?\b(?=\s+(?:inflation|interest[-\s]?rates?|temperatures?)) |
    \bplung(?:ed|ing)\b(?=\s+(?:inflation|interest[-\s]?rates?|temperatures?)) |
    # (b) **containment override** â€“ neutralise culling verbs so they fall
    #     through to Q3 where the containment rule already handles them
    (?i)\b(?:slaughter(?:ed|ing)?|culled?|destroyed?|euthan(?:iz|is)ed|
           depopulated|disposed|buried)\b
  pattern: |-
    # Guard â€” "spark shortages" stays Neutral
    (?!\b(?:spark|sparking)\s+shortage(?:s)?\b)

    (?:
      # vivid verbs / alert phrases
      \b(?:ravaged|devastated|obliterated|skyrocketed|plummeted|crashed|nosedived|
         tanked|exploding|raging|tearing\sthrough|
         overwhelmed|crippling|spiralled?|ballooned|
         writh(?:e|ed|ing)|convuls(?:e|ed|ing)|gasp(?:ing|ed)|twitch(?:ing|ed))\b
      |
      # verb first:  soar(ed/ing) + metric inside 20 chars after
      \bsoar(?:ed|ing)?\b(?=[^.]{0,20}\b(?:cases?|prices?|costs?|loss(?:es)?|
                                    deaths?|fatalities|production|output|
                                    supply|shortages?)\b)
      |
      # metric first: metric â€¦ soar(ed/ing) inside 20 chars after
      \b(?:cases?|prices?|costs?|loss(?:es)?|deaths?|fatalities|production|
          output|supply|shortages?)\b[^.]{0,20}\bsoar(?:ed|ing)?\b
      |
      # superlative-negative nouns
      \b(?:most|record(?:-breaking)?|worst)\s+\w{0,12}?
           \s+(?:disaster|crisis|outbreak|catastrophe|calamity)\b
      |
      # potent metaphors (explicit list for deterministic hits)
      \b(?:ticking\s+time-?bomb|nightmare\s+scenario|powder\s+keg|
         house\s+of\s+cards|train\s+wreck|collateral\s+damage)\b
      |
      # "on high alert" forms
      (?:on\s+high\s+alert(?=[^.]{0,40}\b(?:outbreak|virus|flu|disease|h5n1|
                                           threat|danger|risk)\b)|
       (?:outbreak|virus|flu|disease|h5n1|threat|danger|risk)
         \b[^.]{0,40}on\s+high\s+alert|
       ^[^.]{0,60}\bon\s+high\s+alert\b)
      |
      # spark / stoke *panic-type* emotions  (plain "fears" stays Neutral)
      \b(?:spark|stoke|fuel|reignit(?:e|ing|ed|es))\s+
         (?:mass(?:ive)?\s+|widespread\s+|public\s+|nationwide\s+|
            global\s+)?(?:panic|alarm|outrage|anxiety)\b
    )

# 2025-06-20 â€¢ Zero-FP rule promoted to live
- name: OnHighAlert.Live
  mode: live
  frame: Alarmist
  pattern: (?ix)\bon\W+high\W+alert\b

3:
- name: ModerateVerbPlusScale
  mode: live
  frame: Alarmist
  pattern: |-
    \b(hit|hitting|swept|sweeping|surged|soared|plunged|plummeted|
       spiked|jumped|shot\s+up|prompted(?!\s+authorities\s+to\s+consider))\b
    (?=[^.]{0,120}\b(?:\d|%|percent|hundred|hundreds|thousand|thousands|million|millions|billion|billions|record|
                     largest|unprecedented|severe|significant|overwhelming|
                     devastating|disasters?|emergenc(?:y|ies))\b)
    (?![^.]{0,20}\bfear(?:s|ed|ing)?\b)   # guard: psychological verbs â‰  impact

    # 2025-06-18 containment-verb veto â€“ keeps large-scale culling Neutral
  veto_pattern: |-
    # extend veto to "disposed" and "buried"
    (?i)\b(?:culled?|euthani[sz]ed|destroyed|depopulated|slaughtered|disposed|buried)\b

- name: ScaleMultiplier
  mode: live
  frame: Alarmist
  pattern: |-
    (?i)\b(?:double[ds]?|triple[ds]?|quadruple[ds]?|ten[-\s]*fold)\b

4:
- name: LoadedQuestionAlarm
  mode: live
  frame: Alarmist
  pattern: |-
    \b(?:should|can|could|will)\s+\w+\s+(?:be\s+)?(?:worried|concerned|afraid)\b
    (?=[^.?]{0,40}\b(?:outbreak|virus|flu|disease|h5n1|threat|danger|
                     risk|infection|infected)\b)
    |
    # Rhetorical necessity-of-killing question (captures 'necessary to kill millions...')
    \b(?:is|are|was|were)\s+it\s+(?:really\s+)?necessary\s+to\s+
        (?:kill|cull|slaughter|destroy|euthan(?:ize|ise))\b
        [^?]{0,60}?\b(?:millions?|thousands?|record|\d{1,3}(?:[, ]\d{3})+)\b

  # --- 2025-06-18 addition: Challenge-question over inaction ---

- name: WhatIfQuestion
  mode: live
  frame: Alarmist
  pattern: |-
    (?i)\bwhat\s+if\s+(?:we|this|the\s+\w+)\b

- name: IgnoreDisasterQ
  mode: live
  frame: Alarmist
  pattern: |-
    (?i)\bhow\s+long\s+can\s+we\s+(?:afford\s+to\s+)?(?:ignore|stand\s+by)\b

5:
- name: ExplicitCalming
  mode: live
  frame: Reassuring
  pattern: |-
    \bwe\b(?:\s*\[[^\]]+\]\s*)?\s+(?:are|remain|feel)\s+
        (?:confident|positive|optimistic|hopeful)\s+in\s+(?:\w+\s+){0,8}?(?:preparedness|readiness|ability)\b|
    \b(?:are|feel)\s+(?:confident|positive|optimistic|hopeful)\s+in\s+(?:\w+\s+){0,8}?(?:preparedness|readiness|ability)\b|
    \b(?:no\s+cause\s+for\s+alarm|
        public\s+can\s+rest\s+easy|
        fully\s+under\s+control|
        rest\s+assured|
        completely\s+safe|
        (risk|likelihood|chance)\s+(?:of\s+\w+\s+)?(?:is|are|remains|stay|stays)\s+(?:very|extremely|exceptionally|remarkably)\s+low|
        (?:is|are|remains|remain|stay|stays)\s+(?:completely\s+|totally\s+|perfectly\s+|entirely\s+)?safe\s+(?:to\s+eat|for\s+(?:human\s+)?consumption|for\s+(?:all\s+)?(?:consumers?|people|humans|residents|citizens))|
        \b(?:encouraging|welcome|heartening)\s+news\b|
        (?:fully|well)\s+(?:prepared|ready)\s+(?:to\s+(?:handle|deal\s+with)|for)
          .{0,40}\b(?:public|consumers?|customers?|people|residents|citizens)\b|
        \b[Ff]ortunately\b[^.]{0,40}\b(?:consumers?|public|residents|citizens|people)\b)
    | # generic optimism/confidence without "in X" clause
    \b(?:i|we|they|officials?|authorities?)\s+
      (?:feel|are)\s+
      (?:positive|optimistic|hopeful)\s+
      (?:that|about)\b

- name: ExplicitCalming.SafeToEat.Live
  mode: live
  frame: Reassuring
  pattern: |
    \b(remains?|is|are)\s+(?:perfectly\s+)?safe\s+(?:to\s+eat|for\s+(?:human\s+)?consumption)\b

# 2025-06-20 â€¢ Zero-FP rules promoted to live
- name: DirectNoConcern.Live
  mode: live
  frame: Reassuring
  pattern: (?ix)\bno\W+cause\W+for\W+(?:alarm|concern)\b

- name: NothingToWorry.Live
  mode: live
  frame: Reassuring
  pattern: (?ix)\bnothing\W+to\W+worry\W+about\b

- name: LowRiskEval.Theoretical
  mode: live
  frame: Reassuring
  pattern: (?i)\b(?:purely\s+)?theoretical\s+risk\b

6:
- name: MinimiserScaleContrast
  mode: live
  frame: Reassuring
  pattern: |-
    # minimiser MUST be paired with an explicit denominator token
    \b(?:only|just|merely|a\s+single|very\s+few|relatively\s+few)\b
         [^.;\n]{0,30}
         \b(?:out\s+of|one\s+of|one\s+in|among|nationwide|statewide|across|worldwide|globally)\b
         [^.;\n]{0,30}\b(?:hundred|thousand|million|billion|
                        \d{1,3}(?:[, ]\d{3})*|\d+)\b
    |
    \b(?:only|just|merely)\s+\d+(?:[.,]\d+)?\s*(?:%|percent|per\s+cent)\b
    |
    \b(?:only|just|merely)\s+one\b[^.]{0,120}
         \b(?:of|in|among)\b[^.]{0,20}\bthousands?\b
    |
    # allow dash or parenthesis between parts
    \b(?:only|just|merely|a\s+single|very\s+few)\b
         [^.;\n]{0,50}?\b                  # tolerant gap
         \b(out\s+of|among|in|of)\b
         [^.;\n]{0,50}?\b(total|overall|population|flocks?|barns?|nationwide|worldwide|global(?:ly)?)\b

7:
- name: BareNegationHealthConcern
  mode: live
  frame: Neutral
  pattern: |-
    \b(?:do|does|did|is|are|was|were|will|would|should)\s+(?:not|n't)\s+
       (?:pose|present|constitute)\s+(?:an?\s+)?(?:immediate\s+)?(?:public\s+)?health\s+concern\b
    |
    \bno\s+(?:human|americans?|animal|bird|poultry)\s+cases?\s+
       (?:have|has|are|were)\s+(?:been\s+)?(?:detected|reported|
                                          recorded|found|identified)\b
    |
    \b(?:will|would|can|could)\s+not\s+enter\s+the\s+food\s+(?:system|chain|supply)\b
    |
    \b(?:tests?|samples?)\s+(?:came|come|were|was)\s+negative\b
    |
    \b(?:does|do|is|are|will|would|should)\s+(?:not|n't)\s+
       pose\s+(?:an?\s+)?(?:any\s+)?risk\b
    |
    \b(?:pose|present|constitute)\s+no\s+(?:public\s+)?health\s+(?:threat|risk)\b
    |
    \bno\s+(?:sign|indication|evidence)\s+of\s+spread\b
    |
    \b(?:has|have|had|did)\s+(?:not|n't)\s+
        detect(?:ed)?\s+(?:any\s+)?(?:further|additional|new)\s+
        (?:positive\s+)?samples?\b

- name: BareNegation.PosesNoRisk.Live
  mode: live
  frame: Neutral
  pattern: |
    \b(?:poses?|present(?:s)?)\s+no\s+risk\b

- name: BareNegation.NotContaminate.Live
  mode: live
  frame: Neutral
  # 2025-06-20 â€¢ broadened to cover "has not detected / identified â€¦ cases"
  pattern: |-
    (?ix)
    \b(?:has|have|does|do|did)\b
    \s+(?:not|n't|never|yet\s+to)\s+
    (?:contaminat(?:e|ed)|infect(?:ed)?)\b

# NEW â€” 2025-06-20 -----------------------------------------------------------
- name: BareNegation.NoFurtherCases.Live
  mode: live
  frame: Neutral
  pattern: |-
    (?i)\b(?:has|have|had|did)\s+(?:not|n't)\s+
         (?:detected|identified|reported|found)\s+
         (?:any\s+)?(?:further|additional|new)\s+cases?\b

# NEW â€” 2025-06-20 -----------------------------------------------------------  
- name: BareNegation.NothingSuggests.Live
  mode: live
  frame: Neutral
  pattern: |-
    (?ix)
    \bnothing\s+(?:currently\s+)?(?:in\s+the\s+)?
    (?:data|evidence|sequence|analysis|results)?\s*suggests?
    \s+(?:that\s+)?(?:the\s+)?(?:virus|situation)?\s+
    (?:has\s+become|is|will\s+be|has\s+grown)\s+
    (?:more\s+)?(?:dangerous|contagious|infectious|severe|deadly|threatening)\b

# 2025-06-20 â€¢ Zero-FP rule promoted to live
- name: NoThreatNeutral.Live
  mode: live
  frame: Neutral
  pattern: (?ix)\bdoes\W+not\W+pose\W+a?\W+threat\b

8:
- name: CapabilityNoReassurance
  mode: live
  frame: Neutral
  pattern: |-
    \b(?:(?:fully|well)\s+(?:prepared|ready)\s+(?:to\s+(?:handle|deal\s+with)|for)\b(?![^.]{0,40}\b(?:public|consumers?|customers?|people|residents|citizens)\b)|(?:officials|vaccine|system|plan|protocol|measure|safeguard|capability|prepare|develop|implement|work|contain)\b)\b

9:
- name: NeutralPriceMetrics
  mode: live
  frame: Neutral
  pattern: |-
    (?is)
    \b(?:
          # economic nouns
          (?:prices?|rates?|costs?|loss(?:es)?|profit(?:s)?|revenue|
             value|export(?:s)?|import(?:s)?|sale(?:s)?|output|production)
          \b[^.]{0,120}?                     # allow anything up to the verb (â‰¤ one sentence)
          (?:rose|declined|increased|fell|dropped|gained|lost)\b
        | # "prices were up /down 2 %" form
          (?:prices?|rates?)\s+(?:were|was)\s+(?:up|down)\s+\d+(?:[.,]\d+)?\s*%
        | # PATCH 2b â€“ claim "trending sharply higher/lower" as Neutral
          \b(?:prices?|costs?|rates?|values?|export(?:s)?|import(?:s)?|sale(?:s)?)
            [^.]{0,50}?\btrending\s+sharply\s+(?:higher|lower)\b
    )

10:
- name: ReliefSpeculation
  mode: live
  frame: Neutral
  pattern: |-
    \b(may\ be|could|might|expect.{1,15}improve|predict.{1,15}ease|hope.{1,15}better)\b

11:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  #  Hop 11 â€“ "Primacy of Framed Quotations"
  #  Two explicit patterns:
  #    â€¢ DominantQuoteAlarmist     â†’ frame: Alarmist
  #    â€¢ DominantQuoteReassuring   â†’ frame: Reassuring
  #  First match wins; if both miss, the LLM prompt executes.
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

- name: DominantQuoteAlarmist
  mode: live
  frame: Alarmist
  veto_pattern: |-
    (?i)\bhighly\s+pathogenic\s+(?:avian\s+flu|influenza|avian)\b
  pattern: |-
    (?is)                                    # i=ignore case, s=dot=nl
    ["'\u2018\u2019\u201C\u201D]             # opening quote (straight or curly)
    [^"'\u2018\u2019\u201C\u201D]{0,600}?    # up to 600 chars inside
    \b(?:                                     # key alarmist cues
         (?:extremely|highly|very|deeply|incredibly|particularly|
            frighteningly|definitely|certainly)\s+\w{0,3}\s+
               (?:concerning|alarming|worrying|dangerous|severe|
                  catastrophic|critical|high[-\s]*risk)
       | period\s+of\s+high[-\s]*risk
       | requires\s+immediate\s+action
       | (?:troubling|dire)\s+situation
      )\b
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    ["'\u2018\u2019\u201C\u201D]             # closing quote

- name: DominantQuoteReassuring
  mode: live
  frame: Reassuring
  pattern: |-
    (?is)
    ["'\u2018\u2019\u201C\u201D]
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    \b(?:                                     # key reassuring cues
         no\s+cause\s+for\s+alarm
       | fully\s+under\s+control
       | excellent\s+news
       | very\s+well\s+protected
       | risk\s+(?:is|remains|stays)\s+
             (?:very|extremely|exceptionally|remarkably)\s+low
       | (?:completely|totally|entirely|perfectly|absolutely)\s+safe
       | wholly\s+under\s+control
       | safe\s+to\s+eat
      )\b
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    ["'\u2018\u2019\u201C\u201D]

12:
- name: NeutralStats
  mode: live
  frame: Neutral
  pattern: |-
    \b(report|document|state|announce|confirm|detect|identify|record)\w*\b.*\b(cases|deaths|losses|rates|numbers|percent)\b


## 0024. multi_coder_analysis\runtime\cli.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Command-line interface entry point (Phase 7)."""

import typer
from pathlib import Path
from typing import Optional

from multi_coder_analysis.config.run_config import RunConfig
from multi_coder_analysis.runtime.tot_runner import execute

app = typer.Typer(help="Multi-Coder Analysis toolkit (ToT refactor)")


@app.command()
def run(
    input_csv: Path = typer.Argument(..., help="CSV file with statements"),
    output_dir: Path = typer.Argument(..., help="Directory for outputs"),
    provider: str = typer.Option("gemini", help="LLM provider: gemini|openrouter"),
    model: str = typer.Option("models/gemini-2.5-flash-preview-04-17", help="Model identifier"),
    batch_size: int = typer.Option(
        1,
        min=1,
        help="(DEPRECATED: pipeline mode is non-batching) Batch size for legacy LLM calls",
    ),
    concurrency: int = typer.Option(1, min=1, help="Thread pool size"),
    regex_mode: str = typer.Option("live", help="Regex mode: live|shadow|off"),
    shuffle_batches: bool = typer.Option(False, help="Randomise batch order"),
    phase: str = typer.Option(
        "pipeline",
        "--phase",
        help="Execution mode: legacy | pipeline",
        rich_help_panel="Execution mode",
    ),
    consensus: str = typer.Option(
        "final",
        help="Consensus mode: 'hop' for per-hop majority, 'final' for end-of-tree",
        rich_help_panel="Consensus",
    ),
    # ---- self-consistency decoding flags ----
    decode_mode: str = typer.Option(
        "normal",
        "--decode-mode",
        "-m",
        help="normal | self-consistency",
    ),
    votes: int = typer.Option(1, "--votes", "-n", help="# paths/votes for self-consistency"),
    sc_rule: str = typer.Option("majority", "--sc-rule", help="majority | ranked | ranked-raw"),
    sc_temperature: float = typer.Option(0.7, "--sc-temperature", help="Sampling temperature"),
    sc_top_k: int = typer.Option(40, "--sc-top-k", help="top-k sampling cutoff (0 disables)"),
    sc_top_p: float = typer.Option(0.95, "--sc-top-p", help="nucleus sampling p value"),
):
    """Run the deterministic Tree-of-Thought coder."""

    cfg = RunConfig(
        input_csv=input_csv,
        output_dir=output_dir,
        provider=provider,
        model=model,
        batch_size=batch_size,
        concurrency=concurrency,
        regex_mode=regex_mode,
        shuffle_batches=shuffle_batches,
        phase=phase,
        consensus_mode=consensus,
        decode_mode=decode_mode,
        sc_votes=votes,
        sc_rule=sc_rule,
        sc_temperature=sc_temperature,
        sc_top_k=sc_top_k,
        sc_top_p=sc_top_p,
    )
    execute(cfg)


if __name__ == "__main__":
    app() 

## 0025. multi_coder_analysis\runtime\tot_runner.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Runtime orchestrator for the Tree-of-Thought pipeline.

This thin wrapper bridges the *runtime* layer (CLI / env / I/O) with the
*core* pipeline logic implemented in :pyfunc:`multi_coder_analysis.run_multi_coder_tot`.
"""

import logging
from pathlib import Path
import threading
import sys
import json

from multi_coder_analysis.config.run_config import RunConfig
from multi_coder_analysis import run_multi_coder_tot as tot
from multi_coder_analysis.config import load_settings
from multi_coder_analysis.core.pipeline.tot import build_tot_pipeline
from multi_coder_analysis.core.pipeline.consensus_tot import build_consensus_pipeline
from multi_coder_analysis.core.regex import Engine
from multi_coder_analysis.providers import get_provider
from multi_coder_analysis.providers.base import get_usage_accumulator
from multi_coder_analysis.models import HopContext
from multi_coder_analysis.utils.tie import is_perfect_tie
from datetime import datetime

__all__ = ["execute"]


def execute(cfg: RunConfig) -> Path:
    """Execute the ToT pipeline according to *cfg* and return the output CSV path.

    The function merges any values present in legacy `config.yaml` (loaded via
    `load_settings()`) but **explicit CLI/RunConfig values win**.
    """

    # Merge deprecated YAML settings for backwards compatibility
    legacy = load_settings().dict()
    merged_data = {**legacy, **cfg.dict(exclude_unset=True)}  # CLI overrides YAML
    cfg = RunConfig(**merged_data)

    logging.info(
        "Starting ToT run: provider=%s, model=%s, batch=%s, concurrency=%s",
        cfg.provider,
        cfg.model,
        cfg.batch_size,
        cfg.concurrency,
    )

    cfg.output_dir.mkdir(parents=True, exist_ok=True)

    # regex counters collected via Regex Engine; usage stats via providers.base.track_usage
    token_accumulator = get_usage_accumulator()
    token_lock = threading.Lock()

    if cfg.phase == "pipeline":
        provider = cfg.provider
        provider_inst = get_provider(provider)

        # --------------------------------------------------
        # Configure regex engine:** live / shadow / off **
        # --------------------------------------------------
        engine = Engine.default()
        mode = cfg.regex_mode.lower()
        if mode == "off":
            engine.set_global_enabled(False)
        elif mode == "shadow":
            engine.set_global_enabled(True)
            engine.set_force_shadow(True)
        else:  # "live" (default)
            engine.set_global_enabled(True)
            engine.set_force_shadow(False)

        # ---------- Self-consistency mode -----------
        if cfg.decode_mode == "self-consistency":
            from multi_coder_analysis.core.self_consistency import decode_paths, aggregate

            import pandas as pd
            from datetime import datetime

            df = pd.read_csv(cfg.input_csv, dtype={"StatementID": str})

            from concurrent.futures import ThreadPoolExecutor, as_completed

            results = []

            def _process_row(row_data):
                local_provider = get_provider(cfg.provider)
                base_ctx = HopContext(
                    statement_id=row_data["StatementID"],
                    segment_text=row_data["Statement Text"],
                    article_id=row_data.get("ArticleID", ""),
                )

                pairs = decode_paths(
                    base_ctx,
                    local_provider,
                    cfg.model,
                    votes=cfg.sc_votes,
                    temperature=cfg.sc_temperature,
                    top_k=cfg.sc_top_k,
                    top_p=cfg.sc_top_p,
                )

                frame, conf = aggregate(pairs, rule=cfg.sc_rule)

                # Usage already counted via @track_usage decorator

                return {
                    "StatementID": base_ctx.statement_id,
                    "Frame": frame,
                    "Consistency": f"{conf:.2f}",
                }

            with ThreadPoolExecutor(max_workers=cfg.concurrency) as exe:
                future_to_row = {
                    exe.submit(_process_row, row): row for _, row in df.iterrows()
                }

                for fut in as_completed(future_to_row):
                    results.append(fut.result())

            out_df = pd.DataFrame(results)
            out_path = cfg.output_dir / f"sc_results_{datetime.now():%Y%m%d_%H%M%S}.csv"
            out_df.to_csv(out_path, index=False)

            logging.info("Self-consistency run completed âžœ %s", out_path)

            # --- parameter summary ---
            _write_param_summary(cfg, cfg.output_dir)
            return out_path

        # 'permute' mode deprecated until implemented; behaves like 'normal'

        # ---------- Normal / permute path (default) -----------

        if cfg.consensus_mode == "hop":
            pipeline, hop_var = build_consensus_pipeline(
                provider_inst,
                cfg.model,
                top_k=(None if cfg.sc_top_k == 0 else cfg.sc_top_k),
                top_p=cfg.sc_top_p,
            )
        else:
            pipeline = build_tot_pipeline(
                provider_inst,
                cfg.model,
                top_k=(None if cfg.sc_top_k == 0 else cfg.sc_top_k),
                top_p=cfg.sc_top_p,
            )
            hop_var = {}

        # Minimal CSV writer â€“ mirror legacy layout
        import pandas as pd

        df = pd.read_csv(cfg.input_csv, dtype={"StatementID": str})
        contexts = []
        for _, row in df.iterrows():
            if cfg.consensus_mode == "hop":
                # Generate simple k permutations (identical copies)
                k = 3  # minimal safe default
                perms = [
                    HopContext(
                        statement_id=row["StatementID"],
                        segment_text=row["Statement Text"],
                        article_id=row.get("ArticleID", ""),
                        permutation_idx=i,
                    )
                    for i in range(k)
                ]
                survivors = pipeline.run(perms)

                if not survivors:
                    # All permutations tied and were removed â€“ record placeholder
                    tie_ctx = HopContext(
                        statement_id=row["StatementID"],
                        segment_text=row["Statement Text"],
                        article_id=row.get("ArticleID", ""),
                        final_frame="tie",
                        is_concluded=True,
                    )
                    contexts.append(tie_ctx)
                else:
                    # Pick first concluded context or default to first survivor
                    rep_ctx = next((c for c in survivors if c.is_concluded), survivors[0])
                    contexts.append(rep_ctx)
            else:
                ctx = HopContext(
                    statement_id=row["StatementID"],
                    segment_text=row["Statement Text"],
                    article_id=row.get("ArticleID", ""),
                )
                pipeline.run(ctx)
                contexts.append(ctx)

            # Usage already counted by decorator

        # Convert to DataFrame
        out_rows = [
            {
                "StatementID": c.statement_id,
                "Frame": c.final_frame,
                "Justification": c.final_justification,
            }
            for c in contexts
        ]
        out_df = pd.DataFrame(out_rows)
        out_path = cfg.output_dir / f"tot_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        out_df.to_csv(out_path, index=False)

        # Save variability logs if present
        if hop_var:
            variab_path = cfg.output_dir / f"hop_variability_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            import json
            variab_path.write_text(json.dumps(hop_var, indent=2, ensure_ascii=False))

            # also save tie segments
            ties_csv = cfg.output_dir / f"tie_segments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            import csv
            with ties_csv.open("w", newline="", encoding="utf-8") as fh:
                w = csv.writer(fh)
                w.writerow(["StatementID", "Hop", "Distribution"])
                for hop, rows_ in hop_var.items():
                    for sid, dist in rows_:
                        if is_perfect_tie(dist):
                            w.writerow([sid, hop, json.dumps(dist)])

        logging.info(
            "LLM stats â€“ calls=%s prompt=%s response=%s total=%s",
            token_accumulator.get("llm_calls", 0),
            token_accumulator.get("prompt_tokens", 0),
            token_accumulator.get("response_tokens", 0),
            token_accumulator.get("total_tokens", 0),
        )

        # --- parameter summary ---
        _write_param_summary(cfg, cfg.output_dir)
        return out_path

    # --- Legacy path (default) ---
    _, output_csv = tot.run_coding_step_tot(
        config={},  # legacy param kept for compatibility
        input_csv_path=cfg.input_csv,
        output_dir=cfg.output_dir,
        concurrency=cfg.concurrency,
        model=cfg.model,
        provider=cfg.provider,
        batch_size=cfg.batch_size,
        regex_mode=cfg.regex_mode,
        shuffle_batches=cfg.shuffle_batches,
        token_accumulator=token_accumulator,  # type: ignore[arg-type]
        token_lock=token_lock,  # type: ignore[arg-type]
    )

    logging.info("ToT run completed âžœ %s", output_csv)
    return Path(output_csv)


# ------------------------------------------------------------
# Helper: write comprehensive parameters summary for the run
# ------------------------------------------------------------


def _write_param_summary(cfg: RunConfig, output_dir: Path) -> None:
    """Dump command-line and full RunConfig to JSON in *output_dir*."""

    summary = {
        "timestamp": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "command_line": " ".join(sys.argv),
        "parameters": cfg.dict(),
    }

    out_file = output_dir / "parameters_summary.json"
    out_file.write_text(json.dumps(summary, indent=2, ensure_ascii=False, default=str), encoding="utf-8") 

## 0026. multi_coder_analysis\runtime\tracing.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Structured logging and tracing utilities (Phase 8)."""

import json
import logging
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

import structlog

__all__ = ["setup_logging", "get_logger", "TraceWriter"]

# Global run ID for this process
_RUN_ID = str(uuid.uuid4())


def setup_logging(level: str = "INFO", json_logs: bool = False) -> None:
    """Configure structured logging for the application.
    
    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR)
        json_logs: If True, emit JSON-formatted logs
    """
    
    # Configure standard library logging
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s" if not json_logs else None,
    )
    
    # Configure structlog
    processors = [
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
    ]
    
    if json_logs:
        processors.append(structlog.processors.JSONRenderer())
    else:
        processors.extend([
            structlog.dev.ConsoleRenderer(),
        ])
    
    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(
            getattr(logging, level.upper())
        ),
        logger_factory=structlog.WriteLoggerFactory(),
        cache_logger_on_first_use=True,
    )


def get_logger(name: str) -> structlog.BoundLogger:
    """Get a structured logger instance.
    
    Args:
        name: Logger name (typically __name__)
        
    Returns:
        Configured structlog logger
    """
    return structlog.get_logger(name).bind(run_id=_RUN_ID)


class TraceWriter:
    """NDJSON trace writer with envelope metadata."""
    
    def __init__(self, trace_dir: Path):
        self.trace_dir = Path(trace_dir)
        self.trace_dir.mkdir(parents=True, exist_ok=True)
        self._run_id = _RUN_ID
        
    def write_trace(self, statement_id: str, trace_data: Dict[str, Any]) -> None:
        """Write a single trace entry.
        
        Args:
            statement_id: Unique identifier for the statement
            trace_data: Trace payload data
        """
        envelope = {
            "run_id": self._run_id,
            "statement_id": statement_id,
            "timestamp": datetime.now().isoformat(),
            "trace_data": trace_data,
        }
        
        trace_file = self.trace_dir / f"{statement_id}.ndjson"
        with trace_file.open("a", encoding="utf-8") as f:
            f.write(json.dumps(envelope, ensure_ascii=False) + "\n")
    
    def write_batch_trace(self, batch_id: str, hop_idx: int, batch_data: Dict[str, Any]) -> None:
        """Write a batch trace entry.
        
        Args:
            batch_id: Unique identifier for the batch
            hop_idx: Hop number (1-12)
            batch_data: Batch processing data
        """
        envelope = {
            "run_id": self._run_id,
            "batch_id": batch_id,
            "hop_idx": hop_idx,
            "timestamp": datetime.now().isoformat(),
            "batch_data": batch_data,
        }
        
        batch_dir = self.trace_dir / "batches"
        batch_dir.mkdir(exist_ok=True)
        batch_file = batch_dir / f"{batch_id}_Q{hop_idx:02d}.ndjson"
        
        with batch_file.open("a", encoding="utf-8") as f:
            f.write(json.dumps(envelope, ensure_ascii=False) + "\n")
    
    def write_run_summary(self, summary_data: Dict[str, Any]) -> None:
        """Write a run-level summary.
        
        Args:
            summary_data: Summary statistics and metadata
        """
        envelope = {
            "run_id": self._run_id,
            "timestamp": datetime.now().isoformat(),
            "summary_data": summary_data,
        }
        
        summary_file = self.trace_dir / f"run_summary_{self._run_id}.ndjson"
        with summary_file.open("w", encoding="utf-8") as f:
            f.write(json.dumps(envelope, ensure_ascii=False) + "\n")


# Legacy compatibility adapters
def write_trace_log(trace_dir: Path, statement_id: str, trace_entry: Dict[str, Any]) -> None:
    """Legacy adapter for existing trace writing code."""
    writer = TraceWriter(trace_dir)
    writer.write_trace(statement_id, trace_entry)


def write_batch_trace(trace_dir: Path, batch_id: str, hop_idx: int, batch_payload: Dict[str, Any]) -> None:
    """Legacy adapter for existing batch trace writing code."""
    writer = TraceWriter(trace_dir)
    writer.write_batch_trace(batch_id, hop_idx, batch_payload) 

## 0027. multi_coder_analysis\utils\tie.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Utility helpers for voting distributions."""

from typing import Dict

__all__ = ["is_perfect_tie"]


def is_perfect_tie(dist: Dict[str, int]) -> bool:
    """Return True when *dist* has an exact 50-50 split.

    Works for any number of labels (2-way, 3-way â€¦) as long as the most
    frequent label accounts for exactly half of the votes.
    """
    if not dist:
        return False
    votes = list(dist.values())
    return max(votes) * 2 == sum(votes) 

====================================================================================================
# End of snapshot â€” 27 files
