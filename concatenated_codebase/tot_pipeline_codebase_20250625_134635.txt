# ToT Pipeline Codebase Snapshot — generated 2025-06-25T13:46:35
# Pipeline: python -m multi_coder_analysis.main --use-tot --provider gemini --regex-mode live
====================================================================================================

## 0001. config.yaml
----------------------------------------------------------------------------------------------------
file_paths:
  file_patterns:
    model_majority_output: '{phase}_model_labels.csv'
logging:
  level: INFO
runtime_flags:
  enable_regex: true


## 0002. multi_coder_analysis\__init__.py
----------------------------------------------------------------------------------------------------
"""multi_coder_analysis package

Light-weight namespace placeholder.  No heavy imports at module load time.
"""

from importlib import import_module as _import_module

# Re-export commonly used domain models at the package root for convenience
HopContext = _import_module("multi_coder_analysis.models.hop").HopContext  # type: ignore[attr-defined]
BatchHopContext = _import_module("multi_coder_analysis.models.hop").BatchHopContext  # type: ignore[attr-defined]

__all__ = [
    "HopContext",
    "BatchHopContext",
    "__version__",
]

# Semantic version for package consumers
__version__ = "0.5.2.dev0" 

## 0003. multi_coder_analysis\config.yaml
----------------------------------------------------------------------------------------------------
file_paths:
  file_patterns:
    model_majority_output: '{phase}_model_labels.csv'
logging:
  level: INFO
provider: gemini  # Default provider: gemini or openrouter


## 0004. multi_coder_analysis\config\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Configuration loading utilities (Phase 6)."""

import warnings
from functools import lru_cache
from pathlib import Path
import yaml

# ---------------------------------------------------------------------------
# Optional dependency guard – ``Settings`` relies on *pydantic-settings* which
# may be absent in minimal runtime environments.  We fall back to a dummy class
# that behaves like an empty mapping so that modules which only import
# ``multi_coder_analysis.config`` for its *side effects* (i.e. our permutation
# workers) do not crash.  Full-feature runs that *need* Settings should add the
# dependency as usual:  pip install pydantic-settings
# ---------------------------------------------------------------------------

try:
    from .settings import Settings  # noqa: F401
except ModuleNotFoundError as _e:
    if _e.name == "pydantic_settings":
        warnings.warn(
            "pydantic_settings not installed – falling back to minimal Settings stub.",
            RuntimeWarning,
            stacklevel=2,
        )

        class Settings(dict):  # type: ignore
            """Minimal stub that accepts **kwargs and stores them."""

            def __init__(self, **kwargs):
                super().__init__(**kwargs)

            # maintain attribute access semantics used elsewhere
            def __getattr__(self, item):
                return self.get(item)

            def dict(self):  # mimic Pydantic API subset
                return dict(self)

    else:
        raise

_CFG_PATH = Path.cwd() / "config.yaml"


@lru_cache(maxsize=1)
def load_settings(path: Path | None = None) -> Settings:  # noqa: D401
    """Load settings from *path* or environment overrides.
    
    If *config.yaml* is detected, it is parsed and **deprecated** – values are
    fed into the new Pydantic Settings model and a warning is issued.
    """
    cfg_path = Path(path) if path else _CFG_PATH

    if cfg_path.exists():
        warnings.warn(
            "Reading legacy config.yaml is deprecated; migrate to environment variables or TOML config.",
            DeprecationWarning,
            stacklevel=2,
        )
        try:
            data = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
        except Exception as e:
            warnings.warn(f"Could not parse {cfg_path}: {e}")
            data = {}
    else:
        data = {}

    return Settings(**data) 

## 0005. multi_coder_analysis\config\run_config.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from pathlib import Path
from pydantic import BaseModel, Field, validator, root_validator

__all__ = ["RunConfig"]


class RunConfig(BaseModel):
    """Central runtime configuration for ToT execution."""

    phase: str = Field(
        "pipeline",
        pattern="^(legacy|pipeline)$",
        description="Execution mode: 'legacy' = old monolithic runner, 'pipeline' = new modular ToT stack",
    )
    dimension: str | None = Field(
        None,
        description="(deprecated) reserved for backward compatibility; ignored by pipeline",
    )
    input_csv: Path = Field(..., description="Path to input CSV of statements")
    output_dir: Path = Field(..., description="Directory to write outputs")
    provider: str = Field("gemini", pattern="^(gemini|openrouter)$", description="LLM provider to use")
    model: str = Field("models/gemini-2.5-flash-preview-04-17", description="Model identifier")
    batch_size: int = Field(1, ge=1, description="Batch size for LLM calls")

    # Worker-local file suffix for archive (permutation tag, etc.)
    archive_tag: str | None = Field(
        None,
        description="Optional tag appended to run_id for per-worker archive files",
    )

    concurrency: int = Field(1, ge=1, description="Thread pool size for batch mode")
    regex_mode: str = Field("live", pattern="^(live|shadow|off)$", description="Regex layer mode")
    shuffle_batches: bool = Field(False, description="Randomise batch order for load spreading")
    consensus_mode: str = Field(
        "final",
        pattern="^(hop|final)$",
        description="Consensus strategy: 'hop' = per-hop majority, 'final' = legacy end-of-tree vote",
    )

    # ---------------- Self-consistency decoding ----------------
    decode_mode: str = Field(
        "normal",
        pattern="^(normal|self-consistency)$",
        description="Decoding mode: normal = single path, self-consistency = multi-path sampling + voting",
    )

    sc_votes: int = Field(1, ge=1, le=200, description="Number of sampled paths for self-consistency")
    sc_rule: str = Field(
        "majority",
        pattern=(
            r"^(majority|"                     # legacy
            r"ranked|ranked-raw|"              # legacy weighted
            r"irv|borda|mrr)$"                 # new ranked-list rules
        ),
        description=(
            "Aggregation rule:\n"
            "  • majority        – single-answer hard vote\n"
            "  • ranked          – single-answer length-norm\n"
            "  • ranked-raw      – single-answer raw score\n"
            "  • irv|borda|mrr   – ranked-list self-consistency",
        ),
    )
    sc_top_k: int = Field(40, ge=0, description="top-k sampling cutoff (0 disables)")
    sc_top_p: float = Field(0.95, ge=0.0, le=1.0, description="nucleus sampling p-value")
    sc_temperature: float = Field(0.7, ge=0.0, description="Sampling temperature for self-consistency")

    # housekeeping – whether tot_runner should copy & concatenate the prompts
    copy_prompts: bool = Field(True, description="Copy prompt folder into output_dir and concatenate prompts.txt")

    # ───────── Ranked-list decoding ─────────
    ranked_list: bool = Field(
        False,
        description=(
            "If true, prompts instruct model to emit an ordered list "
            "of candidate answers instead of a single value.",
        ),
    )
    max_candidates: int = Field(
        5,
        ge=1,
        le=10,
        description="Max candidates to keep from the ranked list. "
                    "Ignored when ranked_list == False.",
    )

    # ─────────────────────────────────────────────────────────────
    # Root-level validator – field order requires this approach.
    # ─────────────────────────────────────────────────────────────
    @root_validator(skip_on_failure=True)
    def _validate_ranked_combo(cls, values):  # noqa: D401
        ranked = values.get("ranked_list", False)
        rule = values.get("sc_rule")
        decode_mode = values.get("decode_mode")
        if ranked:
            if decode_mode != "self-consistency":
                raise ValueError("ranked_list=True requires decode_mode='self-consistency'")
            if rule not in {"irv", "borda", "mrr"}:
                raise ValueError(
                    "When ranked_list=True, sc_rule must be one of {'irv', 'borda', 'mrr'}"
                )
        return values

    @validator("output_dir", pre=True)
    def _expand_output_dir(cls, v):  # noqa: D401
        return Path(v).expanduser().absolute()

    @validator("input_csv", pre=True)
    def _expand_input_csv(cls, v):  # noqa: D401
        return Path(v).expanduser().absolute()

    # ----- deprecations -----
    @validator("decode_mode", pre=True)
    def _alias_perm(cls, v):  # noqa: D401
        if v == "permute":
            import warnings
            warnings.warn(
                "decode_mode='permute' is deprecated and treated as 'normal'.",
                DeprecationWarning,
                stacklevel=2,
            )
            return "normal"
        return v 

## 0006. multi_coder_analysis\config\settings.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Pydantic Settings model for configuration (Phase 6)."""

from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional
from pathlib import Path

__all__ = ["Settings"]


class Settings(BaseSettings):
    """Application settings with environment variable overrides.
    
    Environment variables are prefixed with MCA_ (e.g., MCA_PROVIDER=openrouter).
    """
    
    # Core execution settings
    phase: str = Field("pipeline", description="Pipeline phase label")
    provider: str = Field("gemini", description="LLM provider name")
    model: str = Field("models/gemini-2.5-flash-preview-04-17", description="Model identifier")
    
    # Performance settings
    batch_size: int = Field(10, ge=1, description="Batch size for LLM calls")
    concurrency: int = Field(1, ge=1, description="Thread pool size")
    
    # Feature flags
    enable_regex: bool = Field(True, description="Enable regex short-circuiting")
    regex_mode: str = Field("live", description="Regex mode: live|shadow|off")
    shuffle_batches: bool = Field(False, description="Randomise batch order")
    
    # API keys (optional - can be set via environment)
    google_api_key: Optional[str] = Field(None, description="Google Gemini API key")
    openrouter_api_key: Optional[str] = Field(None, description="OpenRouter API key")
    
    # Observability
    log_level: str = Field("INFO", description="Log level")
    json_logs: bool = Field(False, description="Emit JSON-formatted logs")
    
    # ---------------- Lazy materialisation ----------------
    archive_enable: bool = Field(
        True,
        description="Enable on-disk archiving of concluded segments",
    )
    archive_dir: Path = Field(
        default=Path("output/archive"),
        description="Directory for JSONL archives (auto-created)",
    )
    
    class Config:
        env_prefix = "MCA_"
        env_file = ".env"
        case_sensitive = False
        # Accept legacy keys that are no longer explicitly modelled so that
        # users can keep an old config.yaml without breaking validation.
        extra = "allow" 

## 0007. multi_coder_analysis\core\__init__.py
----------------------------------------------------------------------------------------------------
from importlib import import_module as _imp

# ---------------------------------------------------------------------------
# Backward-compat shim: early notebooks did
#   from multi_coder_analysis.core import Engine
# After the package re-org that path vanished.  Re-export the default
# implementation so existing user code keeps working without edits.
# ---------------------------------------------------------------------------

Engine = _imp("multi_coder_analysis.core.regex").Engine  # type: ignore[attr-defined]

__all__: list[str] = ["Engine"]

# Export consensus utilities for external reuse
from .consensus import ConsensusStep  # type: ignore[E402,F401]
__all__.append("ConsensusStep") 

## 0008. multi_coder_analysis\core\consensus.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Per-hop consensus reduction utilities."""

from collections import defaultdict
from typing import Dict, List, Tuple, Iterable

from multi_coder_analysis.models import HopContext
from multi_coder_analysis.core.pipeline import Step
from multi_coder_analysis.core.tiebreaker import conservative_tiebreak
from multi_coder_analysis.utils.tie import is_perfect_tie
from multi_coder_analysis import run_multi_coder_tot as _legacy

__all__ = ["ConsensusStep", "HopVariability"]

# hop -> list[(statement_id, distribution)]
HopVariability = Dict[int, List[Tuple[str, Dict[str, int]]]]


class ConsensusStep(Step[List[HopContext]]):  # type: ignore[misc]
    """Collapse *k* permutations into (optionally) a single survivor.

    The step expects **all** permutations for the same segment at a given hop
    to be present in the incoming list.
    """

    def __init__(
        self,
        hop_idx: int,
        variability_log: HopVariability,
        *,
        tie_collector: list | None = None,
        decision_collector: list | None = None,
    ):
        self.hop_idx = hop_idx
        self._var = variability_log
        self._tie_collector = tie_collector
        self._dec_collector = decision_collector

    def run(self, ctxs: List[HopContext]) -> List[HopContext]:  # type: ignore[override]
        # Group by segment / statement_id
        buckets: Dict[str, List[HopContext]] = defaultdict(list)
        for c in ctxs:
            buckets[c.statement_id].append(c)

        survivors: List[HopContext] = []
        for sid, perms in buckets.items():
            # Collect votes from permutations ("yes" / "no" / "uncertain")
            votes: List[str] = [
                (p.raw_llm_responses[-1].get("answer", "") if p.raw_llm_responses else "uncertain")
                for p in perms
            ]

            decided, winner = conservative_tiebreak(votes)

            # Record distribution regardless of outcome
            dist = {v: votes.count(v) for v in set(votes)}
            self._var.setdefault(self.hop_idx, []).append((sid, dist))

            if decided and winner == "yes":
                # Select representative permutation (first) to continue
                rep = perms[0]
                rep.is_concluded = True
                if not rep.final_frame:
                    from multi_coder_analysis import run_multi_coder_tot as _legacy
                    rep.final_frame = _legacy.Q_TO_FRAME.get(self.hop_idx)
                survivors.append(rep)

                # ------------------- record decision -------------------
                if self._dec_collector is not None:
                    perm_entries = []
                    for p in perms:
                        last_resp = p.raw_llm_responses[-1] if p.raw_llm_responses else {}
                        perm_entries.append({
                            "perm_idx": getattr(p, "permutation_idx", None),
                            "answer": last_resp.get("answer", "uncertain"),
                            "rationale": last_resp.get("rationale", ""),
                            "via": ("regex" if last_resp.get("regex") else "llm")
                        })

                    self._dec_collector.append(
                        {
                            "statement_id": sid,
                            "statement_text": rep.segment_text,
                            "hop": self.hop_idx,
                            "frame": rep.final_frame,
                            "decision": "yes",
                            "distribution": dist,
                            "permutations": perm_entries,
                        }
                    )
            elif decided and winner == "no":
                # No conclusion – all permutations progress
                survivors.extend(perms)
            else:
                # Tie / no majority – mark concluded as tie (filtered out)
                for p in perms:
                    p.is_concluded = True
                    p.final_frame = "tie"

                if self._tie_collector is not None:
                    entry = {
                        "hop": self.hop_idx,
                        "statement_id": sid,
                        "permutations": []
                    }
                    for p in perms:
                        entry["permutations"].append({
                            "permutation_id": getattr(p, "permutation_idx", None),
                            "answer": (p.raw_llm_responses[-1].get("answer") if p.raw_llm_responses else "uncertain"),
                            "raw_llm_responses": p.raw_llm_responses,
                            "analysis_history": p.analysis_history,
                            "reasoning_trace": p.reasoning_trace,
                        })
                    self._tie_collector.append(entry)

                if self._dec_collector is not None:
                    perm_entries = []
                    for p in perms:
                        last_resp = p.raw_llm_responses[-1] if p.raw_llm_responses else {}
                        perm_entries.append({
                            "perm_idx": getattr(p, "permutation_idx", None),
                            "answer": last_resp.get("answer", "uncertain"),
                            "rationale": last_resp.get("rationale", ""),
                            "via": ("regex" if last_resp.get("regex") else "llm")
                        })
                    self._dec_collector.append(
                        {
                            "statement_id": sid,
                            "statement_text": perms[0].segment_text,
                            "hop": self.hop_idx,
                            "frame": "tie",
                            "decision": "tie",
                            "distribution": dist,
                            "permutations": perm_entries,
                        }
                    )
                # Not forwarded further
        return survivors 

## 0009. multi_coder_analysis\core\pipeline\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Lightweight functional pipeline primitives used by the Tree-of-Thought refactor.

The goal is to decouple algorithmic steps from I/O and orchestration while
remaining extremely small and dependency-free.  Each `Step[T]` receives and
returns the same context object enabling natural chaining and testability.
"""

from typing import Generic, TypeVar, Protocol, Callable, List

T_co = TypeVar("T_co", covariant=True)
T = TypeVar("T")


class Step(Generic[T], Protocol):
    """A pure-function processing step.

    Sub-classes implement :py:meth:`run` and **MUST NOT** mutate global state or
    perform side-effects outside the provided context object.
    """

    def run(self, ctx: T) -> T:  # noqa: D401
        """Transform *ctx* and return it (or a *new* instance).
        
        The default Tree-of-Thought implementation mutates the context in-place
        and returns the same object for convenience.
        """
        raise NotImplementedError


class FunctionStep(Generic[T]):
    """Adapter turning a plain function into a :class:`Step`."""

    def __init__(self, fn: Callable[[T], T]):
        self._fn = fn

    def run(self, ctx: T) -> T:  # type: ignore[override]
        return self._fn(ctx)


class Pipeline(Generic[T]):
    """Composable list of :class:`Step` objects executed sequentially."""

    def __init__(self, steps: List[Step[T]]):
        self._steps = steps

    def run(self, ctx: T) -> T:
        for step in self._steps:
            # Allow steps (e.g., answer evaluators) to signal early termination
            if (
                isinstance(ctx, list)
                and ctx
                and all(getattr(c, "is_concluded", False) for c in ctx)
            ) or getattr(ctx, "is_concluded", False):
                break
            ctx = step.run(ctx)
        return ctx


__all__ = [
    "Step",
    "FunctionStep",
    "Pipeline",
    "build_tot_pipeline",
]

# Re-export for higher layers needing direct access without circular imports
from .tot import build_tot_pipeline  # noqa: F401 

## 0010. multi_coder_analysis\core\pipeline\consensus_tot.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Consensus-aware Tree-of-Thought pipeline builder."""

from typing import List, Tuple
from pathlib import Path
import os as _os

from multi_coder_analysis.models import HopContext, BatchHopContext
from multi_coder_analysis.core.pipeline import Pipeline, Step
from multi_coder_analysis.core.pipeline.tot import _HopStep  # type: ignore
from multi_coder_analysis.core.consensus import ConsensusStep, HopVariability
from multi_coder_analysis.providers import ProviderProtocol
from multi_coder_analysis import run_multi_coder_tot as _legacy
from multi_coder_analysis.core.regex import Engine

__all__ = ["build_consensus_pipeline"]

# ------------------------------------------------------------
#  New pruning step – must run *after* ConsensusStep so that the
#  decision to conclude a statement is global across permutations
# ------------------------------------------------------------

class _ArchivePruneStep(Step[List[HopContext]]):  # type: ignore[misc]
    def __init__(self, *, run_id: str, archive_dir: Path, tag: str):
        self._run_id = run_id
        self._archive_dir = archive_dir
        self._tag = tag

    def run(self, ctxs: List[HopContext]) -> List[HopContext]:  # type: ignore[override]
        from multi_coder_analysis.utils import archive_resolved

        # Persist concluded contexts
        archive_resolved(
            ctxs,
            run_id=self._run_id,
            tag=self._tag,
            archive_dir=self._archive_dir,
        )

        # Return only unresolved segments to keep RAM low
        return [c for c in ctxs if not c.is_concluded]


class _ParallelHopStep(Step[List[HopContext]]):  # type: ignore[misc]
    """Map a single-hop step across all permutations in the list."""

    def __init__(
        self,
        hop_idx: int,
        provider: ProviderProtocol,
        model: str,
        *,
        batch_size: int,
        temperature: float,
        concurrency: int,
        top_k: int | None = None,
        top_p: float | None = None,
        run_id: str,
        archive_dir: Path,
        tag: str,
        ranked_list: bool = False,
        max_candidates: int = 5,
    ):
        self.hop_idx = hop_idx  # store for progress logging
        self._inner = _HopStep(
            hop_idx,
            provider,
            model,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            ranked_list=ranked_list,
            max_candidates=max_candidates,
        )
        self._provider = provider
        self._model = model
        self._temperature = temperature
        self._batch_size = max(1, batch_size)
        self._concurrency = max(1, concurrency)
        # Preserve for child steps (needed only for _log_hop; kept anyway)
        self._run_id = run_id
        self._archive_dir = archive_dir
        self._tag = tag
        self._rx = Engine.default()

    def run(self, ctxs: List[HopContext]) -> List[HopContext]:  # type: ignore[override]
        # --- Progress log (aggregated) ----------------------------------
        try:
            # Unique unresolved statement IDs – matches legacy banner expectation
            _active_ids = {c.statement_id for c in ctxs if not c.is_concluded}
            _active = len(_active_ids)

            # This is filled later during batch processing – initialise empty set
            regex_yes_ids: set[str] = set()
        except Exception:  # noqa: BLE001 – logging must never break flow
            pass

        # Emit *start* banner before any processing
        _primary = _os.getenv("PRIMARY_PERMUTATION", "1") == "1"

        try:
            if _primary:
                print(
                    f"*** START Hop {self.hop_idx:02} → start:{_active} regex:0 llm:0 remain:{_active} ***",
                    flush=True,
                )
        except Exception:
            pass

        # ------------------ Batch processing ---------------------------
        results: List[HopContext] = []

        # Align output ordering with input
        pending: List[HopContext] = []
        for c in ctxs:
            if c.is_concluded:
                results.append(c)
            else:
                pending.append(c)

        # Early exit if nothing to do
        if not pending:
            return results

        # ------------------------------------------------------
        # Split pending into *per-permutation* mini-pools so that
        # batches are formed independently within each permutation.
        # ------------------------------------------------------
        from collections import defaultdict
        from math import ceil

        groups: defaultdict[int | None, list[HopContext]] = defaultdict(list)
        for seg in pending:
            perm_id = getattr(seg, "permutation_idx", None)
            groups[perm_id].append(seg)

        llm_yes_ids: set[str] = set()
        banner_printed = False

        # Collect all LLM tasks across *all* permutations
        all_tasks: list[tuple[BatchHopContext, dict[str, HopContext]]] = []

        for perm_id, segs in groups.items():
            num_batches = ceil(len(segs) / self._batch_size)
            for b_idx in range(num_batches):
                batch_segments = segs[b_idx * self._batch_size : (b_idx + 1) * self._batch_size]

                # -------------- Regex pre-check -----------------
                unresolved: List[HopContext] = []
                for seg in batch_segments:
                    seg.q_idx = self.hop_idx
                    rx_ans = None
                    try:
                        rx_ans = self._rx.match(seg)
                    except Exception as _e:
                        import logging as _lg
                        _lg.warning("Regex engine error on %s Q%s: %s", seg.statement_id, self.hop_idx, _e)

                    if rx_ans and rx_ans.get("answer") == "yes":
                        seg.raw_llm_responses.append(rx_ans)  # type: ignore[arg-type]
                        seg.final_frame = rx_ans.get("frame") or _legacy.Q_TO_FRAME.get(self.hop_idx)
                        seg.final_justification = rx_ans.get("rationale")
                        seg.is_concluded = True
                        results.append(seg)
                        regex_yes_ids.add(seg.statement_id)
                    else:
                        unresolved.append(seg)

                if _primary and not banner_printed:
                    # Emit regex banner before the first LLM call (or immediately
                    # if the entire hop resolves via regex and no LLM call is needed).
                    if len(regex_yes_ids) > 0:
                        print(
                            f"*** REGEX HIT Hop {self.hop_idx:02} → regex:{len(regex_yes_ids)} ***",
                            flush=True,
                        )
                    else:
                        print(
                            f"*** REGEX MISS Hop {self.hop_idx:02} ***",
                            flush=True,
                        )
                    banner_printed = True

                if not unresolved:
                    continue  # this batch fully resolved by regex

                # -------------- Defer LLM batch ------------------
                import uuid
                batch_id = f"batch_{self.hop_idx}_{perm_id}_{uuid.uuid4().hex[:6]}"
                batch_ctx = BatchHopContext(batch_id=batch_id, hop_idx=self.hop_idx, segments=unresolved)

                sid_to_ctx = {c.statement_id: c for c in unresolved}
                all_tasks.append((batch_ctx, sid_to_ctx))

        # Re-attach results in original ordering
        sid_to_processed = {c.statement_id: c for c in results}
        ordered: List[HopContext] = [sid_to_processed[c.statement_id] if c.statement_id in sid_to_processed else c for c in ctxs]

        # --- Execute all queued LLM batches in parallel ------------------
        if all_tasks:
            from concurrent.futures import ThreadPoolExecutor, as_completed

            def _worker(bctx: BatchHopContext):
                return _legacy._call_llm_batch(
                    bctx,
                    self._provider,
                    self._model,
                    self._temperature,
                    ranked=self._inner._ranked_list,
                    max_candidates=self._inner._max_candidates,
                )

            with ThreadPoolExecutor(max_workers=self._concurrency) as exe:
                future_map = {exe.submit(_worker, ctx): (ctx, sid_map) for ctx, sid_map in all_tasks}

                for fut in as_completed(future_map):
                    batch_ctx, sid_to_ctx = future_map[fut]
                    try:
                        resp_objs = fut.result()
                    except Exception as exc:
                        import logging as _lg
                        _lg.error("LLM batch %s failed: %s", batch_ctx.batch_id, exc)
                        resp_objs = []

                    for obj in resp_objs:
                        sid = str(obj.get("segment_id", "")).strip()
                        ctx = sid_to_ctx.get(sid)
                        if ctx is None:
                            continue
                        ans = str(obj.get("answer", "uncertain")).lower().strip()
                        rationale = str(obj.get("rationale", ""))

                        ctx.raw_llm_responses.append(obj)  # type: ignore[arg-type]

                        if ans == "yes":
                            # Hop 11 special: look for explicit frame tag in rationale
                            _frame = None
                            if self.hop_idx == 11:
                                import re as _re
                                m_tag = _re.search(r"\|\|FRAME=(Alarmist|Reassuring)", rationale, _re.I)
                                if m_tag:
                                    _frame = m_tag.group(1).title()

                            ctx.final_frame = _frame or _legacy.Q_TO_FRAME.get(self.hop_idx)
                            ctx.final_justification = rationale
                            ctx.is_concluded = True
                            llm_yes_ids.add(ctx.statement_id)

                    # any still unresolved after responses
                    for ctx in sid_to_ctx.values():
                        if ctx not in results and ctx.is_concluded is False:
                            results.append(ctx)

        # ---------------- FINISH banner -------------------
        if _primary:
            end_active = _active - len(regex_yes_ids) - len(llm_yes_ids)
            # Ensure regex banner emitted at least once
            if not banner_printed:
                if len(regex_yes_ids) > 0:
                    print(
                        f"*** REGEX HIT Hop {self.hop_idx:02} → regex:{len(regex_yes_ids)} ***",
                        flush=True,
                    )
                else:
                    print(
                        f"*** REGEX MISS Hop {self.hop_idx:02} ***",
                        flush=True,
                    )
            try:
                print(
                    f"*** FINISH Hop {self.hop_idx:02} → start:{_active} "
                    f"regex:{len(regex_yes_ids)} llm:{len(llm_yes_ids)} "
                    f"remain:{end_active} ***",
                    flush=True,
                )
            except Exception:
                pass

        return ordered


def build_consensus_pipeline(
    provider: ProviderProtocol,
    model: str,
    temperature: float = 0.0,
    batch_size: int = 10,
    concurrency: int = 1,
    top_k: int | None = None,
    top_p: float | None = None,
    tie_collector: list | None = None,
    variability_log: HopVariability | None = None,
    *,
    run_id: str,
    archive_dir: Path,
    tag: str,
    ranked_list: bool = False,
    max_candidates: int = 5,
    **kwargs,
) -> Tuple[Pipeline[List[HopContext]], HopVariability]:
    """Return a pipeline operating on *lists* of HopContext objects."""

    var: HopVariability = variability_log or {}

    decision_collector = kwargs.get("decision_collector") if isinstance(kwargs.get("decision_collector"), list) else None

    steps: List[Step[List[HopContext]]] = []
    for h in range(1, 13):
        steps.append(
            _ParallelHopStep(
                h,
                provider,
                model,
                batch_size=batch_size,
                temperature=temperature,
                concurrency=concurrency,
                top_k=top_k,
                top_p=top_p,
                run_id=run_id,
                archive_dir=archive_dir,
                tag=tag,
                ranked_list=ranked_list,
                max_candidates=max_candidates,
            )
        )
        steps.append(ConsensusStep(h, var, tie_collector=tie_collector, decision_collector=decision_collector))
        steps.append(_ArchivePruneStep(run_id=run_id, archive_dir=archive_dir, tag=tag))

    return Pipeline(steps), var 

## 0011. multi_coder_analysis\core\pipeline\tot.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Pure-function Tree-of-Thought pipeline built on :class:`Step`. (Phase 5)"""

from typing import List

from multi_coder_analysis.models import HopContext
from multi_coder_analysis.core.pipeline import Step, Pipeline
from multi_coder_analysis.core.regex import Engine
from multi_coder_analysis.providers import ProviderProtocol

# Re-use existing helper from legacy implementation to avoid code duplication
from multi_coder_analysis import run_multi_coder_tot as _legacy

__all__ = ["build_tot_pipeline"]


class _HopStep(Step[HopContext]):
    """Single-hop processing step.

    The step first tries the regex engine; if inconclusive it delegates to the
    provider using the _legacy._call_llm_single_hop helper to preserve existing
    behaviour.  The class is internal – use :func:`build_tot_pipeline` instead.
    """

    _rx = Engine.default()

    def __init__(
        self,
        hop_idx: int,
        provider: ProviderProtocol,
        model: str,
        *,
        temperature: float,
        top_k: int | None = None,
        top_p: float | None = None,
        ranked_list: bool = False,
        max_candidates: int = 5,
    ):
        self.hop_idx = hop_idx
        self._provider = provider
        self._model = model
        self._temperature = temperature
        self._top_k = top_k
        self._top_p = top_p
        self._ranked_list = ranked_list
        self._max_candidates = max(1, max_candidates)

    # ------------------------------------------------------------------
    # The heavy lifting is delegated to code already battle-tested in the
    # legacy module.  This guarantees behavioural parity while moving the
    # orchestration into the new pipeline layer.
    # ------------------------------------------------------------------
    def run(self, ctx: HopContext) -> HopContext:  # noqa: D401
        ctx.q_idx = self.hop_idx

        regex_ans = self._rx.match(ctx)
        if regex_ans:
            ctx.raw_llm_responses.append(regex_ans)
            if regex_ans.get("answer") == "yes":
                ctx.final_frame = regex_ans.get("frame") or _legacy.Q_TO_FRAME[self.hop_idx]
                ctx.final_justification = regex_ans.get("rationale")
                ctx.is_concluded = True
            return ctx

        # Fall-through to LLM
        llm_resp = _legacy._call_llm_single_hop(
            ctx,
            self._provider,
            self._model,
            temperature=self._temperature,
            top_k=self._top_k,
            top_p=self._top_p,
            ranked=self._ranked_list,
            max_candidates=self._max_candidates,
        )  # type: ignore[arg-type]

        ctx.raw_llm_responses.append(llm_resp)

        # --- ranked-list aware extraction ---
        raw_ans = llm_resp.get("answer", "")
        try:
            from multi_coder_analysis.run_multi_coder_tot import _extract_frame_and_ranking  # lazy import to avoid cycles
            _, ranking = _extract_frame_and_ranking(raw_ans)
        except Exception:
            ranking = None

        if ranking:
            ranking = ranking[: self._max_candidates]
            ctx.ranking = ranking
            top_choice = ranking[0]
        else:
            top_choice = raw_ans

        if str(top_choice).lower().strip() == "yes":
            ctx.final_frame = _legacy.Q_TO_FRAME[self.hop_idx]
            ctx.final_justification = llm_resp.get("rationale", "").strip()
            ctx.is_concluded = True
        return ctx


def build_tot_pipeline(
    provider: ProviderProtocol,
    model: str,
    *,
    temperature: float = 0.0,
    top_k: int | None = None,
    top_p: float | None = None,
    ranked_list: bool = False,
    max_candidates: int = 5,
) -> Pipeline[HopContext]:
    """Return a :class:`Pipeline` implementing the 12-hop deterministic chain."""

    steps: List[Step[HopContext]] = [
        _HopStep(
            h,
            provider,
            model,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            ranked_list=ranked_list,
            max_candidates=max_candidates,
        )
        for h in range(1, 13)
    ]
    return Pipeline(steps) 

## 0012. multi_coder_analysis\core\prompt.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from functools import lru_cache
from pathlib import Path
from typing import Tuple, Dict, Any, TypedDict
import re

import yaml

__all__ = ["parse_prompt", "PromptMeta"]


class PromptMeta(TypedDict, total=False):
    hop: int
    short_name: str
    description: str
    # Extend with other known keys as needed.


# Regex to match YAML front-matter at top of file
_FM_RE = re.compile(r"^---\s*\n(.*?)\n---\s*\n?", re.DOTALL)


@lru_cache(maxsize=128)
def parse_prompt(path: Path) -> Tuple[str, PromptMeta]:
    """Return (prompt_body, front_matter) for *path*.

    The result is cached for the lifetime of the process to avoid unnecessary
    disk I/O during batch processing.
    """
    text = path.read_text(encoding="utf-8")

    m = _FM_RE.match(text)
    if not m:
        return text, {}  # type: ignore[return-value]

    meta_yaml = m.group(1)
    try:
        meta: PromptMeta = yaml.safe_load(meta_yaml) or {}
    except Exception:
        meta = {}  # type: ignore[assignment]

    body = text[m.end() :]
    # Normalise – drop the *single* blank line often left between '---' and
    # the actual prompt content so downstream tokenisers don't pay for it.
    if body.startswith("\n"):
        body = body.lstrip("\n")

    return body, meta 

## 0013. multi_coder_analysis\core\prompt_utils.py
----------------------------------------------------------------------------------------------------
from pathlib import Path

# Default directory points to the project prompts folder.
_PROMPTS_DIR = Path(__file__).resolve().parent.parent / "prompts"

__all__ = ["build_prompts"]


def build_prompts(question: str, *, ranked: bool) -> tuple[str, str]:
    """Return (system, user) prompt pair.

    Parameters
    ----------
    question : str
        The raw question or hop prompt body (with placeholders already filled).
    ranked : bool
        When True, instructs the model to emit a ranked list of answers.  When
        False, the legacy single-answer format is used.
    """
    # Load global header once for both modes.
    try:
        system_block = (_PROMPTS_DIR / "global_header.txt").read_text(encoding="utf-8")
    except FileNotFoundError:
        # Fallback to new canonical name if legacy not found.
        system_block = (_PROMPTS_DIR / "GLOBAL_HEADER.txt").read_text(encoding="utf-8")

    # Mode-specific few-shot examples and footer.
    flavour = "ranked" if ranked else "single"
    examples_path = _PROMPTS_DIR / f"examples_{flavour}.txt"
    footer_path = _PROMPTS_DIR / f"footer_{flavour}.txt"

    examples = examples_path.read_text(encoding="utf-8") if examples_path.exists() else ""
    footer = footer_path.read_text(encoding="utf-8") if footer_path.exists() else ""

    user_block_parts = []
    if examples:
        user_block_parts.append(examples)
    user_block_parts.append(question)
    if footer:
        user_block_parts.append(footer)

    user_block = "\n\n".join(user_block_parts)

    return system_block, user_block 

## 0014. multi_coder_analysis\core\regex\__init__.py
----------------------------------------------------------------------------------------------------
from .engine import Engine
from . import stats
from . import loader

__all__ = ["Engine"] 

## 0015. multi_coder_analysis\core\regex\engine.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Class-based regex engine for the 12-hop Tree-of-Thought pipeline.

The Engine class encapsulates regex matching logic as a stateless, first-class
object that can be instantiated with different rule sets.
"""

# Prefer the "regex" engine if available (supports variable-width look-behinds).
try:
    import regex as re  # type: ignore
except ImportError:  # pragma: no cover
    import re  # type: ignore

import logging
from typing import Optional, TypedDict, Callable
from collections import Counter
import threading
import importlib

# Import rules - handle both package and script contexts
try:
    from ...regex_rules import COMPILED_RULES, PatternInfo, RAW_RULES  # type: ignore
except ImportError:  # pragma: no cover
    try:
        from multi_coder_analysis.regex_rules import COMPILED_RULES, PatternInfo, RAW_RULES  # type: ignore
    except ImportError:
        # Final fallback for script execution
        from regex_rules import COMPILED_RULES, PatternInfo, RAW_RULES  # type: ignore

__all__ = ["Engine", "Answer"]


class Answer(TypedDict):
    """Typed structure returned when a regex rule fires."""
    answer: str
    rationale: str
    frame: Optional[str]
    regex: dict  # detailed match information


class Engine:
    """Stateless regex matching engine with configurable rule sets.
    
    Each Engine instance encapsulates its own rule statistics and configuration,
    enabling multiple engines with different behaviors to coexist.
    """
    
    # Class-level singleton for the default engine
    _DEFAULT: Optional["Engine"] = None
    
    def __init__(
        self,
        rules: Optional[dict[int, list[PatternInfo]]] = None,
        global_enabled: bool = True,
        force_shadow: bool = False,
        hit_logger: Optional[Callable[[dict], None]] = None,
    ):
        """Initialize a new Engine instance.
        
        Args:
            rules: Hop -> PatternInfo mapping. If None, uses COMPILED_RULES.
            global_enabled: Whether regex matching is enabled.
            force_shadow: If True, regex runs but never short-circuits.
            hit_logger: Optional callback for successful matches.
        """
        self._rules = rules if rules is not None else COMPILED_RULES
        self._global_enabled = global_enabled
        self._force_shadow = force_shadow
        self._hit_logger = hit_logger
        self._rule_stats: dict[str, Counter] = {}
        self._stat_lock = threading.Lock()
    
    @classmethod
    def default(cls) -> "Engine":
        """Get the default singleton Engine instance.
        
        This provides backward compatibility with the module-level API.
        """
        if cls._DEFAULT is None:
            cls._DEFAULT = cls()
        return cls._DEFAULT
    
    def set_global_enabled(self, flag: bool) -> None:
        """Enable or disable regex matching globally."""
        self._global_enabled = flag
    
    def set_force_shadow(self, flag: bool) -> None:
        """When True, regex runs but never short-circuits (shadow mode)."""
        self._force_shadow = flag
    
    def set_hit_logger(self, fn: Optional[Callable[[dict], None]]) -> None:
        """Register a callback to receive detailed match information."""
        self._hit_logger = fn
    
    def get_rule_stats(self) -> dict[str, Counter]:
        """Get per-rule statistics for this engine instance."""
        return dict(self._rule_stats)
    
    def _rule_fires(self, rule: PatternInfo, text: str) -> bool:
        """Return True iff rule matches positively and is not vetoed."""
        if not isinstance(rule.yes_regex, re.Pattern):
            logging.error("COMPILED_RULES must contain compiled patterns")
            return False

        positive = bool(rule.yes_regex.search(text))
        if not positive:
            return False

        if rule.veto_regex and isinstance(rule.veto_regex, re.Pattern):
            if rule.veto_regex.search(text):
                return False
        return True
    
    def match(self, ctx) -> Optional[Answer]:  # noqa: ANN001
        """Attempt to answer the current hop deterministically.

        Parameters
        ----------
        ctx : HopContext
            The current hop context (expects attributes: `q_idx`, `segment_text`).

        Returns
        -------
        Optional[Answer]
            • Dict with keys {answer, rationale, frame} when a single live rule
              fires with certainty.
            • None when no rule (or >1 rules) fire, or hop not covered, or rule is
              in shadow mode.
        """
        hop: int = getattr(ctx, "q_idx")
        text: str = getattr(ctx, "segment_text")

        if not self._global_enabled:
            return None

        rules = self._rules.get(hop, [])
        if not rules:
            # Try lazy reload for robustness
            try:
                from ... import regex_rules as _rr  # type: ignore
                importlib.reload(_rr)
                rules = _rr.COMPILED_RULES.get(hop, [])
            except Exception:  # pragma: no cover
                rules = []

        if not rules:
            return None

        # Safety net: merge missing canonical rules
        try:
            from ... import regex_rules as _rr  # package context
        except ImportError:  # script context
            try:
                from multi_coder_analysis import regex_rules as _rr  # type: ignore
            except ImportError:
                import regex_rules as _rr  # type: ignore

        _master_rules = _rr.COMPILED_RULES.get(hop, [])
        if _master_rules:
            existing_names = {r.name for r in rules}
            for _r in _master_rules:
                if _r.name not in existing_names:
                    rules.append(_r)

        winning_rule: Optional[PatternInfo] = None
        first_hit_rule: Optional[PatternInfo] = None

        # Evaluate every rule for statistics
        for rule in rules:
            fired = self._rule_fires(rule, text)

            # Thread-safe stats update
            with self._stat_lock:
                ctr = self._rule_stats.setdefault(rule.name, Counter())
                ctr["total"] += 1
                if fired:
                    ctr["hit"] += 1

            # Record first hit for shadow-mode logging
            if fired and first_hit_rule is None:
                first_hit_rule = rule

            if (
                fired
                and not self._force_shadow
                and (rule.mode == "live" or rule.mode == "shadow")
            ):
                if winning_rule is not None:
                    # Tolerate multiple hits if they agree on frame
                    if rule.yes_frame == winning_rule.yes_frame:
                        continue

                    # Conflicting frames → fall-through to LLM
                    logging.debug(
                        "Regex engine ambiguity: >1 conflicting rule fired for hop %s (%s vs %s)",
                        hop,
                        winning_rule.name,
                        rule.name,
                    )
                    return None
                winning_rule = rule

        # Shadow-mode logging
        if winning_rule is None:
            logging.debug("No regex rule matched for hop %s", hop)

            # shadow mode accounting
            from multi_coder_analysis.providers.base import _USAGE_ACCUMULATOR as _ACC  # avoid cycle
            _ACC["total_hops"] = _ACC.get("total_hops", 0) + 1
            if self._force_shadow:
                _ACC["regex_hit_shadow"] = _ACC.get("regex_hit_shadow", 0) + 1

            return None

        # ---- counters ----
        from multi_coder_analysis.providers.base import _USAGE_ACCUMULATOR as _ACC  # local import
        _ACC["regex_yes"] = _ACC.get("regex_yes", 0) + 1
        _ACC["total_hops"] = _ACC.get("total_hops", 0) + 1

        # Compute match details
        m = winning_rule.yes_regex.search(text)  # type: ignore[arg-type]
        span = [m.start(), m.end()] if m else None
        captures = list(m.groups()) if m else []

        rationale = f"regex:{winning_rule.name} matched"

        # Emit hit record
        if self._hit_logger is not None:
            try:
                self._hit_logger({
                    "statement_id": getattr(ctx, "statement_id", None),
                    "hop": hop,
                    "segment": text,
                    "rule": winning_rule.name,
                    "frame": winning_rule.yes_frame,
                    "mode": winning_rule.mode,
                    "span": span,
                })
            except Exception as e:  # pragma: no cover
                logging.debug("Regex hit logger raised error: %s", e, exc_info=True)

        return {
            "answer": "yes",
            "rationale": rationale,
            "frame": winning_rule.yes_frame,
            "regex": {
                "rule": winning_rule.name,
                "span": span,
                "captures": captures,
            },
        }


# Backward compatibility: module-level functions delegate to default engine
def match(ctx) -> Optional[Answer]:  # noqa: ANN001
    """Backward compatibility function - delegates to Engine.default().match()."""
    return Engine.default().match(ctx)


def set_global_enabled(flag: bool) -> None:
    """Backward compatibility function."""
    Engine.default().set_global_enabled(flag)


def set_force_shadow(flag: bool) -> None:
    """Backward compatibility function."""
    Engine.default().set_force_shadow(flag)


def get_rule_stats() -> dict[str, Counter]:
    """Backward compatibility function."""
    return Engine.default().get_rule_stats()


def set_hit_logger(fn: Callable[[dict], None]) -> None:
    """Backward compatibility function."""
    Engine.default().set_hit_logger(fn)


# Expose module-level globals for backward compatibility
_GLOBAL_ENABLE = True
_FORCE_SHADOW = False
_HIT_LOG_FN: Optional[Callable[[dict], None]] = None 

## 0016. multi_coder_analysis\core\regex\loader.py
----------------------------------------------------------------------------------------------------
"""Regex rule loader with plugin support (Phase 3)."""

from __future__ import annotations

import re
import yaml
from importlib import resources
from typing import List, Pattern

__all__ = ["load_rules"]


def load_rules() -> List[List[Pattern[str]]]:
    """Load regex rules from YAML files.
    
    Returns:
        List of rule lists, indexed by hop number (1-12)
    """
    # --------------------------------------------------
    # Locate the bundled YAML via importlib.resources – this works regardless
    # of whether the package is executed from an unpacked directory tree *or*
    # an installed, zipped wheel.
    # --------------------------------------------------
    try:
        # Fallback to package *root* then sub-path because 'multi_coder_analysis.regex'
        # is not a Python package (no __init__.py).
        rules_text = resources.files("multi_coder_analysis").joinpath("regex", "hop_patterns.yml").read_text("utf-8")
    except (FileNotFoundError, ModuleNotFoundError):
        # Package wasn't shipped with rules – treat as "no-op" engine.
        return [[] for _ in range(13)]  # 0-12, using 1-12

    try:
        data = yaml.safe_load(rules_text)
        
        # Convert to compiled patterns
        rules = [[] for _ in range(13)]  # 0-12, using 1-12
        
        if data and isinstance(data, dict):
            for hop_key, patterns in data.items():
                try:
                    hop_num = int(str(hop_key).lstrip("Qq"))
                    if 1 <= hop_num <= 12 and isinstance(patterns, list):
                        rules[hop_num] = [
                            re.compile(pattern, re.IGNORECASE)
                            for pattern in patterns
                            if isinstance(pattern, str)
                        ]
                except (ValueError, IndexError):
                    continue
        
        return rules
        
    except Exception:
        # Fallback to empty rules on any error
        return [[] for _ in range(13)] 

## 0017. multi_coder_analysis\core\regex\stats.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Regex engine statistics and reporting utilities."""

from collections import Counter
from typing import Dict, Any
import json
from pathlib import Path


def format_rule_stats(stats: Dict[str, Counter]) -> Dict[str, Any]:
    """Format rule statistics for human-readable output.
    
    Args:
        stats: Dictionary mapping rule names to hit/total counters.
        
    Returns:
        Formatted statistics with coverage percentages.
    """
    formatted = {}
    total_evaluations = 0
    total_hits = 0
    
    for rule_name, counter in stats.items():
        hits = counter.get("hit", 0)
        total = counter.get("total", 0)
        coverage = (hits / total * 100) if total > 0 else 0.0
        
        formatted[rule_name] = {
            "hits": hits,
            "total_evaluations": total,
            "coverage_percent": round(coverage, 2)
        }
        
        total_evaluations += total
        total_hits += hits
    
    # Add overall summary
    overall_coverage = (total_hits / total_evaluations * 100) if total_evaluations > 0 else 0.0
    formatted["_summary"] = {
        "total_rules": len(stats),
        "total_hits": total_hits,
        "total_evaluations": total_evaluations,
        "overall_coverage_percent": round(overall_coverage, 2)
    }
    
    return formatted


def export_stats_to_json(stats: Dict[str, Counter], output_path: Path) -> None:
    """Export rule statistics to a JSON file.
    
    Args:
        stats: Rule statistics from Engine.get_rule_stats().
        output_path: Path where to write the JSON file.
    """
    formatted = format_rule_stats(stats)
    
    with output_path.open('w', encoding='utf-8') as f:
        json.dump(formatted, f, indent=2, ensure_ascii=False)


def print_stats_summary(stats: Dict[str, Counter]) -> None:
    """Print a human-readable summary of rule statistics.
    
    Args:
        stats: Rule statistics from Engine.get_rule_stats().
    """
    formatted = format_rule_stats(stats)
    summary = formatted.pop("_summary")
    
    print(f"\n📊 Regex Engine Statistics Summary")
    print(f"{'='*50}")
    print(f"Total rules: {summary['total_rules']}")
    print(f"Total evaluations: {summary['total_evaluations']}")
    print(f"Total hits: {summary['total_hits']}")
    print(f"Overall coverage: {summary['overall_coverage_percent']:.2f}%")
    print()
    
    if formatted:
        print("Per-rule breakdown:")
        print(f"{'Rule Name':<30} {'Hits':<8} {'Total':<8} {'Coverage':<10}")
        print("-" * 60)
        
        # Sort by coverage descending
        sorted_rules = sorted(
            formatted.items(),
            key=lambda x: x[1]["coverage_percent"],
            reverse=True
        )
        
        for rule_name, data in sorted_rules:
            print(f"{rule_name:<30} {data['hits']:<8} {data['total_evaluations']:<8} {data['coverage_percent']:<10.2f}%")


__all__ = ["format_rule_stats", "export_stats_to_json", "print_stats_summary"] 

## 0018. multi_coder_analysis\core\self_consistency.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Self-consistency decoding helpers.

This module implements the multi-path sampling + voting strategy popularised
by the *Self-Consistency* paper.  The public surface consists of two helpers:

1. decode_paths() → runs *N* independent passes through the deterministic
   Tree-of-Thought pipeline using stochastic decoding.
2. aggregate()    → collapses the list of answers into a single prediction
   according to one of three voting rules.

The implementation is intentionally dependency-free – we rely on the existing
`build_tot_pipeline` to perform a single ToT pass and use provider-supplied
usage metadata as a crude log-prob proxy when length-normalisation is needed.
"""

from collections import Counter, defaultdict
from typing import Dict, List, Tuple

from multi_coder_analysis.core.pipeline.tot import build_tot_pipeline
from multi_coder_analysis.models import HopContext
from multi_coder_analysis.providers import ProviderProtocol, get_provider
from multi_coder_analysis.providers.base import get_usage_accumulator

__all__ = [
    "decode_paths",
    "aggregate",
]


# ---------------------------------------------------------------------------
# Multi-path decoding
# ---------------------------------------------------------------------------

def decode_paths(
    base_ctx: HopContext,
    provider: ProviderProtocol,
    model: str,
    *,
    votes: int,
    temperature: float,
    top_k: int,
    top_p: float,
    ranked_list: bool = False,
    max_candidates: int = 5,
) -> List[Tuple[list[str] | str, float]]:
    """Run the ToT pipeline *votes* times with stochastic sampling.

    Parameters
    ----------
    base_ctx
        Template context holding the statement text + IDs.
    provider
        Provider implementing the generate() interface.
    model
        Model identifier.
    votes
        Number of independent samples.
    temperature, top_k, top_p
        Sampling hyper-parameters forwarded to the provider.  Note that the
        internal ToT pipeline currently passes only *temperature*.  Providers
        expose *top_k*/*top_p* anyway, so we call them directly when ToT falls
        through to the LLM.
    ranked_list
        Whether to return a ranked list of candidates.
    max_candidates
        Maximum number of candidates to return in ranked list.

    Returns
    -------
    list of tuples
        Each tuple is (answer, score) where *answer* is the final frame label
        or a list of labels if multiple answers are possible, and *score* is a
        rough heuristic of likelihood (negative length-normalised token count
        when probability not available).
    """

    # Determine provider short name once for quick cloning per vote
    prov_name = provider.__class__.__name__.replace("Provider", "").lower()

    samples: List[Tuple[list[str] | str, float]] = []

    for _ in range(votes):
        local_provider = get_provider(prov_name)

        # Build fresh pipeline each vote to avoid state leakage
        pipeline = build_tot_pipeline(
            local_provider,
            model,
            temperature=temperature,
            top_k=(None if top_k == 0 else top_k),
            top_p=top_p,
            ranked_list=ranked_list,
            max_candidates=max_candidates,
        )

        before_usage = get_usage_accumulator().copy()

        ctx = HopContext(
            statement_id=base_ctx.statement_id,
            segment_text=base_ctx.segment_text,
            article_id=base_ctx.article_id,
        )

        pipeline.run(ctx)

        after_usage = get_usage_accumulator().copy()

        score = float(after_usage.get("total_tokens", 0) - before_usage.get("total_tokens", 0))

        ans_payload = ctx.final_frame or (ctx.ranking or None)
        if ans_payload is not None:
            if isinstance(ans_payload, list):
                ans_payload = ans_payload[: max_candidates]
            samples.append((ans_payload, score))

    return samples


# ---------------------------------------------------------------------------
# Voting aggregation
# ---------------------------------------------------------------------------

def _majority(pairs):  # type: ignore[override]
    """Hard vote.

    If an element is a ranked list, use its first candidate. Otherwise treat it
    as a plain string label.
    """

    def _top(label):
        return label[0] if isinstance(label, list) else label

    counts = Counter(_top(a) for a, _ in pairs)
    ans, freq = counts.most_common(1)[0]
    return ans, freq / len(pairs)


def _ranked(pairs: List[Tuple[list[str] | str, float]], normalise: bool) -> Tuple[str, float]:
    buckets: Dict[str, float] = defaultdict(float)
    counts: Dict[str, int] = defaultdict(int)

    for ans, score in pairs:
        key = ans[0] if isinstance(ans, list) else ans
        buckets[key] += score
        counts[key] += 1

    if normalise:
        # Average score per candidate
        for ans in buckets:
            buckets[ans] /= max(counts[ans], 1)

    # Edge cases
    ans = min(buckets, key=buckets.get)
    best = buckets[ans]
    worst = max(buckets.values()) or 1

    # unanimous vote → certainty 1.0
    if len(buckets) == 1:
        return ans, 1.0
    # zero-cost paths (all regex) → unknown confidence
    if worst == 0:
        return ans, 0.0

    conf = 1 - (best / worst)
    return ans, conf


def _irv(pairs: list[tuple[list[str] | str, float]]) -> tuple[str, float]:
    from collections import Counter
    rankings = [(r if isinstance(r, list) else [r]) for r, _ in pairs]
    if not rankings:
        return _majority([(a[0] if isinstance(a, list) else a, s) for a, s in pairs])
    while True:
        first = Counter(r[0] for r in rankings if r)
        if not first:
            return _majority([(a[0] if isinstance(a, list) else a, s) for a, s in pairs])
        winner, votes = first.most_common(1)[0]
        if votes > len(rankings) / 2:
            return winner, votes / len(rankings)
        # Determine loser deterministically (alphabetical among least votes)
        min_votes = min(first.values())
        losers = sorted([c for c, v in first.items() if v == min_votes])
        loser = losers[0]
        rankings = [[c for c in r if c != loser] for r in rankings]


def _borda(pairs: list[tuple[list[str] | str, float]]) -> tuple[str, float]:
    from collections import defaultdict
    scores = defaultdict(int)
    for ranking, _ in pairs:
        ranking = ranking if isinstance(ranking, list) else [ranking]
        for idx, cand in enumerate(ranking):
            scores[cand] += len(ranking) - idx
    if not scores:
        return _majority([(a[0] if isinstance(a, list) else a, s) for a, s in pairs])
    winner = max(scores, key=scores.get)
    total = sum(scores.values()) or 1
    return winner, scores[winner] / total


def _mrr(pairs: list[tuple[list[str] | str, float]]) -> tuple[str, float]:
    from collections import defaultdict
    mrr_scores = defaultdict(float)
    for ranking, _ in pairs:
        ranking = ranking if isinstance(ranking, list) else [ranking]
        for idx, cand in enumerate(ranking, 1):
            mrr_scores[cand] += 1 / idx
    if not mrr_scores:
        return _majority([(a[0] if isinstance(a, list) else a, s) for a, s in pairs])
    winner = max(mrr_scores, key=mrr_scores.get)
    total = sum(mrr_scores.values()) or 1
    return winner, mrr_scores[winner] / total


def aggregate(pairs, rule: str = "majority") -> tuple[str, float]:
    """Collapse *pairs* into (answer, confidence) according to *rule*."""
    if not pairs:
        return "unknown", 0.0

    rule = rule.lower()
    if rule == "irv":
        return _irv(pairs)
    if rule == "borda":
        return _borda(pairs)
    if rule == "mrr":
        return _mrr(pairs)
    if rule == "majority":
        return _majority(pairs)
    if rule == "ranked":
        return _ranked(pairs, normalise=True)
    if rule == "ranked-raw":
        return _ranked(pairs, normalise=False)
    raise ValueError(f"Unknown rule: {rule}") 

## 0019. multi_coder_analysis\core\tiebreaker.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Generic tie-breaking utilities used by consensus layers."""

from collections import Counter
from typing import List, Optional, Tuple

__all__ = ["conservative_tiebreak"]


def conservative_tiebreak(votes: List[str]) -> Tuple[bool, Optional[str]]:
    """Return (has_consensus, winning_value).

    The function implements a conservative rule: a value only wins if it
    secures *strict* (>50 %) majority.  Otherwise, it reports no consensus.
    """
    ctr = Counter(votes)
    if not ctr:
        return False, None
    winner, n = ctr.most_common(1)[0]
    if n > len(votes) / 2:
        return True, winner
    return False, None 

## 0020. multi_coder_analysis\llm_providers\__init__.py
----------------------------------------------------------------------------------------------------
# LLM Providers package 

## 0021. multi_coder_analysis\llm_providers\base.py
----------------------------------------------------------------------------------------------------
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    """Uniform interface for all LLM back‑ends."""

    @abstractmethod
    def generate(self, prompt: str, model: str, temperature: float = 0.0) -> str:
        """Return the raw assistant message text."""
        ...
    
    @abstractmethod
    def get_last_thoughts(self) -> str:
        """Retrieve thinking traces from the last generate() call."""
        ...

    @abstractmethod
    def get_last_usage(self) -> dict:
        """Return token usage metadata from last call (keys: prompt_tokens, response_tokens, total_tokens)."""
        ... 

## 0022. multi_coder_analysis\llm_providers\gemini_provider.py
----------------------------------------------------------------------------------------------------
import os
# NOTE: Google Gemini SDK is optional in many test environments. We therefore
# *attempt* to import it lazily and only raise a helpful error message at
# instantiation time, not at module import time (which would break unrelated
# unit-tests that merely import the parent package).
import logging
from typing import Optional, TYPE_CHECKING

# Defer heavy / optional dependency import – set sentinels instead.  We rely
# on run-time checks within `__init__` to raise when the SDK is truly needed.
if TYPE_CHECKING:
    # Type-hints only (ignored at run-time when type checkers are disabled)
    from google import genai as _genai  # pragma: no cover
    from google.genai import types as _types  # pragma: no cover

genai = None  # will attempt to import lazily in __init__
types = None
from .base import LLMProvider

class GeminiProvider(LLMProvider):
    def __init__(self, api_key: Optional[str] = None):
        global genai, types  # noqa: PLW0603 – we intentionally mutate module globals

        # Lazy-load the Google SDK only when the provider is actually
        # instantiated (i.e. during *production* runs, not unit-tests that
        # only touch auxiliary helpers like _assemble_prompt).
        if genai is None or types is None:
            try:
                from google import genai as _genai  # type: ignore
                from google.genai import types as _types  # type: ignore

                genai = _genai  # promote to module-level for reuse
                types = _types
            except ImportError as e:
                # Surface a clear, actionable message **only** when the class
                # is actually used.  This keeps import-time side effects
                # minimal and avoids breaking unrelated tests.
                raise ImportError(
                    "The google-genai SDK is required to use GeminiProvider."
                    "\n   ➜  pip install google-genai"
                ) from e

        key = api_key or os.getenv("GOOGLE_API_KEY")
        if not key:
            raise ValueError("GOOGLE_API_KEY not set")

        # Initialize client directly with the API key (newer SDK style)
        self._client = genai.Client(api_key=key)

        # Per-instance cumulative usage (resettable)
        self._acc_usage = {
            'prompt_tokens': 0,
            'response_tokens': 0,
            'thought_tokens': 0,
            'total_tokens': 0,
            'cached_tokens': 0,
        }

    def generate(self, system_prompt: str, user_prompt: str, model: str, temperature: float = 0.0) -> str:
        cfg = {"temperature": temperature}
        # Enforce deterministic nucleus + top-k
        cfg["top_p"] = 0.1
        cfg["top_k"] = 1
        cfg["system_instruction"] = system_prompt

        if "2.5" in model:
            cfg["thinking_config"] = types.ThinkingConfig(include_thoughts=True)
        
        resp = self._client.models.generate_content(
            model=model,
            contents=user_prompt,
            config=types.GenerateContentConfig(**cfg)
        )
        
        # Capture usage metadata if available
        usage_meta = getattr(resp, 'usage_metadata', None)
        if usage_meta:
            def _safe_int(val):
                try:
                    return int(val or 0)
                except Exception:
                    return 0

            prompt_toks = _safe_int(getattr(usage_meta, 'prompt_token_count', 0))
            # SDK renamed field from response_token_count → candidates_token_count
            resp_toks = _safe_int(getattr(usage_meta, 'response_token_count', getattr(usage_meta, 'candidates_token_count', 0)))
            thought_toks = _safe_int(getattr(usage_meta, 'thoughts_token_count', 0))
            total_toks = _safe_int(getattr(usage_meta, 'total_token_count', 0))
            self._last_usage = {
                'prompt_tokens': prompt_toks,
                'response_tokens': resp_toks,
                'thought_tokens': thought_toks,
                'total_tokens': total_toks or (prompt_toks + resp_toks + thought_toks)
            }
            # ---- accumulate ----
            for k, v in self._last_usage.items():
                if isinstance(v, int):
                    self._acc_usage[k] = self._acc_usage.get(k, 0) + v
        else:
            self._last_usage = {'prompt_tokens': 0, 'response_tokens': 0, 'thought_tokens': 0, 'total_tokens': 0}
        
        # Stitch together parts (Gemini returns list‑of‑parts)
        thoughts = ""
        content = ""
        
        for part in resp.candidates[0].content.parts:
            if not part.text:
                continue
            if hasattr(part, 'thought') and part.thought:
                thoughts += part.text
            else:
                content += part.text
        
        # Store thoughts for potential retrieval
        self._last_thoughts = thoughts
        
        return content.strip()
    
    def get_last_thoughts(self) -> str:
        """Retrieve thinking traces from the last generate() call."""
        return getattr(self, '_last_thoughts', '')

    def get_last_usage(self) -> dict:
        return getattr(self, '_last_usage', {'prompt_tokens': 0, 'response_tokens': 0, 'total_tokens': 0})

    # --------------------------------------------------------------
    # Incremental usage helpers (new)
    # --------------------------------------------------------------

    def reset_usage(self) -> None:  # noqa: D401
        self._acc_usage = {k: 0 for k in self._acc_usage}

    def get_acc_usage(self) -> dict:  # noqa: D401
        return dict(self._acc_usage) 

## 0023. multi_coder_analysis\main.py
----------------------------------------------------------------------------------------------------
import argparse
import logging
import sys
import yaml
from pathlib import Path
from datetime import datetime
import os
from typing import Dict, Optional
import threading
import signal
import shutil
import multiprocessing as _mp, os as _os, signal as _signal

# --- Import step functions from other modules --- #
# from run_multi_coder import run_coding_step  # TODO: Create this for standard pipeline
# Support both "python multi_coder_analysis/main.py" (script) and
# "python -m multi_coder_analysis.main" (module) invocation styles.
try:
    from .run_multi_coder_tot import run_coding_step_tot  # package-relative when executed as module
    from . import run_multi_coder_tot as _tot_mod
except ImportError:
    from run_multi_coder_tot import run_coding_step_tot  # direct import when executed as script
    import run_multi_coder_tot as _tot_mod
# from merge_human_and_models import run_merge_step  # TODO: Create this
# from reliability_stats import run_stats_step  # TODO: Create this
# from sampling import run_sampling_for_phase  # TODO: Create this

# --- Import prompt concatenation utility ---
# Support both module and script execution styles
try:
    from .concat_prompts import concatenate_prompts  # package-relative
except ImportError:
    from concat_prompts import concatenate_prompts  # script-level fallback

# --- Import reproducibility utils ---
# from utils.reproducibility import generate_run_manifest, get_file_sha256  # TODO: Create this

# --- Global Shutdown Event ---
shutdown_event = threading.Event()

# ---------------------------------------------------------------------------
# Immediate SIGINT handler
# ---------------------------------------------------------------------------
# Many runs spawn background threads *and* ProcessPool workers.  In practice
# the default graceful KeyboardInterrupt takes several seconds (or never
# finishes) while those workers clean up.  Users have asked for an
# *instant* abort when they hit Ctrl-C.  The new handler terminates all
# active child processes, sets the global shutdown flag (so long-running
# loops that catch KeyboardInterrupt can still see it), flushes the
# logging streams, and then exits the interpreter via os._exit which does
# not invoke cleanup handlers – but guarantees prompt return to the shell.
# ---------------------------------------------------------------------------

def _terminate_children() -> None:  # pragma: no cover – best effort clean-up
    try:
        for p in _mp.active_children():
            try:
                p.terminate()
            except Exception:
                pass
    except Exception:
        # Multiprocessing may not be initialised yet; ignore
        pass


def handle_sigint(sig, frame):  # type: ignore[override]
    print()  # newline after ^C so next prompt starts fresh
    logging.error("⌧  Ctrl-C received – aborting run immediately …")

    # Notify any cooperative loops
    shutdown_event.set()

    # Kill child processes (ProcessPoolExecutor workers, etc.)
    _terminate_children()

    # Flush logging to be sure the message appears
    for h in logging.getLogger().handlers:
        try:
            h.flush()
        except Exception:
            pass

    # Exit *now* – bypass atexit/finally blocks to avoid hangs
    _os._exit(130)  # 130 conventionally signals SIGINT termination

# --- Configuration Loading ---
def load_config(config_path):
    """Loads configuration from a YAML file."""
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        logging.error(f"Configuration file not found: {config_path}")
        sys.exit(1)
    except yaml.YAMLError as e:
        logging.error(f"Error parsing configuration file: {e}")
        sys.exit(1)

# --- Logging Setup ---
def setup_logging(config):
    """Configures logging based on the config file."""
    log_config = config.get('logging', {})
    level = log_config.get('level', 'INFO').upper()
    log_format = log_config.get('format', '%(asctime)s - %(levelname)s - %(message)s')
    log_file = log_config.get('file')  # optional path for on-disk logging
    
    # Set Google SDK to ERROR level immediately to prevent AFC noise
    logging.getLogger("google").setLevel(logging.ERROR)
    logging.getLogger("google.genai").setLevel(logging.ERROR)
    logging.getLogger("google.genai.client").setLevel(logging.ERROR)
    
    logging.basicConfig(level=level, format=log_format, handlers=[logging.StreamHandler(sys.stdout)])

    # ------------------------------------------------------------------
    # Optional FileHandler – writes the same log stream to disk when the
    # user specifies ``logging.file`` in config.yaml (or passes it via env
    # injection).  Keeps stdout behaviour unchanged.
    # ------------------------------------------------------------------
    if log_file:
        try:
            fh = logging.FileHandler(log_file, encoding="utf-8")
            fh.setLevel(level)
            fh.setFormatter(logging.Formatter(log_format))
            logging.getLogger().addHandler(fh)
            logging.debug("File logging enabled → %s", log_file)
        except Exception as e:
            logging.warning("⚠ Could not set up file logging (%s): %s", log_file, e)

    # Silence noisy AFC-related logs emitted by external libraries
    class _AFCNoiseFilter(logging.Filter):
        _PHRASES = ("AFC is enabled", "AFC remote call", "max remote calls")

        def filter(self, record: logging.LogRecord) -> bool:  # type: ignore[override]
            msg = record.getMessage()
            return not any(p in msg for p in self._PHRASES)

    # Apply filter to root logger and specifically to google logger
    logging.getLogger().addFilter(_AFCNoiseFilter())
    logging.getLogger("google").addFilter(_AFCNoiseFilter())
    logging.getLogger("google.genai").addFilter(_AFCNoiseFilter())

    # Patch sys.stdout/stderr to filter out noisy AFC print statements outside logging
    import sys as _sys, io as _io

    class _FilteredStream(_io.TextIOBase):
        def __init__(self, original):
            self._orig = original

        def write(self, s):  # type: ignore[override]
            # Skip lines containing AFC noise phrases
            if any(p in s for p in _AFCNoiseFilter._PHRASES):
                return len(s)  # Pretend we wrote it to keep caller happy
            return self._orig.write(s)

        def flush(self):  # type: ignore[override]
            return self._orig.flush()

    _sys.stdout = _FilteredStream(_sys.stdout)
    _sys.stderr = _FilteredStream(_sys.stderr)

    # Reduce noise from HTTP libraries / Google SDK unless user sets DEBUG
    if level != "DEBUG":
        for noisy in ("google", "httpx", "urllib3"):
            logging.getLogger(noisy).setLevel(logging.ERROR)  # Changed to ERROR

# --- Main Orchestration ---
def run_pipeline(config: Dict, phase: str, coder_prefix: str, dimension: str, args: argparse.Namespace, shutdown_event: threading.Event):
    """Runs the full multi-coder analysis pipeline."""
    start_time = datetime.now()
    pipeline_timestamp = start_time.strftime("%Y%m%d_%H%M%S")
    logging.info(f"Starting pipeline run ({pipeline_timestamp}) for Phase: {phase}, Coder: {coder_prefix}, Dimension: {dimension}")

    # --- Path Setup & Initial Config Population ---
    try:
        # Create simple input/output structure for testing
        base_output_dir = Path("multi_coder_analysis") / "output" / phase / dimension / pipeline_timestamp
        base_output_dir.mkdir(parents=True, exist_ok=True)
        logging.info(f"Created output directory: {base_output_dir}")

        # --- Copy prompt directory verbatim for auditability (replaces old concatenation) ---
        try:
            src_prompt_dir = Path(args.prompts_dir)
            dst_prompt_dir = base_output_dir / "prompts"
            shutil.copytree(src_prompt_dir, dst_prompt_dir, dirs_exist_ok=True)
            logging.info("Copied prompt folder → %s", dst_prompt_dir)
            # NEW: also write a snapshot of all prompt files concatenated into one for auditability
            try:
                concat_filename = f"concatenated_prompts_{pipeline_timestamp}.txt"
                concatenate_prompts(src_prompt_dir, concat_filename, base_output_dir)
                logging.info("Generated concatenated prompts file → %s", base_output_dir / concat_filename)
            except Exception as _e:
                logging.warning("Could not generate concatenated prompts file: %s", _e)
            # Override prompts dir globally for ToT module
            try:
                _tot_mod.PROMPTS_DIR = src_prompt_dir.resolve()
                logging.info("Using custom prompts directory: %s", _tot_mod.PROMPTS_DIR)
            except Exception as _e:
                logging.warning("Could not override PROMPTS_DIR in run_multi_coder_tot: %s", _e)
        except Exception as e:
            logging.warning("Could not copy prompts folder: %s", e)

        # ------------------------------------------------------------------
        # Copy the exact regex catalogue used for this run into the output
        # directory for audit / reproducibility.
        # ------------------------------------------------------------------
        patterns_src = Path("multi_coder_analysis/regex/hop_patterns.yml")
        try:
            shutil.copy(patterns_src, base_output_dir / "hop_patterns.yml")
            logging.info("Copied hop_patterns.yml to output folder for auditability.")
        except Exception as e:
            logging.warning("Could not copy hop_patterns.yml (%s): %s", patterns_src, e)

        # Compiled_rules.txt no longer generated (redundant with hop_patterns.yml)

        # Determine input file source
        if args.input:
            # Use user-specified input file
            input_file = Path(args.input)
            if not input_file.exists():
                logging.error(f"Specified input file does not exist: {input_file}")
                raise FileNotFoundError(f"Input file not found: {input_file}")
            logging.info(f"Using specified input file: {input_file}")
        else:
            # Create a simple test input file if it doesn't exist (original behavior)
            input_file = Path("data") / f"{phase}_for_human.csv"
            if not input_file.exists():
                input_file.parent.mkdir(parents=True, exist_ok=True)
                # Create a minimal test CSV
                import pandas as pd
                test_data = pd.DataFrame({
                    'StatementID': ['TEST_001', 'TEST_002'],
                    'Statement Text': [
                        'The flu is so deadly that entire flocks are culled.',
                        'Health officials say the outbreak is fully under control.'
                    ]
                })
                test_data.to_csv(input_file, index=False)
                logging.info(f"Created test input file: {input_file}")
            else:
                logging.info(f"Using existing input file: {input_file}")

        # Update config with runtime paths
        config['runtime_input_dir'] = str(input_file.parent)
        config['runtime_output_dir'] = str(base_output_dir)
        config['runtime_phase'] = phase
        config['runtime_coder_prefix'] = coder_prefix
        config['runtime_dimension'] = dimension
        config['runtime_provider'] = args.provider
        config['individual_fallback'] = args.individual_fallback

    except Exception as e:
        logging.error(f"Error during path setup: {e}")
        raise

    # --- Pipeline Step 1: LLM Coding ---
    logging.info("--- Starting Step 1: LLM Coding ---")
    
    if args.use_tot:
        logging.info("Using Tree-of-Thought (ToT) method.")
        if hasattr(args, 'gemini_only') and args.gemini_only:
            logging.warning("--gemini-only flag is ignored when --use-tot is active.")
        
        try:
            raw_votes_path, majority_labels_path = run_coding_step_tot(
                config, 
                input_file,
                base_output_dir,
                limit=args.limit,
                start=args.start,
                end=args.end,
                concurrency=args.concurrency,
                model=args.model,
                provider=args.provider,
                batch_size=args.batch_size,
                regex_mode=args.regex_mode,
                shuffle_batches=args.shuffle_batches,
                skip_eval=args.no_eval,
                only_hop=args.only_hop,
                gold_standard_file=args.gold_standard,
            )
        except Exception as e:
            logging.error(f"Tree-of-Thought pipeline failed with error: {e}", exc_info=True)
            sys.exit(1)
            
    else:
        logging.info("Standard multi-model consensus method not yet implemented in this version.")
        logging.error("Please use --use-tot flag to run the Tree-of-Thought pipeline.")
        sys.exit(1)

    logging.info(f"LLM coding finished. Majority labels at: {majority_labels_path}")

    # TODO: Add merge and stats steps when those modules are implemented
    logging.info("Pipeline completed successfully!")

def re_evaluate_run(run_dir: Path, gold_standard_file: str) -> None:
    """Re-evaluate an existing pipeline run against a new gold standard."""
    import pandas as pd
    from datetime import datetime
    
    logging.info(f"Re-evaluating run: {run_dir}")
    logging.info(f"Using gold standard: {gold_standard_file}")
    
    # Load pipeline results
    model_labels_file = run_dir / "model_labels_tot.csv"
    df_results = pd.read_csv(model_labels_file, dtype={'StatementID': str})
    logging.info(f"Loaded {len(df_results)} pipeline results from {model_labels_file}")
    
    # Load new gold standard
    df_gold = pd.read_csv(gold_standard_file, dtype={'StatementID': str})
    
    # Verify gold standard file has required columns
    if 'StatementID' not in df_gold.columns:
        raise ValueError("Gold standard file must contain 'StatementID' column")
    if 'Gold Standard' not in df_gold.columns:
        raise ValueError("Gold standard file must contain 'Gold Standard' column")
    
    logging.info(f"Loaded {len(df_gold)} gold standard entries from {gold_standard_file}")
    
    # Merge results with new gold standard
    df_comparison = df_results.merge(
        df_gold[['StatementID', 'Gold Standard']], 
        on='StatementID', 
        how='left'
    )
    
    # Log merge statistics  
    total_results = len(df_comparison)
    has_gold = df_comparison['Gold Standard'].notna().sum()
    missing_gold = total_results - has_gold
    logging.info(f"Gold standard merge: {has_gold}/{total_results} results have gold labels, {missing_gold} missing")
    
    if missing_gold > 0:
        missing_ids = df_comparison[df_comparison['Gold Standard'].isna()]['StatementID'].tolist()
        logging.warning(f"Results missing gold standard labels: {missing_ids[:10]}{'...' if len(missing_ids) > 10 else ''}")
    
    # Filter to only results that have gold standard labels
    df_comparison = df_comparison[df_comparison['Gold Standard'].notna()].copy()
    
    if len(df_comparison) == 0:
        logging.error("No pipeline results match the new gold standard")
        return
    
    # Rename columns for consistency with existing evaluation code
    df_comparison = df_comparison.rename(columns={'Gold Standard': 'Gold_Standard'})
    
    # Add mismatch column
    df_comparison['Mismatch'] = df_comparison['Gold_Standard'] != df_comparison['Pipeline_Result']
    
    # Create timestamped comparison file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    comparison_path = run_dir / f"re_evaluation_{timestamp}_comparison.csv"
    df_comparison.to_csv(comparison_path, index=False)
    logging.info(f"Saved re-evaluation comparison to: {comparison_path}")
    
    # Import evaluation functions from run_multi_coder_tot
    try:
        from .run_multi_coder_tot import calculate_metrics, print_evaluation_report, print_mismatches, reorganize_traces_by_match_status
    except ImportError:
        from run_multi_coder_tot import calculate_metrics, print_evaluation_report, print_mismatches, reorganize_traces_by_match_status
    
    # Calculate metrics
    predictions = df_comparison['Pipeline_Result'].tolist()
    actuals = df_comparison['Gold_Standard'].tolist()
    metrics = calculate_metrics(predictions, actuals)
    
    # Print evaluation report
    print(f"\n🔄 RE-EVALUATION RESULTS")
    print(f"📁 Run directory: {run_dir}")
    print(f"📊 Gold standard: {gold_standard_file}")
    print(f"📈 Evaluated {len(df_comparison)} results")
    
    # Calculate mismatch counts
    mismatch_count = int(df_comparison["Mismatch"].sum())
    print(f"❌ Mismatches: {mismatch_count}/{len(df_comparison)} ({mismatch_count/len(df_comparison):.1%})")
    
    print(f"\n🎯 OVERALL ACCURACY: {metrics['accuracy']:.2%}")
    print(f"\n=== Per-Frame Precision / Recall ===")
    
    for frame, stats in metrics['frame_metrics'].items():
        if stats['tp'] + stats['fp'] + stats['fn'] == 0:
            continue  # Skip frames not present in the data
            
        p_str = f"{stats['precision']:.2%}" if stats['precision'] > 0 else "nan%"
        r_str = f"{stats['recall']:.2%}" if stats['recall'] > 0 else "0.00%"
        f1_str = f"{stats['f1']:.2%}" if stats['f1'] > 0 else "nan%"
        
        print(f"{frame:<12} P={p_str:<8} R={r_str:<8} F1={f1_str:<8} "
              f"(tp={stats['tp']}, fp={stats['fp']}, fn={stats['fn']})")
    
    # Print detailed mismatches
    if mismatch_count > 0:
        print_mismatches(df_comparison)
    else:
        print(f"🎉 Perfect match! All {len(df_comparison)} results consistent with gold standard.")
    
    # Reorganize trace files by new match/mismatch status
    trace_dir = run_dir / "traces_tot"
    if trace_dir.exists():
        logging.info("Reorganizing trace files by new match/mismatch status...")
        
        # Create timestamped trace directories for re-evaluation
        re_eval_trace_dir = run_dir / f"traces_tot_re_eval_{timestamp}"
        re_eval_trace_dir.mkdir(exist_ok=True)
        
        # Copy comparison dataframe with proper column names for reorganize function
        df_for_traces = df_comparison.copy()
        if 'Statement Text' not in df_for_traces.columns:
            # Add a placeholder if Statement Text is missing
            df_for_traces['Statement Text'] = df_for_traces['StatementID']
        
        # Copy original traces to re-evaluation directory, then reorganize
        import shutil
        if trace_dir.exists():
            for trace_file in trace_dir.glob("*.jsonl"):
                if not trace_file.name.startswith("consolidated_"):
                    shutil.copy2(trace_file, re_eval_trace_dir)
        
        reorganize_traces_by_match_status(re_eval_trace_dir, df_for_traces)
        logging.info(f"Trace files reorganized in: {re_eval_trace_dir}")
    
    print(f"\n✅ Re-evaluation complete!")
    print(f"📋 Comparison saved to: {comparison_path}")
    if trace_dir.exists():
        print(f"📂 Reorganized traces in: {re_eval_trace_dir}")

def main():
    """Main entry point for the analysis pipeline."""
    # Setup signal handling for graceful shutdown
    signal.signal(signal.SIGINT, handle_sigint)

    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(description="Run the multi-coder analysis pipeline.")
    parser.add_argument("--config", default="config.yaml", help="Path to configuration file")
    parser.add_argument("--phase", default="test", help="Analysis phase (e.g., pilot, validation, test)")
    parser.add_argument("--coder-prefix", default="model", help="Coder prefix for output files")
    parser.add_argument("--dimension", default="framing", help="Analysis dimension")
    parser.add_argument("--input", help="Path to input CSV file (overrides default input file generation)")
    parser.add_argument("--limit", type=int, help="Limit number of statements to process (for testing)")
    parser.add_argument("--start", type=int, help="Start index for processing (1-based, inclusive)")
    parser.add_argument("--end", type=int, help="End index for processing (1-based, inclusive)")
    parser.add_argument("--concurrency", type=int, default=1, help="Number of statements to process concurrently (default: 1)")
    parser.add_argument("--test", action="store_true", help="Run in test mode")
    parser.add_argument("--gemini-only", action="store_true", help="Use only Gemini models (ignored with --use-tot)")
    parser.add_argument(
        "--use-tot", 
        action="store_true",
        help="Activates the 12-hop Tree-of-Thought reasoning chain instead of the standard multi-model consensus method."
    )
    parser.add_argument("--model", default="models/gemini-2.5-flash-preview-04-17", help="Model to use for LLM calls (e.g., models/gemini-2.0-flash)")
    parser.add_argument("--provider", choices=["gemini", "openrouter"], default="gemini", help="LLM provider to use")
    parser.add_argument("--batch-size", "-b", type=int, default=1, help="Number of segments to process in a single LLM call per hop (default: 1)")
    parser.add_argument('--individual-fallback', action='store_true', help='Re-run mismatches individually for batch-sensitivity check')
    parser.add_argument('--regex-mode', choices=['live', 'shadow', 'off'], default='live', help='Regex layer mode: live (default), shadow (evaluate but do not short-circuit), off (disable regex)')
    parser.add_argument('--shuffle-batches', action='store_true', help='Randomly shuffle active segments before batching at each hop')

    # NEW: eight-order permutation sweep mode
    parser.add_argument('--permutations', action='store_true',
                        help='Run the eight-order permutation sweep instead of a single straight pass')

    # Parallel processes for permutation suite
    parser.add_argument('--perm-workers', type=int, default=1,
                        help='Number of permutations to execute in parallel (process-based). Default 1 (serial).')

    # Threshold for identifying low majority-ratio segments (used in permutation suite)
    parser.add_argument('--low-ratio-threshold', type=float, default=7,
                        help='Majority-label ratio below which a segment is considered low-confidence (default: 7)')

    # Skip evaluation even if Gold Standard column is present
    parser.add_argument('--no-eval', action='store_true',
                        help='Disable any comparison against the Gold Standard column. The pipeline will still run and output majority labels, but no accuracy metrics or mismatch files are created.')

    # Fallback pass: re-run permutations on low_ratio_segments.csv only
    parser.add_argument('--fallback-low-ratio', action='store_true',
                        help='After the normal permutation suite finishes, run an extra pass on low_ratio_segments.csv and write *_fallback files. Off by default.')

    # Custom prompts directory
    parser.add_argument('--prompts-dir', default=str(Path('multi_coder_analysis') / 'prompts'),
                        help='Path to prompts directory to use instead of the package default')

    # Diagnostic: run a single hop only (1-12)
    parser.add_argument('--only-hop', type=int, help='If set, run only the specified hop index (1-12) for diagnostic testing')

    # ------------------------------------------------------------------
    # NEW – Gold standard reference file
    # ------------------------------------------------------------------
    parser.add_argument('--gold-standard', help='Path to gold standard reference CSV file (optional - if not provided, will look for Gold Standard column in input file)')

    # ------------------------------------------------------------------
    # NEW – Re-evaluation of existing runs
    # ------------------------------------------------------------------
    parser.add_argument('--re-evaluate', help='Path to existing run output directory to re-evaluate against a new gold standard (use with --gold-standard)')

    # ------------------------------------------------------------------
    # NEW – Consensus strategy & self-consistency decoding (pipeline mode)
    # ------------------------------------------------------------------
    parser.add_argument('--consensus', default='final', choices=['hop', 'final'],
                        help="Consensus strategy: 'hop' = per-hop majority, 'final' = legacy end-of-tree vote (default)")

    parser.add_argument('--decode-mode', default='normal', choices=['normal', 'self-consistency'],
                        help='Decoding mode: normal (single deterministic path) or self-consistency')

    parser.add_argument('--votes', type=int, default=1, dest='sc_votes',
                        help='# paths/votes for self-consistency')

    parser.add_argument('--sc-rule', default='majority', choices=['majority', 'ranked', 'ranked-raw'],
                        help='Aggregation rule for self-consistency votes')

    parser.add_argument('--sc-temperature', type=float, default=0.7,
                        help='Sampling temperature for self-consistency')

    parser.add_argument('--sc-top-k', type=int, default=40,
                        help='top-k sampling cutoff (0 disables)')

    parser.add_argument('--sc-top-p', type=float, default=0.95,
                        help='nucleus sampling p-value')

    args = parser.parse_args()

    # --- Validate Arguments ---
    if args.start is not None and args.start < 1:
        logging.error("Start index must be >= 1")
        sys.exit(1)
    
    if args.end is not None and args.end < 1:
        logging.error("End index must be >= 1")
        sys.exit(1)
    
    if args.start is not None and args.end is not None and args.start > args.end:
        logging.error("Start index must be <= end index")
        sys.exit(1)
    
    if (args.start is not None or args.end is not None) and args.limit is not None:
        logging.error("Cannot use both --limit and --start/--end arguments together")
        sys.exit(1)

    if args.batch_size < 1:
        logging.error("--batch-size must be >= 1")
        sys.exit(1)

    if args.only_hop is not None and not (1 <= args.only_hop <= 12):
        logging.error("--only-hop must be between 1 and 12")
        sys.exit(1)

    # Validate re-evaluation arguments
    if args.re_evaluate:
        if not args.gold_standard:
            logging.error("--re-evaluate requires --gold-standard to specify the new gold standard file")
            sys.exit(1)
        
        re_eval_dir = Path(args.re_evaluate)
        if not re_eval_dir.exists() or not re_eval_dir.is_dir():
            logging.error(f"Re-evaluation directory does not exist: {args.re_evaluate}")
            sys.exit(1)
            
        # Check for required files
        model_labels_file = re_eval_dir / "model_labels_tot.csv"
        if not model_labels_file.exists():
            logging.error(f"Pipeline results file not found: {model_labels_file}")
            sys.exit(1)

    # --- Load Configuration ---
    if not os.path.exists(args.config):
        # Create a minimal config file if it doesn't exist
        default_config = {
            'logging': {'level': 'INFO'},
            'file_paths': {
                'file_patterns': {
                    'model_majority_output': '{phase}_model_labels.csv'
                }
            }
        }
        with open(args.config, 'w') as f:
            yaml.dump(default_config, f)
        logging.info(f"Created default config file: {args.config}")

    config = load_config(args.config)
    setup_logging(config)

    # ------------------------------------------------------------------
    # Re-evaluation mode: evaluate existing run against new gold standard
    # ------------------------------------------------------------------
    if args.re_evaluate:
        try:
            re_evaluate_run(Path(args.re_evaluate), args.gold_standard)
            return  # Exit after re-evaluation
        except Exception as e:
            logging.error(f"Re-evaluation failed: {e}", exc_info=True)
            sys.exit(1)

    # ------------------------------------------------------------------
    # Permutation mode short-circuits the normal pipeline and delegates
    # to the dedicated suite runner.
    # ------------------------------------------------------------------
    if args.permutations:
        # Package-relative first, then local fallback
        try:
            from .permutation_suite import run_permutation_suite  # type: ignore
        except ImportError:
            try:
                from multi_coder_analysis.permutation_suite import run_permutation_suite  # type: ignore
            except ImportError:
                try:
                    from permutation_suite import run_permutation_suite  # type: ignore
                except ImportError as err:
                    logging.error("Permutation suite could not be imported: %s", err)
                    sys.exit(1)

        try:
            perm_root = run_permutation_suite(config, args, shutdown_event)
        except Exception as e:
            logging.error("Permutation suite failed: %s", e, exc_info=True)
            sys.exit(1)

        # ------------------------------------------------------------------
        # Optional fallback permutation on low_ratio_segments.csv
        # ------------------------------------------------------------------
        if args.fallback_low_ratio:
            import pandas as _pd
            low_csv = Path(perm_root) / "low_ratio_segments.csv"
            if low_csv.exists():
                logging.info("🔁  Fallback permutation pass on %s", low_csv)
                try:
                    df_low = _pd.read_csv(low_csv)
                    run_permutation_suite(
                        config,
                        args,
                        shutdown_event,
                        override_df=df_low,
                        out_dir_suffix="fallback",
                    )
                except Exception as _e:
                    logging.error("Fallback permutation failed: %s", _e, exc_info=True)
            else:
                logging.warning("--fallback-low-ratio requested but %s not found", low_csv)
        return  # Skip the rest of main once permutations complete

    try:
        run_pipeline(config, args.phase, args.coder_prefix, args.dimension, args, shutdown_event)
    except Exception as e:
        logging.error(f"Pipeline failed: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main() 

## 0024. multi_coder_analysis\models\__init__.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from .hop import HopContext, BatchHopContext  # noqa: F401

__all__ = [
    "HopContext",
    "BatchHopContext",
] 

## 0025. multi_coder_analysis\models\hop.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, TypedDict

__all__ = [
    "HopContext",
    "BatchHopContext",
]

# ---------------------------------------------------------------------------
# Typed aliases help downstream static analysis without dict[str, Any] noise.
# ---------------------------------------------------------------------------
AnalysisHistory = List[str]
ReasoningTrace = List[Dict[str, Any]]
RawLLMResponses = List[Dict[str, Any]]


@dataclass
class HopContext:
    """State container for a single segment as it progresses through the 12-hop ToT chain."""

    # -------------- Static Data --------------
    statement_id: str
    segment_text: str
    # Optional article identifier (source document) – used for trace exports
    article_id: Optional[str] = None

    # -------------- Dynamic State --------------
    q_idx: int = 0
    is_concluded: bool = False
    final_frame: Optional[str] = None           # The definitive frame once concluded (e.g., Alarmist, Neutral)
    final_justification: Optional[str] = None   # The rationale for the final decision
    ranking: Optional[list[str]] = None         # ordered list when ranked_list=True

    # Track consecutive "uncertain" responses to support early termination.
    uncertain_count: int = 0

    # -------------- Logging & Audit Trails --------------
    analysis_history: AnalysisHistory = field(default_factory=list)
    reasoning_trace: ReasoningTrace = field(default_factory=list)
    raw_llm_responses: RawLLMResponses = field(default_factory=list)

    # ───────── Batch Positional Meta ─────────
    batch_pos: Optional[int] = None  # 1-based index within API call
    batch_size: Optional[int] = None  # total number of segments in API call

    # -------------- Parsed prompt metadata --------------
    prompt_meta: Dict[str, Any] = field(default_factory=dict, repr=False)

    # ───────── Permutation bookkeeping ─────────
    permutation_idx: int | None = None

    # -------------- Convenience Properties --------------
    @property
    def dim1_frame(self) -> Optional[str]:
        """Alias retained for backward compatibility with downstream scripts."""
        return self.final_frame


@dataclass
class BatchHopContext:
    """Container for a batch of segments processed together at a single hop."""

    batch_id: str
    hop_idx: int
    segments: List[HopContext]

    # Raw LLM I/O for debugging
    raw_prompt: str = ""
    raw_response: str = ""
    thoughts: Optional[str] = None 

## 0026. multi_coder_analysis\regex\hop_patterns.yml
----------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------
# Canonical regex catalogue for 12‑hop pipeline
# File‑name:  hop_patterns.yml        (UTF‑8, no tabs)
# ------------------------------------------------------------------
# Schema
#   <hop_number> (int) :
#     - name          : CamelCase identifier (unique within hop)
#       mode          : live | shadow           # default = live
#       frame         : Alarmist | Reassuring | null
#       pattern       : |-                      # block scalar, preserves NL
#           <raw regex, unchanged>
#       # veto_pattern: |-                      # optional
#           <regex that cancels a positive hit>
#
# Notes
# • Newlines are significant for readability—do **not** re‑wrap patterns.
# • Indent the block scalar exactly two spaces so YAML treats the regex
#   as literal text (nothing is escaped).
# • Keep ordering by hop → pattern name; the engine preserves this order.
# ------------------------------------------------------------------

1:
- name: IntensifierRiskAdjV2
  mode: live
  frame: Alarmist
  veto_pattern: |-
    # TECH_TERM_GUARD (2025-06-20)
    (?ix)
      \b(?:
           highly\s+(?:pathogenic|contagious|transmissible|virulent) |
           high[-\s]?pathogenic |
           high[-\s]?path\B
        )\b
      (?=\s+(?:strain|bird\s+flu|avian(?:\s+flu|\s+influenza)?|h5n1|virus))
  # Added negative look‑behind to exclude "deadly toll", "deadly cost"
  pattern: |-
    (?i)    # case-insensitive once – faster compile
    # block idioms unrelated to epidemiological danger
    (?<!toll\s|cost\s|sins\s|silence\s|so\scalled\s)
    \b(?:(?:highly(?!\s+(?:pathogenic|susceptible)\b))
        |alarmingly|certainly|deadlier|definitely|ever[-\s]*more|extremely|
        frighteningly|more|particularly|progressively|severely|so|unusually|very)  # A-Z order
        (?:\s+\w+){0,3}\s+
    (?:brutal|catastrophic|contagious|dangerous|deadly|destructive|
       infectious|lethal|severe|transmissible|virulent)\b|
    \bless\s+safe\b|                                       # (2025-06-23) comparative risk form
    \bdeadly\s+from\s+(?:the\s+)?(?:start|outset)\b|
    \bmost\s+(?:\w+\s+){0,2}?(?:deadly|destructive|dangerous|severe|catastrophic|devastating|virulent|contagious|lethal)\b
    \b(?:incredibly|unbelievably|increasingly)\s+(?:\w+\s+){0,2}?(?:deadly|dangerous|severe|lethal|catastrophic|virulent|contagious)\b

- name: H1.AutoIntensifierRiskAdj
  mode: live
  frame: Alarmist
  veto_pattern: |-
    # TECH_TERM_GUARD (2025-06-20)
    (?ix)
      \b(?:
           highly\s+(?:pathogenic|contagious|transmissible|virulent) |
           high[-\s]?pathogenic |
           high[-\s]?path\B
        )\b
      (?=\s+(?:strain|bird\s+flu|avian(?:\s+flu|\s+influenza)?|h5n1|virus))
  pattern: 
    (?:highly|particularly)\s+(?:contagious|dangerous|deadly|infectious|lethal|transmissible)\b
2:
- name: HighPotencyVerbMetaphor
  mode: live
  frame: Alarmist
  veto_pattern: |-
    # (a) keep price-trend guard
    \btrending\s+sharply\s+(?:higher|lower)\b
    |
    # (c) macro-context guard – price/temperature/inflation backdrop ≠ impact
    \bsoar(?:ed|ing)?\b(?=\s+(?:inflation|interest[-\s]?rates?|temperatures?)) |
    \bplung(?:ed|ing)\b(?=\s+(?:inflation|interest[-\s]?rates?|temperatures?)) |
    # (b) **containment override** – neutralise culling verbs so they fall
    #     through to Q3 where the containment rule already handles them
    (?i)\b(?:slaughter(?:ed|ing)?|culled?|destroyed?|euthan(?:iz|is)ed|
           depopulated|disposed|buried)\b
  pattern: |-
    # Hop-2 literal guard – keep "spark shortages" Neutral
    (?i)
    # Guard — "spark shortages" stays Neutral
    (?!\b(?:spark|sparking)\s+shortage(?:s)?\b)

    (?:
      # vivid verbs / alert phrases
      \b(?:ravaged|devastated|obliterated|skyrocketed|plummeted|crashed|nosedived|
         tanked|exploding|raging|tearing\sthrough|
         overwhelmed|crippling|spiralled?|ballooned|
         writh(?:e|ed|ing)|convuls(?:e|ed|ing)|gasp(?:ing|ed)|twitch(?:ing|ed))\b
      |
      # verb first:  soar(ed/ing) + metric inside 20 chars after
      \bsoar(?:ed|ing)?\b(?=[^.]{0,20}\b(?:cases?|prices?|costs?|loss(?:es)?|
                                    deaths?|fatalities|production|output|
                                    supply|shortages?)\b)
      |
      # metric first: metric … soar(ed/ing) inside 20 chars after
      \b(?:cases?|prices?|costs?|loss(?:es)?|deaths?|fatalities|production|
          output|supply|shortages?)\b[^.]{0,20}\bsoar(?:ed|ing)?\b
      |
      # superlative-negative nouns
      \b(?:most|record(?:-breaking)?|worst)\s+\w{0,12}?
           \s+(?:disaster|crisis|outbreak|catastrophe|calamity)\b
      |
      # potent metaphors (explicit list for deterministic hits)
      \b(?:ticking\s+time-?bomb|nightmare\s+scenario|powder\s+keg|
         house\s+of\s+cards|train\s+wreck|collateral\s+damage)\b
      |
      # "on high alert" forms
      (?:on\s+high\s+alert(?=[^.]{0,40}\b(?:outbreak|virus|flu|disease|h5n1|
                                           threat|danger|risk)\b)|
       (?:outbreak|virus|flu|disease|h5n1|threat|danger|risk)
         \b[^.]{0,40}on\s+high\s+alert|
       ^[^.]{0,60}\bon\s+high\s+alert\b)
      |
      # spark / stoke *panic-type* emotions  (plain "fears" stays Neutral)
      \b(?:spark|stoke|fuel|reignit(?:e|ing|ed|es))\s+
         (?:mass(?:ive)?\s+|widespread\s+|public\s+|nationwide\s+|
            global\s+)?(?:panic|alarm|outrage|anxiety)\b
      |
      # intensifier + harm-noun (alphabetised for readability)
      \b(?:enormous|gigantic|huge|immense|major|massive|
         record(?:-breaking)?|severe|significant|unprecedented|vast)\s+
         (?:crisis|damage|deaths?|disaster|fatalities|
         loss(?:es)?|mortality|outbreaks?|shortages?|toll)\b
    )

# 2025-06-20 • Zero-FP rule promoted to live
- name: OnHighAlert.Live
  mode: live
  frame: Alarmist
  pattern: (?ix)\bon\W+high\W+alert\b

3:
- name: ModerateVerbPlusScale
  mode: live
  frame: Alarmist
  pattern: |-
    \b(hit|hitting|swept|sweeping|surged|soared|plunged|plummeted|
       spiked|jumped|shot\s+up|prompted(?!\s+authorities\s+to\s+consider))\b
    (?=[^.]{0,120}\b(?:\d|%|percent|hundred|hundreds|thousand|thousands|million|millions|billion|billions|record|
                     largest|unprecedented|severe|significant|overwhelming|
                     devastating|disasters?|emergenc(?:y|ies))\b)
    (?![^.]{0,20}\bfear(?:s|ed|ing)?\b)   # guard: psychological verbs ≠ impact

    # 2025-06-18 containment-verb veto – keeps large-scale culling Neutral
  veto_pattern: |-
    # extend veto to "disposed" and "buried"
    (?i)\b(?:culled?|euthani[sz]ed|destroyed|depopulated|slaughtered|disposed|buried)\b

- name: ScaleMultiplier
  mode: live
  frame: Alarmist
  pattern: |-
    (?i)\b(?:double[ds]?|triple[ds]?|quadruple[ds]?|ten[-\s]*fold)\b

4:
- name: LoadedQuestionAlarm
  mode: live
  frame: Alarmist
  pattern: |-
    \b(?:should|can|could|will)\s+\w+\s+(?:be\s+)?(?:worried|concerned|afraid)\b
    (?=[^.?]{0,40}\b(?:outbreak|virus|flu|disease|h5n1|threat|danger|
                     risk|infection|infected)\b)
    |
    # Rhetorical necessity-of-killing question (captures 'necessary to kill millions...')
    \b(?:is|are|was|were)\s+it\s+(?:really\s+)?necessary\s+to\s+
        (?:kill|cull|slaughter|destroy|euthan(?:ize|ise))\b
        [^?]{0,60}?\b(?:millions?|thousands?|record|\d{1,3}(?:[, ]\d{3})+)\b
    |
    # safety-question pattern (2025-06-23)
    \b(?:is|are|was|were)\s+it\s+(?:really\s+)?safe\s+to\b
    |
    # "How many more … have to die?” rhetorical question
    \bhow\s+many\s+more\s+\w+\s+have\s+to\s+(?:die|be\s+killed|be\s+culled|perish)\b

  # --- 2025-06-18 addition: Challenge-question over inaction ---

- name: WhatIfQuestion
  mode: live
  frame: Alarmist
  pattern: |-
    (?i)\bwhat\s+if\s+(?:we|this|the\s+\w+)\b

- name: IgnoreDisasterQ
  mode: live
  frame: Alarmist
  pattern: |-
    (?i)\bhow\s+long\s+can\s+we\s+(?:afford\s+to\s+)?(?:ignore|stand\s+by)\b

5:
- name: ExplicitCalming
  mode: live
  frame: Reassuring
  pattern: |-
    \bwe\b(?:\s*\[[^\]]+\]\s*)?\s+(?:are|remain|feel)\s+
        (?:confident|positive|optimistic|hopeful)\s+in\s+(?:\w+\s+){0,8}?(?:preparedness|readiness|ability)\b|
    \b(?:are|feel)\s+(?:confident|positive|optimistic|hopeful)\s+in\s+(?:\w+\s+){0,8}?(?:preparedness|readiness|ability)\b|
    \b(?:no\s+cause\s+for\s+alarm|
        public\s+can\s+rest\s+easy|
        fully\s+under\s+control|
        rest\s+assured|
        completely\s+safe|
        (risk|likelihood|chance)\s+(?:of\s+\w+\s+)?(?:is|are|remains|stay|stays)\s+(?:very|extremely|exceptionally|remarkably)\s+low|
        (?:is|are|remains|remain|stay|stays)\s+(?:completely\s+|totally\s+|perfectly\s+|entirely\s+)?safe\s+(?:to\s+eat|for\s+(?:human\s+)?consumption|for\s+(?:all\s+)?(?:consumers?|people|humans|residents|citizens))|
        \b(?:encouraging|welcome|heartening|excellent)\s+news\b|
        \bwonderfully\s+high\b|
        \bvery\s+well\s+protected\b|
        \bprovid(?:ing|es)\s+relief\b|
        \bshort[-\s]?term\s+blip\b|
        \btemporary\s+setback\b|
        (?:expected|likely|set)\s+to\s+resolve\s+quickly\b|
        \bthankfully\b[^.]{0,40}\b(?:consumers?|public|residents|citizens|people)\b|
        \b(?:fully|well)\s+(?:prepared|ready)\s+(?:to\s+(?:handle|deal\s+with)|for)
          .{0,40}\b(?:public|consumers?|customers?|people|residents|citizens)\b|
        \b[Ff]ortunately\b[^.]{0,40}\b(?:consumers?|public|residents|citizens|people)\b)
    | # generic optimism/confidence without "in X" clause
    \b(?:i|we|they|officials?|authorities?)\s+
      (?:feel|are)\s+
      (?:positive|optimistic|hopeful)\s+
      (?:that|about)\b

- name: ExplicitCalming.SafeToEat.Live
  mode: live
  frame: Reassuring
  pattern: |
    \b(remains?|is|are)\s+(?:perfectly\s+)?safe\s+(?:to\s+eat|for\s+(?:human\s+)?consumption)\b

# 2025-06-20 • Zero-FP rules promoted to live
- name: DirectNoConcern.Live
  mode: live
  frame: Reassuring
  pattern: (?ix)\bno\W+cause\W+for\W+(?:alarm|concern)\b

- name: NothingToWorry.Live
  mode: live
  frame: Reassuring
  pattern: (?ix)\bnothing\W+to\W+worry\W+about\b

- name: LowRiskEval.Theoretical
  mode: live
  frame: Reassuring
  pattern: (?i)\b(?:purely\s+)?theoretical\s+risk\b

6:
- name: MinimiserScaleContrast
  mode: live
  frame: Reassuring
  pattern: |-
    # minimiser MUST be paired with an explicit denominator token
    (?i)
    \b(?:only|just|merely|a\s+single|very\s+few|relatively\s+few|no\s+more\s+than)\b
         [^.;\n]{0,30}
         \b(?:out\s+of|one\s+of|one\s+in|among|nationwide|statewide|across|worldwide|globally)\b
         [^.;\n]{0,30}\b(?:hundred|thousand|million|billion|
                        \d{1,3}(?:[, ]\d{3})*|\d+)\b
    |
    \b(?:only|just|merely)\s+\d+(?:[.,]\d+)?\s*(?:%|percent|per\s+cent)\b
    |
    \b(?:only|just|merely)\s+one\b[^.]{0,120}
         \b(?:of|in|among)\b[^.]{0,20}\bthousands?\b
    |
    # allow dash or parenthesis between parts
    \b(?:only|just|merely|a\s+single|very\s+few)\b
         [^.;\n]{0,50}?\b                  # tolerant gap
         \b(out\s+of|among|in|of)\b
         [^.;\n]{0,50}?\b(total|overall|population|flocks?|barns?|nationwide|worldwide|global(?:ly)?)\b

7:
- name: BareNegationHealthConcern
  mode: live
  frame: Neutral
  pattern: |-
    \b(?:do|does|did|is|are|was|were|will|would|should)\s+(?:not|n't)\s+
       (?:pose|present|constitute)\s+(?:an?\s+)?(?:immediate\s+)?(?:public\s+)?health\s+concern\b
    |
    \bno\s+(?:human|americans?|animal|bird|poultry)\s+cases?\s+
       (?:have|has|are|were)\s+(?:been\s+)?(?:detected|reported|
                                          recorded|found|identified)\b
    |
    \b(?:will|would|can|could)\s+not\s+enter\s+the\s+food\s+(?:system|chain|supply)\b
    |
    \b(?:tests?|samples?)\s+(?:came|come|were|was)\s+negative\b
    |
    \b(?:does|do|is|are|will|would|should)\s+(?:not|n't)\s+
       pose\s+(?:an?\s+)?(?:any\s+)?risk\b
    |
    \b(?:pose|present|constitute)\s+no\s+(?:public\s+)?health\s+(?:threat|risk)\b
    |
    \bno\s+(?:sign|indication|evidence)\s+of\s+(?:spread|transmission)\b
    |
    \b(?:is|are|was|were|will|would|could|may|might|should)\s+unlikely\s+to\s+
       (?:affect|impact|cause|pose|present|enter|spread|infect)\b
    |
    \b(?:has|have|had|did)\s+(?:not|n't)\s+
        detect(?:ed)?\s+(?:any\s+)?(?:further|additional|new)\s+
        (?:positive\s+)?samples?\b

- name: BareNegation.PosesNoRisk.Live
  mode: live
  frame: Neutral
  pattern: |
    \b(?:poses?|present(?:s)?)\s+no\s+risk\b

- name: BareNegation.NotContaminate.Live
  mode: live
  frame: Neutral
  # 2025-06-20 • broadened to cover "has not detected / identified … cases"
  pattern: |-
    (?ix)
    \b(?:has|have|does|do|did)\b
    \s+(?:not|n't|never|yet\s+to)\s+
    (?:contaminat(?:e|ed)|infect(?:ed)?)\b

# NEW — 2025-06-20 -----------------------------------------------------------
- name: BareNegation.NoFurtherCases.Live
  mode: live
  frame: Neutral
  pattern: |-
    (?i)\b(?:has|have|had|did)\s+(?:not|n't)\s+
         (?:detected|identified|reported|found)\s+
         (?:any\s+)?(?:further|additional|new)\s+cases?\b

# NEW — 2025-06-20 -----------------------------------------------------------  
- name: BareNegation.NothingSuggests.Live
  mode: live
  frame: Neutral
  pattern: |-
    (?ix)
    \bnothing\s+(?:currently\s+)?(?:in\s+the\s+)?
    (?:data|evidence|sequence|analysis|results)?\s*suggests?
    \s+(?:that\s+)?(?:the\s+)?(?:virus|situation)?\s+
    (?:has\s+become|is|will\s+be|has\s+grown)\s+
    (?:more\s+)?(?:dangerous|contagious|infectious|severe|deadly|threatening)\b

# 2025-06-20 • Zero-FP rule promoted to live
- name: NoThreatNeutral.Live
  mode: live
  frame: Neutral
  pattern: (?ix)\bdoes\W+not\W+pose\W+a?\W+threat\b

8:
- name: CapabilityNoReassurance
  mode: live
  frame: Neutral
  pattern: |-
    (?i)
    \b(?:(?:fully|well)\s+(?:prepared|ready)\s+(?:to\s+(?:handle|deal\s+with)|for)\b(?![^.]{0,40}\b(?:consumers?|customers?|people|public|residents|citizens)\b)|
       (?:capability|contain|develop|implement|measure|
       monitor(?:ing)?|officials|plan|prepare|protocol|
       resource(?:s)?|safeguard|stockpile(?:d|s|ing)?|
       surveillance|system|vaccine|work))\b

9:
- name: NeutralPriceMetrics
  mode: live
  frame: Neutral
  pattern: |-
    (?is)
    \b(?:
          # economic nouns
          (?:prices?|rates?|costs?|loss(?:es)?|profit(?:s)?|revenue|
             value|export(?:s)?|import(?:s)?|sale(?:s)?|output|production)
          \b[^.]{0,120}?                     # allow anything up to the verb (≤ one sentence)
          (?:rose|declined|increased|fell|dropped|gained|lost)\b
        | # "prices were up /down 2 %" form
          (?:prices?|rates?)\s+(?:were|was)\s+(?:up|down)\s+\d+(?:[.,]\d+)?\s*%
        | # PATCH 2b – claim "trending sharply higher/lower" as Neutral
          \b(?:prices?|costs?|rates?|values?|export(?:s)?|import(?:s)?|sale(?:s)?)\b
            [^.]{0,50}?\btrending\s+sharply\s+(?:higher|lower)\b
        | # volatility (neutral)
          \b(?:prices?|rates?|costs?|values?|export(?:s)?|import(?:s)?|sale(?:s)?)\b
            [^.]{0,80}?\b(?:be(?:come|came|coming)|are|were)\s+(?:more\s+)?volatile\b
    )

10:
- name: ReliefSpeculation
  mode: live
  frame: Neutral
  pattern: |-
    \b(may\ be|could|might|expect.{1,15}improve|predict.{1,15}ease|hope.{1,15}better)\b

11:
  # ─────────────────────────────────────────────────────────────
  #  Hop 11 – "Primacy of Framed Quotations"
  #  Two explicit patterns:
  #    • DominantQuoteAlarmist     → frame: Alarmist
  #    • DominantQuoteReassuring   → frame: Reassuring
  #  First match wins; if both miss, the LLM prompt executes.
  # ─────────────────────────────────────────────────────────────

- name: DominantQuoteAlarmist
  mode: live
  frame: Alarmist
  veto_pattern: |-
    (?i)\bhighly\s+pathogenic\s+(?:avian\s+flu|influenza|avian)\b
  pattern: |-
    (?is)                                    # i=ignore case, s=dot=nl
    ["'\u2018\u2019\u201C\u201D]             # opening quote (straight or curly)
    [^"'\u2018\u2019\u201C\u201D]{0,600}?    # up to 600 chars inside
    \b(?:                                     # key alarmist cues
         (?:extremely|highly|very|deeply|incredibly|particularly|
            frighteningly|definitely|certainly)\s+\w{0,3}\s+
               (?:concerning|alarming|worrying|dangerous|severe|
                  catastrophic|critical|high[-\s]*risk)
       | period\s+of\s+high[-\s]*risk
       | requires\s+immediate\s+action
       | (?:troubling|dire)\s+situation
      )\b
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    ["'\u2018\u2019\u201C\u201D]             # closing quote

- name: DominantQuoteReassuring
  mode: live
  frame: Reassuring
  pattern: |-
    (?is)
    ["'\u2018\u2019\u201C\u201D]
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    \b(?:                                     # key reassuring cues
         no\s+cause\s+for\s+alarm
       | fully\s+under\s+control
       | excellent\s+news
       | very\s+well\s+protected
       | risk\s+(?:is|remains|stays)\s+
             (?:very|extremely|exceptionally|remarkably)\s+low
       | (?:completely|totally|entirely|perfectly|absolutely)\s+safe
       | wholly\s+under\s+control
       | safe\s+to\s+eat
      )\b
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    ["'\u2018\u2019\u201C\u201D]

- name: BaseRiskAdjQuoteNeutral          # NEW – blocks base risk adj only
  mode: live
  frame: Neutral
  pattern: |-
    (?is)
    ["'\u2018\u2019\u201C\u201D]             # opening quote
    [^"'\u2018\u2019\u201C\u201D]{0,600}?    # quote body
    \b(?:deadly|dangerous|severe|lethal|virulent|contagious|catastrophic)\b
    (?![^"'\u2018\u2019\u201C\u201D]{0,60}?
        (?:extremely|highly|very|deeply|incredibly|particularly|
           frighteningly|definitely|certainly|so|more|deadlier)\b)
    [^"'\u2018\u2019\u201C\u201D]{0,600}?
    ["'\u2018\u2019\u201C\u201D]

12:
- name: NeutralStats
  mode: live
  frame: Neutral
  pattern: |-
    \b(report|document|state|announce|confirm|detect|identify|record)\w*\b.*\b(cases|deaths|losses|rates|numbers|percent)\b


## 0027. multi_coder_analysis\run_multi_coder_tot.py
----------------------------------------------------------------------------------------------------
"""
Deterministic 12-hop Tree-of-Thought (ToT) coder.
This module is an alternative to run_multi_coder.py and is activated via --use-tot.
It processes an input CSV of statements through a sequential, rule-based reasoning
chain, producing a single, deterministic label for each statement.
"""
from __future__ import annotations
import json
import time
import logging
import os
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
from collections import defaultdict
import collections
import re
import random  # For optional shuffling of segments before batching

import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import shutil
import uuid

# Local project imports
from hop_context import HopContext, BatchHopContext
from multi_coder_analysis.models import HopContext, BatchHopContext
from llm_providers.gemini_provider import GeminiProvider
from llm_providers.openrouter_provider import OpenRouterProvider
from multi_coder_analysis.providers import get_provider
from utils.tracing import write_trace_log
from utils.tracing import write_batch_trace
from multi_coder_analysis.core.prompt import parse_prompt

# Backward compatibility alias during refactor
load_prompt_and_meta = parse_prompt

# --- Hybrid Regex Engine ---
try:
    from . import regex_engine as _re_eng  # when imported as package
    from . import regex_rules as _re_rules
except ImportError:
    # Fallback when running as script
    import regex_engine as _re_eng  # type: ignore
    import regex_rules as _re_rules  # type: ignore

# Load environment variables from .env file
load_dotenv(Path(__file__).parent.parent / ".env")

# Constants can be moved to config.yaml if more flexibility is needed
TEMPERATURE = 0.0
MAX_RETRIES = 10
BACKOFF_FACTOR = 1.5
if "PROMPTS_DIR" not in globals():
    PROMPTS_DIR = Path(__file__).parent / "prompts"

# ---------------------------------------------------------------------------
# Helpers to (lazily) read header / footer each time so that tests that monkey-
# patch PROMPTS_DIR *after* import still pick up the temporary files.
# ---------------------------------------------------------------------------


def _load_global_header() -> str:  # noqa: D401
    path = PROMPTS_DIR / "GLOBAL_HEADER.txt"
    if not path.exists():
        # Legacy support – remove once all deployments updated
        path = PROMPTS_DIR / "global_header.txt"
    try:
        return path.read_text(encoding="utf-8")
    except FileNotFoundError:
        logging.debug("Global header file not found at %s", path)
        return ""


def _load_global_footer() -> str:
    path = PROMPTS_DIR / "GLOBAL_FOOTER.txt"
    try:
        return path.read_text(encoding="utf-8")
    except FileNotFoundError:
        logging.debug("Global footer file not found at %s", path)
        return ""

# Map question index to the frame assigned if the answer is "yes"
Q_TO_FRAME = {
    1: "Alarmist", 2: "Alarmist", 3: "Alarmist", 4: "Alarmist",
    5: "Reassuring", 6: "Reassuring",
    7: "Neutral", 8: "Neutral", 9: "Neutral", 10: "Neutral",
    # Hop 11 (dominant‑quote check) is now *strictly* resolved by an explicit
    # "||FRAME=Alarmist / Reassuring" token in the LLM reply.  When that token
    # is missing we *always* drop to Neutral and log a warning.  Never map to
    # a pseudo‑label such as "Variable".
    11: "Neutral",
    12: "Neutral",
}

# --- LLM Interaction ---

def _assemble_prompt(ctx: HopContext) -> Tuple[str, str]:
    """Dynamically assembles the full prompt for the LLM for a given hop."""
    try:
        hop_file = PROMPTS_DIR / f"hop_Q{ctx.q_idx:02}.txt"

        # --- NEW: strip YAML front-matter and capture metadata ---
        hop_body, meta = load_prompt_and_meta(hop_file)
        ctx.prompt_meta = meta  # save for downstream consumers

        # Simple template replacement
        user_prompt = hop_body.replace(
            "{{segment_text}}", ctx.segment_text
        ).replace("{{statement_id}}", ctx.statement_id)

        # Prefer new canonical filename; fallback to legacy if absent
        header_path = hop_file.parent / "GLOBAL_HEADER.txt"
        if not header_path.exists():
            header_path = hop_file.parent / "global_header.txt"
        try:
            local_header = header_path.read_text(encoding="utf-8")
        except FileNotFoundError:
            local_header = _load_global_header()

        local_footer = ""
        try:
            local_footer = (hop_file.parent / "GLOBAL_FOOTER.txt").read_text(encoding="utf-8")
        except FileNotFoundError:
            local_footer = _load_global_footer()

        system_block = local_header + "\n\n" + hop_body
        user_block = user_prompt + "\n\n" + local_footer
        return system_block, user_block

    except FileNotFoundError:
        logging.error(f"Error: Prompt file not found for Q{ctx.q_idx} at {hop_file}")
        raise
    except Exception as e:
        logging.error(f"Error assembling prompt for Q{ctx.q_idx}: {e}")
        raise

def _call_llm_single_hop(
    ctx: HopContext,
    provider,
    model: str,
    temperature: float = TEMPERATURE,
    *,
    top_k: int | None = None,
    top_p: float | None = None,
    ranked: bool = False,
    max_candidates: int = 5,
) -> Dict[str, str]:
    """Makes a single, retrying API call to the LLM for one hop."""
    sys_prompt, user_prompt = _assemble_prompt(ctx)
    
    for attempt in range(MAX_RETRIES):
        try:
            # Use provider abstraction
            raw_text = provider.generate(sys_prompt, user_prompt, model, temperature, top_k=top_k, top_p=top_p)
            
            # Handle cases where content might be empty
            if not raw_text.strip():
                logging.warning(f"Q{ctx.q_idx} for {ctx.statement_id}: Response content is empty. This may indicate a token limit or safety issue.")
                raise ValueError("Response content is empty")
            
            content = raw_text.strip()
            
            # Handle markdown-wrapped JSON responses
            if content.startswith('```json') and content.endswith('```'):
                # Extract JSON from markdown code block
                json_content = content[7:-3].strip()  # Remove ```json and ```
            elif content.startswith('```') and content.endswith('```'):
                # Handle generic code block
                json_content = content[3:-3].strip()  # Remove ``` and ```
            else:
                json_content = content
            
            # The model is instructed to reply with JSON only.
            parsed_json = json.loads(json_content)
            
            # Basic validation of the parsed JSON structure
            if "answer" in parsed_json and "rationale" in parsed_json:
                result = {
                    "answer": str(parsed_json["answer"]), 
                    "rationale": str(parsed_json["rationale"])
                }
                # Note: Thoughts handling removed for simplicity in provider abstraction
                return result
            else:
                logging.warning(f"Q{ctx.q_idx} for {ctx.statement_id}: LLM response missing 'answer' or 'rationale'. Content: {content}")
                # Fall through to retry logic

        except json.JSONDecodeError as e:
            logging.warning(f"Q{ctx.q_idx} for {ctx.statement_id}: Failed to parse LLM JSON response on attempt {attempt + 1}. Error: {e}. Content: {content}")
        except Exception as e:
            logging.warning(f"Q{ctx.q_idx} for {ctx.statement_id}: API error on attempt {attempt + 1}: {e}. Retrying after backoff.")

        time.sleep(BACKOFF_FACTOR * (2 ** attempt)) # Exponential backoff

    # If all retries fail
    logging.error(f"Q{ctx.q_idx} for {ctx.statement_id}: All {MAX_RETRIES} retries failed.")
    return {"answer": "uncertain", "rationale": f"LLM call failed after {MAX_RETRIES} retries."}

# --- Response Processing Helper ---

def _apply_llm_response(ctx: HopContext, llm_response: dict) -> None:
    """Apply LLM response to HopContext, handling hop 11 token guard logic."""
    answer = llm_response.get("answer", "uncertain").lower().strip()
    rationale = llm_response.get("rationale", "No rationale provided.")
    hop_idx = getattr(ctx, 'current_hop', None) or getattr(ctx, 'q_idx', 11)
    
    if answer == "yes":
        # ──────────────────────────────────────────────────────────
        # HOP‑11 SPECIAL‑CASE — token‑guard then safe fallback
        # ──────────────────────────────────────────────────────────
        if hop_idx == 11:
            token = re.search(r"\|\|FRAME=(Alarmist|Reassuring)\b", rationale)
            if token:
                ctx.final_frame = token.group(1)
                ctx.final_justification = (
                    f"Frame determined by Q11 explicit token {token.group(0)}."
                )
            else:
                logging.warning(
                    "Q11 answered 'yes' but missing ||FRAME token; "
                    "defaulting to Neutral  (SID=%s)", ctx.statement_id
                )
                ctx.final_frame = "Neutral"
                ctx.final_justification = (
                    "Hop 11 'yes' without explicit token – forced Neutral."
                )
            ctx.is_concluded = True
            ctx._resolved_by_llm_this_hop = True

        # All other hops use the static map
        elif hop_idx in Q_TO_FRAME:
            ctx.final_frame = Q_TO_FRAME[hop_idx]
            ctx.final_justification = f"Frame determined by Q{hop_idx} trigger. Rationale: {rationale}"
            ctx.is_concluded = True
            ctx._resolved_by_llm_this_hop = True

# --- Core Orchestration ---

def run_tot_chain(segment_row: pd.Series, provider, trace_dir: Path, model: str, token_accumulator: dict, token_lock: threading.Lock, temperature: float = TEMPERATURE) -> HopContext:
    """Orchestrates the 12-hop reasoning chain for a single text segment."""
    ctx = HopContext(
        statement_id=segment_row["StatementID"],
        segment_text=segment_row["Statement Text"],
        article_id=segment_row.get("ArticleID", "")
    )
    
    # Ensure positional metadata exists even when processed individually
    ctx.batch_size = 1
    ctx.batch_pos = 1
    
    # Assign a sentinel batch_id for single-segment path
    ctx.batch_id = "single"  # type: ignore[attr-defined]
    
    uncertain_streak = 0

    for q_idx in range(1, 13):
        # Log progress for single-segment execution
        _log_hop(q_idx, 1, token_accumulator.get('regex_yes', 0), 0)
        ctx.q_idx = q_idx
        # --- metrics counter ---
        with token_lock:
            token_accumulator['total_hops'] += 1
        
        # --------------------------------------
        # 1. Try conservative regex short-circuit
        # --------------------------------------
        regex_ans = None
        try:
            regex_ans = _re_eng.match(ctx)
        except Exception as exc:
            logging.warning(f"Regex engine error on {ctx.statement_id} Q{q_idx}: {exc}")

        provider_called = False

        if regex_ans:
            llm_response = {
                "answer": regex_ans["answer"],
                "rationale": regex_ans["rationale"],
            }
            frame_override = regex_ans.get("frame")
            via = "regex"
            regex_meta = regex_ans.get("regex", {})
            with token_lock:
                if _re_eng._FORCE_SHADOW:
                    token_accumulator['regex_hit_shadow'] += 1
                else:
                    token_accumulator['regex_yes'] += 1
        else:
            llm_response = _call_llm_single_hop(ctx, provider, model, temperature)
            frame_override = None
            provider_called = True
            via = "llm"
            regex_meta = None
            with token_lock:
                token_accumulator['llm_calls'] += 1
        
        ctx.raw_llm_responses.append(llm_response)
        
        # --------------------------------------------------------------
        # NEW: Support ranked-list output
        # --------------------------------------------------------------
        raw_answer_text = llm_response.get("answer", "") or ""
        choice, ranking = _extract_frame_and_ranking(raw_answer_text)

        if ranking:
            _MAX_KEEP = 5  # fallback when pipeline config not available in this scope
            ctx.ranking = ranking[:_MAX_KEEP]
            # fall back to top choice for downstream yes/uncertain logic
            choice = ranking[0] if ranking else choice

        choice = (choice or "uncertain").lower().strip()
        rationale = llm_response.get("rationale", "No rationale provided.")
        
        # Update logs and traces
        trace_entry = {
            "Q": q_idx,
            "answer": choice,
            "rationale": rationale,
            "via": via,
            "regex": regex_meta,
            "batch_id": ctx.batch_id,
            "batch_size": ctx.batch_size,
            "batch_pos": ctx.batch_pos,
            # Include raw statement text and article ID for easier debugging across all traces
            "statement_text": ctx.segment_text,
            "article_id": ctx.article_id,
        }
        
        # Add thinking traces if available
        thoughts = provider.get_last_thoughts()
        if thoughts:
            trace_entry["thoughts"] = thoughts
        
        # --- Token accounting ---
        if provider_called:
            usage = provider.get_last_usage()
            if usage:
                with token_lock:
                    token_accumulator['prompt_tokens'] += usage.get('prompt_tokens', 0)
                    token_accumulator['response_tokens'] += usage.get('response_tokens', 0)
                    token_accumulator['thought_tokens'] += usage.get('thought_tokens', 0)
                    token_accumulator['total_tokens'] += usage.get('total_tokens', 0)
        
        ctx.analysis_history.append(f"Q{q_idx}: {choice}")
        ctx.reasoning_trace.append(trace_entry)
        write_trace_log(trace_dir, ctx.statement_id, trace_entry)

        if choice == "uncertain":
            uncertain_streak += 1
            if uncertain_streak >= 3:
                ctx.final_frame = "LABEL_UNCERTAIN"
                ctx.is_concluded = True
                ctx.final_justification = f"ToT chain terminated at Q{q_idx} due to 3 consecutive 'uncertain' responses."
                break
        else:
            uncertain_streak = 0 # Reset streak on a clear answer

        if choice == "yes":
            # ──────────────────────────────────────────────────────────
            # HOP‑11 SPECIAL‑CASE — token‑guard then safe fallback
            # ──────────────────────────────────────────────────────────
            if q_idx == 11 and not frame_override:
                token = re.search(r"\|\|FRAME=(Alarmist|Reassuring)\b", rationale)
                if token:
                    ctx.final_frame = token.group(1)
                    ctx.final_justification = (
                        f"Frame determined by Q11 explicit token {token.group(0)}."
                    )
                else:
                    logging.warning(
                        "Q11 answered 'yes' but missing ||FRAME token; "
                        "defaulting to Neutral  (SID=%s)", ctx.statement_id
                    )
                    ctx.final_frame = "Neutral"
                    ctx.final_justification = (
                        "Hop 11 'yes' without explicit token – forced Neutral."
                    )
                ctx.is_concluded = True
                ctx._resolved_by_llm_this_hop = True

            # All other hops use the static map or frame override
            else:
                ctx.final_frame = frame_override or Q_TO_FRAME[q_idx]
                ctx.is_concluded = True
                # Frame override from regex, else standard mapping
                if frame_override:
                    ctx.final_justification = f"Frame determined by Q{q_idx} trigger. Rationale: {rationale}"
                else:
                    ctx.final_justification = f"Frame determined by Q{q_idx} trigger. Rationale: {rationale}"
                ctx._resolved_by_llm_this_hop = True
            break # Exit the loop on the first 'yes'

    # If loop completes without any 'yes' answers
    if not ctx.is_concluded:
        ctx.final_frame = "Neutral" # Default outcome
        ctx.final_justification = "Default to Neutral: No specific framing cue triggered in Q1-Q12."
        ctx.is_concluded = True

    return ctx

# --- NEW: Batch Prompt Assembly ---

def _assemble_prompt_batch(segments: List[HopContext], hop_idx: int) -> Tuple[str, str]:
    """Assemble a prompt that contains multiple segments for the same hop."""
    try:
        hop_file = PROMPTS_DIR / f"hop_Q{hop_idx:02}.txt"
        hop_content, meta = load_prompt_and_meta(hop_file)

        # Attach same meta to every HopContext in this batch for consistency
        for ctx in segments:
            ctx.prompt_meta = meta

        # Remove any single-segment placeholders
        hop_content = hop_content.replace("{{segment_text}}", "<SEGMENT_TEXT>")
        hop_content = hop_content.replace("{{statement_id}}", "<STATEMENT_ID>")

        # Enumerate the segments
        segment_block_lines = []
        for idx, ctx in enumerate(segments, start=1):
            segment_block_lines.append(f"### Segment {idx} (ID: {ctx.statement_id})")
            segment_block_lines.append(ctx.segment_text)
            segment_block_lines.append("")
        segment_block = "\n".join(segment_block_lines)

        instruction = (
            f"\nYou will answer the **same question** (Q{hop_idx}) for EACH segment listed below.\n"
            "Respond with **one JSON array**. Each element must contain: `segment_id`, `answer`, `rationale`.\n"
            "Return NOTHING except valid JSON.\n\n"
        )

        # Prefer new canonical filename; fallback to legacy if absent
        header_path = hop_file.parent / "GLOBAL_HEADER.txt"
        if not header_path.exists():
            header_path = hop_file.parent / "global_header.txt"
        try:
            local_header = header_path.read_text(encoding="utf-8")
        except FileNotFoundError:
            local_header = _load_global_header()

        local_footer = ""
        try:
            local_footer = (hop_file.parent / "GLOBAL_FOOTER.txt").read_text(encoding="utf-8")
        except FileNotFoundError:
            local_footer = _load_global_footer()

        system_block = local_header + "\n\n" + hop_content
        user_block = instruction + segment_block + "\n\n" + local_footer
        return system_block, user_block
    except Exception as e:
        logging.error(f"Error assembling batch prompt for Q{hop_idx}: {e}")
        raise

# --- NEW: Batch LLM Call ---

def _call_llm_batch(batch_ctx, provider, model: str, temperature: float = TEMPERATURE, *, ranked: bool = False, max_candidates: int = 5):
    """Call the LLM on a batch of segments for a single hop and parse the JSON list response."""
    sys_prompt, user_prompt = _assemble_prompt_batch(batch_ctx.segments, batch_ctx.hop_idx)
    batch_ctx.raw_prompt = sys_prompt

    unresolved = list(batch_ctx.segments)
    collected: list[dict] = []

    attempt = 0
    consecutive_none = 0  # track consecutive None/empty responses
    while unresolved and attempt < MAX_RETRIES:
        attempt += 1
        sys_prompt, user_prompt = _assemble_prompt_batch(unresolved, batch_ctx.hop_idx)
        try:
            size_requested = len(unresolved)

            try:
                raw_text = provider.generate(sys_prompt, user_prompt, model, temperature)
            except Exception as e:
                logging.warning(f"Error generating batch response: {e}")
                raise

            if not raw_text.strip():
                raise ValueError("Empty response from LLM")

            content = raw_text.strip()
            if content.startswith('```json') and content.endswith('```'):
                content = content[7:-3].strip()
            elif content.startswith('```') and content.endswith('```'):
                content = content[3:-3].strip()

            try:
                parsed = json.loads(content)
                if not isinstance(parsed, list):
                    raise ValueError("Batch response is not a JSON array")
            except json.JSONDecodeError as dex:
                # ------------------------------------------------------
                # Truncated / malformed JSON – attempt best-effort salvage
                # ------------------------------------------------------
                import re as _re, json as _json

                obj_txts = _re.findall(r"\{[^{}]*\}", content)
                parsed = []
                for _frag in obj_txts:
                    try:
                        parsed.append(_json.loads(_frag))
                    except Exception:
                        # Skip fragments that still fail to parse
                        continue

                logging.warning(
                    f"Batch {batch_ctx.batch_id} Q{batch_ctx.hop_idx:02}: salvaged {len(parsed)} objects from truncated JSON (original error: {dex})"
                )

                if not parsed:
                    raise  # re-raise to trigger retry logic

            # Basic validation & matching
            valid_objs = []
            returned_ids = set()
            for obj in parsed:
                if not all(k in obj for k in ("segment_id", "answer", "rationale")):
                    continue  # skip malformed entry
                sid = str(obj["segment_id"]).strip()
                returned_ids.add(sid)
                valid_objs.append(obj)

            collected.extend(valid_objs)

            # Filter unresolved list to those still missing
            unresolved = [c for c in unresolved if c.statement_id not in returned_ids]

            # store raw for first attempt only to keep size small
            if attempt == 1:
                batch_ctx.raw_http = raw_text  # type: ignore[attr-defined]

            # Include the original `size_requested` denominator so users can easily
            # spot discrepancies across batches/permutations (helpful when regex
            # pre-filtering removes items before the LLM call).
            logging.info(
                f"Batch {batch_ctx.batch_id} Q{batch_ctx.hop_idx:02}: attempt {attempt} succeeded for {len(valid_objs)}/{size_requested} objects; still missing {len(unresolved)}/{size_requested}"
            )

        except Exception as e:
            # Preserve raw response for diagnostics even when parsing fails or provider errors
            if attempt == 1 and not hasattr(batch_ctx, 'raw_http'):
                # Best-effort capture of whatever content we have
                try:
                    batch_ctx.raw_http = raw_text  # type: ignore[attr-defined]
                except Exception:
                    batch_ctx.raw_http = f"<no response captured – {e}>"  # type: ignore[attr-defined]

            # Track consecutive None-type failures to trigger cool-down
            if isinstance(e, TypeError) and "NoneType" in str(e):
                consecutive_none += 1
            else:
                consecutive_none = 0

            logging.warning(
                f"Batch {batch_ctx.batch_id} Q{batch_ctx.hop_idx:02}: attempt {attempt} failed: {e} (missing {len(unresolved)} segments)"
            )

            # Short-circuit: after 2 consecutive None failures, signal caller
            if consecutive_none >= 2:
                batch_ctx._cooldown_required = True  # type: ignore[attr-defined]
                break

            time.sleep(BACKOFF_FACTOR * (2 ** (attempt - 1)))

    # mark any still-unresolved segments as uncertain
    for ctx in unresolved:
        collected.append({
            "segment_id": ctx.statement_id,
            "answer": "uncertain",
            "rationale": "LLM call failed after incremental retries."})

    # NEW: expose attempts used to caller
    batch_ctx.attempts_used = attempt

    return collected

# --- NEW: Batch Orchestration with Concurrency ---

def run_tot_chain_batch(
    df: pd.DataFrame,
    provider_name: str,
    trace_dir: Path,
    model: str,
    batch_size: int = 10,
    concurrency: int = 1,
    token_accumulator: dict = None,
    token_lock: threading.Lock = None,
    temperature: float = TEMPERATURE,
    shuffle_batches: bool = False,
    hop_range: Optional[list[int]] = None,
) -> List[HopContext]:
    """Process dataframe through the 12-hop chain using batching with optional concurrency.
    Returns (contexts, summary_dict).
    The summary dict aggregates high-level stats (batches, duplicates, residuals, tokens …) that
    the caller can serialize into a run-level report.
    """
    # Build HopContext objects
    contexts: List[HopContext] = [
        HopContext(
            statement_id=row["StatementID"],
            segment_text=row["Statement Text"],
            article_id=row.get("ArticleID", "")
        )
        for _, row in df.iterrows()
    ]

    # NEW: collect batches that ultimately failed after MAX_RETRIES so we can write a run-level summary later
    failed_batches: List[dict] = []
    long_retry_batches: List[dict] = []

    def _provider_factory():
        return get_provider(provider_name)

    # --- aggregated run-level counters -------------------------------------------
    # (run-level summary aggregation removed; will be handled after processing)

    def _process_batch(batch_segments: List[HopContext], hop_idx: int):
        """Process a single batch of segments for the given hop."""
        batch_id = f"batch_{hop_idx}_{uuid.uuid4().hex[:6]}"
        
        # Assign batch metadata to each segment
        for i, seg in enumerate(batch_segments):
            seg.batch_id = batch_id  # type: ignore[attr-defined]
            seg.batch_size = len(batch_segments)  # type: ignore[attr-defined] 
            seg.batch_pos = i  # type: ignore[attr-defined]

        # Clear hop tracking flags
        for seg in batch_segments:
            seg._resolved_by_regex_this_hop = False  # type: ignore[attr-defined]
            seg._resolved_by_llm_this_hop = False  # type: ignore[attr-defined]

        # Track LLM calls for token accumulator
        token_accumulator['llm_calls'] += 1
        
        # Step 1: Check regex patterns for early resolution
        regex_resolved: List[HopContext] = []
        unresolved_segments: List[HopContext] = []
        regex_matches_meta: List[Dict] = []

        for seg_ctx in batch_segments:
            # Skip if already concluded from a previous hop
            if seg_ctx.is_concluded:
                continue
                
            # Query regex engine for this hop/segment combo
            seg_ctx.q_idx = hop_idx
            token_accumulator['total_hops'] += 1
            
            r_answer = None
            try:
                r_answer = _re_eng.match(seg_ctx)
            except Exception as e:
                logging.warning(f"Regex engine error on {seg_ctx.statement_id} Q{hop_idx}: {e}")
            
            if r_answer and r_answer.get("answer") == "yes":
                # Track that this segment was resolved by regex
                seg_ctx._resolved_by_regex_this_hop = True  # type: ignore[attr-defined]
                
                # Update token accumulator
                if _re_eng._FORCE_SHADOW:
                    token_accumulator['regex_hit_shadow'] += 1
                else:
                    token_accumulator['regex_yes'] += 1
                
                # Regex provided a definitive answer
                trace_entry = {
                    "Q": hop_idx,
                    "answer": r_answer["answer"],
                    "rationale": r_answer.get("rationale", "Regex match"),
                    "method": "regex",
                    "batch_id": batch_id,
                    "batch_size": seg_ctx.batch_size,
                    "batch_pos": seg_ctx.batch_pos,
                    "regex": r_answer.get("regex", {}),
                    # Include raw statement text and article ID for easier debugging across all traces
                    "statement_text": seg_ctx.segment_text,
                    "article_id": seg_ctx.article_id,
                }
                write_trace_log(trace_dir, seg_ctx.statement_id, trace_entry)
                
                seg_ctx.analysis_history.append(f"Q{hop_idx}: yes (regex)")
                seg_ctx.reasoning_trace.append(trace_entry)
                
                # Set final frame if this is a frame-determining hop and answer is yes
                if r_answer["answer"] == "yes" and hop_idx in Q_TO_FRAME:
                    seg_ctx.final_frame = r_answer.get("frame") or Q_TO_FRAME[hop_idx]
                    seg_ctx.is_concluded = True
                
                regex_resolved.append(seg_ctx)
                regex_matches_meta.append({
                    "statement_id": seg_ctx.statement_id,
                    "regex": r_answer.get("regex", {}),
                    "frame": r_answer.get("frame"),
                    "answer": r_answer["answer"],
                })
            else:
                # No definitive regex answer → send to LLM later
                unresolved_segments.append(seg_ctx)
        
        # Step 2: If any segments remain unresolved, call LLM for the batch
        # Before LLM call, dump regex matches for this batch (if any)
        if regex_matches_meta:
            import json as _json
            batch_dir = trace_dir / "batch_traces"
            batch_dir.mkdir(parents=True, exist_ok=True)
            regex_path = batch_dir / f"{batch_id}_Q{hop_idx:02}_regex.json"
            try:
                with regex_path.open("w", encoding="utf-8") as fh:
                    _json.dump({
                        "batch_id": batch_id,
                        "hop_idx": hop_idx,
                        "segment_count": len(regex_matches_meta),
                        "matches": regex_matches_meta,
                    }, fh, ensure_ascii=False, indent=2)
            except Exception as e:
                logging.warning("Could not write regex batch trace %s: %s", regex_path, e)

        if unresolved_segments:
            # Create batch context
            batch_ctx = BatchHopContext(batch_id=batch_id, hop_idx=hop_idx, segments=unresolved_segments)
            
            # Call LLM for the batch
            provider_inst = _provider_factory()

            # --------------------------------------------------
            # Cool-down logic: if _call_llm_batch marks the batch
            # with `_cooldown_required`, we wait 5 minutes and then
            # retry *once*. If the subsequent call again sets the
            # flag we repeat (max 3 cycles to avoid infinite loop).
            # --------------------------------------------------
            cooldown_cycles = 0
            while True:
                batch_responses = _call_llm_batch(batch_ctx, provider_inst, model, temperature)

                if getattr(batch_ctx, "_cooldown_required", False):
                    cooldown_cycles += 1
                    if cooldown_cycles > 3:
                        logging.warning(
                            "Batch %s Q%02d: exceeded max cool-down cycles (%s) – marking remaining %s segments uncertain",
                            batch_id,
                            hop_idx,
                            cooldown_cycles,
                            len(unresolved_segments),
                        )
                        break  # give up after marking uncertain later in code

                    # Clear flag for next attempt
                    delattr(batch_ctx, "_cooldown_required")
                    delay_sec = {1: 300, 2: 600, 3: 900}.get(cooldown_cycles, 900)
                    human_delay = f"{delay_sec//60} minute(s)"
                    logging.info(
                        "Batch %s Q%02d: entering %s cool-down (cycle %s)…",
                        batch_id,
                        hop_idx,
                        human_delay,
                        cooldown_cycles,
                    )
                    time.sleep(delay_sec)
                    provider_inst = _provider_factory()  # fresh client after wait
                    continue  # retry batch

                break  # normal path – no cool-down requested

            # NEW: record batches that consumed >3 retries
            attempts_used = getattr(batch_ctx, 'attempts_used', None)
            if attempts_used and attempts_used > 3:
                retry_stat = {
                    "batch_id": batch_id,
                    "hop_idx": hop_idx,
                    "retries_used": attempts_used,
                }
                if token_lock:
                    with token_lock:
                        long_retry_batches.append(retry_stat)
                else:
                    long_retry_batches.append(retry_stat)

            # Token accounting (prompt/response/thought)
            usage = provider_inst.get_last_usage()
            if usage and token_lock:
                with token_lock:
                    token_accumulator['prompt_tokens'] += usage.get('prompt_tokens', 0)
                    token_accumulator['response_tokens'] += usage.get('response_tokens', 0)
                    token_accumulator['thought_tokens'] += usage.get('thought_tokens', 0)
                    token_accumulator['total_tokens'] += usage.get('total_tokens', 0)
            
            # Build lookup for faster association
            sid_to_ctx = {c.statement_id: c for c in unresolved_segments}
            unknown_responses = []  # responses whose segment_id not in batch
            # Track duplicate answers for the same segment_id within this batch
            processed_sid_answers: dict[str, list[str]] = {}
            duplicates_meta: list[dict] = []

            for resp_obj in batch_responses:
                sid = str(resp_obj.get("segment_id", "")).strip()
                ctx = sid_to_ctx.get(sid)
                if ctx is None:
                    unknown_responses.append(resp_obj)
                    continue  # skip unknown ids

                # Check for duplicate answers with conflicting content
                answer_raw = str(resp_obj.get("answer", "uncertain")).lower().strip()
                prev_answers = processed_sid_answers.get(sid)
                if prev_answers is not None and answer_raw not in prev_answers:
                    logging.warning(
                        f"Duplicate responses for {sid} in batch {batch_id} Q{hop_idx}: prev='{prev_answers}' new='{answer_raw}'. Keeping last."
                    )
                    duplicates_meta.append({
                        "segment_id": sid,
                        "prev_answers": prev_answers,
                        "new_answer": answer_raw,
                    })
                # Append current answer to list (deduplicated)
                processed_sid_answers.setdefault(sid, [])
                if answer_raw not in processed_sid_answers[sid]:
                    processed_sid_answers[sid].append(answer_raw)

                answer = answer_raw
                rationale = str(resp_obj.get("rationale", "No rationale provided"))
                
                trace_entry = {
                    "Q": hop_idx,
                    "answer": answer,
                    "rationale": rationale,
                    "method": "llm_batch",
                    "batch_id": batch_id,
                    "batch_size": ctx.batch_size,
                    "batch_pos": ctx.batch_pos,
                    # Include raw statement text and article ID for easier debugging across all traces
                    "statement_text": ctx.segment_text,
                    "article_id": ctx.article_id,
                }
                write_trace_log(trace_dir, ctx.statement_id, trace_entry)
                
                ctx.analysis_history.append(f"Q{hop_idx}: {answer}")
                ctx.reasoning_trace.append(trace_entry)
                
                # Check for early termination
                if answer == "uncertain":
                    ctx.uncertain_count += 1
                    if ctx.uncertain_count >= 3:
                        logging.warning(
                            f"ToT chain terminated at Q{hop_idx} due to 3 consecutive 'uncertain' responses."
                        )
                        ctx.final_frame = "LABEL_UNCERTAIN"
                        ctx.final_justification = "Three consecutive uncertain responses"
                        continue
                
                # ──────────────────────────────────────────────────────────
                # HOP‑11 SPECIAL‑CASE — token‑guard then safe fallback
                # ──────────────────────────────────────────────────────────
                if answer == "yes" and hop_idx == 11:
                    token = re.search(r"\|\|FRAME=(Alarmist|Reassuring)\b", rationale)
                    if token:
                        ctx.final_frame = token.group(1)
                        ctx.final_justification = (
                            f"Frame determined by Q11 explicit token {token.group(0)}."
                        )
                    else:
                        logging.warning(
                            "Q11 answered 'yes' but missing ||FRAME token; "
                            "defaulting to Neutral  (SID=%s)", ctx.statement_id
                        )
                        ctx.final_frame = "Neutral"
                        ctx.final_justification = (
                            "Hop 11 'yes' without explicit token – forced Neutral."
                        )
                    ctx.is_concluded = True
                    ctx._resolved_by_llm_this_hop = True

                # All other hops use the static map
                elif answer == "yes" and hop_idx in Q_TO_FRAME:
                    ctx.final_frame = Q_TO_FRAME[hop_idx]
                    ctx.final_justification = (
                        f"Frame determined by Q{hop_idx} trigger. Rationale: {rationale}"
                    )
                    ctx.is_concluded = True
                    ctx._resolved_by_llm_this_hop = True

            # Any ctx not covered by response → mark uncertain
            for ctx in unresolved_segments:
                if ctx.statement_id not in sid_to_ctx or all(r.get("segment_id") != ctx.statement_id for r in batch_responses):
                    trace_entry = {
                        "hop_idx": hop_idx,
                        "answer": "uncertain",
                        "rationale": "Missing response from batch",
                        "method": "fallback",
                        "batch_id": batch_id,
                        "batch_size": ctx.batch_size,
                        "batch_pos": ctx.batch_pos,
                        # Include raw statement text and article ID for easier debugging across all traces
                        "statement_text": ctx.segment_text,
                        "article_id": ctx.article_id,
                    }
                    write_trace_log(trace_dir, ctx.statement_id, trace_entry)
            
            # Write batch trace
            batch_payload = {
                "batch_id": batch_id,
                "hop_idx": hop_idx,
                "segment_count": len(unresolved_segments),
                "responses": batch_responses,
                "timestamp": datetime.now().isoformat(),
            }
            write_batch_trace(trace_dir, batch_id, hop_idx, batch_payload)

            # Dump raw HTTP body if available
            raw_http_text = getattr(batch_ctx, 'raw_http', None)
            if raw_http_text:
                raw_dir = trace_dir / 'batch_traces'
                raw_dir.mkdir(parents=True, exist_ok=True)
                raw_path = raw_dir / f"{batch_id}_Q{hop_idx:02}_raw_http.txt"
                try:
                    raw_path.write_text(raw_http_text, encoding='utf-8')
                except Exception as e:
                    logging.warning('Could not write raw HTTP trace %s: %s', raw_path, e)

            # --------------------------------------------------------------
            # Retry path for any segments that failed to return a response
            # --------------------------------------------------------------
            MAX_MISS_RETRY = 3
            missing_ctxs = [c for c in unresolved_segments if c.statement_id not in sid_to_ctx or all(r.get("segment_id") != c.statement_id for r in batch_responses)]

            for retry_idx in range(1, MAX_MISS_RETRY + 1):
                if not missing_ctxs:
                    break  # all accounted for

                retry_batch_id = f"{batch_id}_r{retry_idx}"
                for ctx in missing_ctxs:
                    ctx.batch_id = retry_batch_id  # type: ignore[attr-defined]

                retry_ctx = BatchHopContext(batch_id=retry_batch_id, hop_idx=hop_idx, segments=missing_ctxs)

                provider_retry = _provider_factory()
                retry_resps = _call_llm_batch(retry_ctx, provider_retry, model, temperature)

                # token accounting
                usage_r = provider_retry.get_last_usage()
                if usage_r and token_lock:
                    with token_lock:
                        token_accumulator['prompt_tokens'] += usage_r.get('prompt_tokens', 0)
                        token_accumulator['response_tokens'] += usage_r.get('response_tokens', 0)
                        token_accumulator['thought_tokens'] += usage_r.get('thought_tokens', 0)
                        token_accumulator['total_tokens'] += usage_r.get('total_tokens', 0)

                # Map again
                sid_to_ctx_retry = {c.statement_id: c for c in missing_ctxs}
                new_missing = []

                for r_obj in retry_resps:
                    sid_r = str(r_obj.get("segment_id", "")).strip()
                    ctx_r = sid_to_ctx_retry.get(sid_r)
                    if ctx_r is None:
                        unknown_responses.append(r_obj)
                        continue
                    answer_r = str(r_obj.get("answer", "uncertain")).lower().strip()
                    rationale_r = str(r_obj.get("rationale", "No rationale provided"))

                    trace_entry_r = {
                        "Q": hop_idx,
                        "answer": answer_r,
                        "rationale": rationale_r,
                        "method": "llm_batch_retry",
                        "retry": retry_idx,
                        "batch_id": retry_batch_id,
                        "batch_size": ctx_r.batch_size,
                        "batch_pos": ctx_r.batch_pos,
                        # Include raw statement text and article ID for easier debugging across all traces
                        "statement_text": ctx_r.segment_text,
                        "article_id": ctx_r.article_id,
                    }
                    write_trace_log(trace_dir, ctx_r.statement_id, trace_entry_r)

                    ctx_r.analysis_history.append(f"Q{hop_idx}: {answer_r} (retry{retry_idx})")
                    ctx_r.reasoning_trace.append(trace_entry_r)

                    # ──────────────────────────────────────────────────────────
                    # HOP‑11 SPECIAL‑CASE — token‑guard then safe fallback (retry)
                    # ──────────────────────────────────────────────────────────
                    if answer_r == "yes" and hop_idx == 11:
                        token = re.search(r"\|\|FRAME=(Alarmist|Reassuring)\b", rationale_r)
                        if token:
                            ctx_r.final_frame = token.group(1)
                            ctx_r.final_justification = (
                                f"Frame determined by Q11 explicit token {token.group(0)}."
                            )
                        else:
                            logging.warning(
                                "Q11 answered 'yes' but missing ||FRAME token; "
                                "defaulting to Neutral  (SID=%s)", ctx_r.statement_id
                            )
                            ctx_r.final_frame = "Neutral"
                            ctx_r.final_justification = (
                                "Hop 11 'yes' without explicit token – forced Neutral."
                            )
                        ctx_r.is_concluded = True
                        ctx_r._resolved_by_llm_this_hop = True

                    # All other hops use the static map
                    elif answer_r == "yes" and hop_idx in Q_TO_FRAME:
                        ctx_r.final_frame = Q_TO_FRAME[hop_idx]
                        ctx_r.is_concluded = True
                        ctx_r._resolved_by_llm_this_hop = True
                # dump retry batch trace and raw http
                retry_payload = {
                    "batch_id": retry_batch_id,
                    "hop_idx": hop_idx,
                    "segment_count": len(missing_ctxs),
                    "responses": retry_resps,
                    "timestamp": datetime.now().isoformat(),
                }
                write_batch_trace(trace_dir, retry_batch_id, hop_idx, retry_payload)
                raw_http_retry = getattr(retry_ctx, 'raw_http', None)
                if raw_http_retry:
                    raw_path_r = trace_dir / 'batch_traces' / f"{retry_batch_id}_Q{hop_idx:02}_raw_http.txt"
                    try:
                        raw_path_r.write_text(raw_http_retry, encoding='utf-8')
                    except Exception as e:
                        logging.warning('Could not write raw HTTP retry trace %s: %s', raw_path_r, e)

                # determine still missing
                missing_ctxs = [c for c in missing_ctxs if not any(resp.get('segment_id') == c.statement_id for resp in retry_resps)]

            # If still missing after retries, they'll retain fallback uncertain entry set earlier

            # Dump residual unknown responses if any
            if unknown_responses:
                import json as _json
                residual_path = trace_dir / "batch_traces" / f"{batch_id}_Q{hop_idx:02}_residual.json"
                try:
                    with residual_path.open("w", encoding="utf-8") as fh:
                        _json.dump({
                            "batch_id": batch_id,
                            "hop_idx": hop_idx,
                            "unknown_count": len(unknown_responses),
                            "unknown_responses": unknown_responses,
                        }, fh, ensure_ascii=False, indent=2)
                except Exception as e:
                    logging.warning("Could not write residual file %s: %s", residual_path, e)

            # Dump duplicate-answer diagnostics if any were detected
            if duplicates_meta:
                import json as _json
                dup_path = trace_dir / "batch_traces" / f"{batch_id}_Q{hop_idx:02}_duplicates.json"
                try:
                    with dup_path.open("w", encoding="utf-8") as fh:
                        _json.dump({
                            "batch_id": batch_id,
                            "hop_idx": hop_idx,
                            "duplicate_count": len(duplicates_meta),
                            "duplicates": duplicates_meta,
                        }, fh, ensure_ascii=False, indent=2)
                except Exception as e:
                    logging.warning("Could not write duplicate file %s: %s", dup_path, e)

            # ------------------------------------------------------------------
            # 📝  NEW: human-readable summary file for the batch
            # ------------------------------------------------------------------
            try:
                summary_path = trace_dir / "batch_traces" / f"{batch_id}_Q{hop_idx:02}_summary.txt"

                batch_ids = [c.statement_id for c in batch_segments]
                regex_ids = [c.statement_id for c in regex_resolved]
                llm_sent_ids = [c.statement_id for c in unresolved_segments]
                returned_ids = list(processed_sid_answers.keys())
                missing_ids = [sid for sid in llm_sent_ids if sid not in returned_ids]
                residual_ids = [str(r.get("segment_id")) for r in unknown_responses]

                with summary_path.open("w", encoding="utf-8") as sf:
                    sf.write(f"Batch ID         : {batch_id}\n")
                    sf.write(f"Hop              : Q{hop_idx:02}\n")
                    sf.write(f"Total segments   : {len(batch_ids)}\n")
                    sf.write(f"Regex-resolved   : {len(regex_ids)}\n")
                    sf.write(f"Sent to LLM      : {len(llm_sent_ids)}\n")
                    sf.write(f"Returned by LLM  : {len(returned_ids)}\n")
                    sf.write(f"Missing in reply : {len(missing_ids)}\n")
                    sf.write(f"Residual (extra) : {len(residual_ids)}\n")
                    sf.write("\n=== ID LISTS ===\n")
                    def _dump(name, ids):
                        sf.write(f"\n{name} ({len(ids)}):\n")
                        for _id in ids:
                            sf.write(f"  {_id}\n")

                    _dump("Batch IDs", batch_ids)
                    _dump("Regex-resolved IDs", regex_ids)
                    _dump("Sent to LLM IDs", llm_sent_ids)
                    _dump("Returned IDs", returned_ids)
                    _dump("Missing IDs", missing_ids)
                    _dump("Residual IDs", residual_ids)
                    sf.write(f"Duplicate conflicts : {len(duplicates_meta)}\n")

                # NEW: capture batches that still have missing IDs after final retry
                if missing_ids:
                    failed_rec = {
                        "batch_id": batch_id,
                        "hop_idx": hop_idx,
                        "missing_count": len(missing_ids),
                        "missing_ids": missing_ids,
                    }
                    if token_lock:
                        with token_lock:
                            failed_batches.append(failed_rec)
                    else:
                        failed_batches.append(failed_rec)
            except Exception as e:
                logging.warning("Could not write batch summary %s: %s", summary_path, e)
        
        # Return all segments (resolved + unresolved)
        return regex_resolved + unresolved_segments

    active_contexts: List[HopContext] = contexts[:]

    _hop_iter = hop_range if hop_range else range(1, 13)
    for hop_idx in _hop_iter:
        active_contexts = [c for c in active_contexts if not c.is_concluded]
        if not active_contexts:
            break

        # Optional randomisation to spread heavy segments across batches
        if shuffle_batches:
            random.shuffle(active_contexts)

        # Build batches of current active segments
        batches: List[List[HopContext]] = [
            active_contexts[i : i + batch_size] for i in range(0, len(active_contexts), batch_size)
        ]

        logging.info(
            f"Hop {hop_idx}: processing {len(batches)} batches (size={batch_size}) with concurrency={concurrency}"
        )

        # Print START banner
        print(f"*** START Hop {hop_idx:02} → start:{len(active_contexts)} regex:0 llm:0 remain:{len(active_contexts)} ***", flush=True)

        # PHASE 1: Process ALL regex first across all segments
        regex_resolved_segments: List[HopContext] = []
        llm_pending_segments: List[HopContext] = []
        
        for seg_ctx in active_contexts:
            if seg_ctx.is_concluded:
                continue
                
            # Query regex engine for this hop/segment combo
            seg_ctx.q_idx = hop_idx
            token_accumulator['total_hops'] += 1
            
            r_answer = None
            try:
                r_answer = _re_eng.match(seg_ctx)
            except Exception as e:
                logging.warning(f"Regex engine error on {seg_ctx.statement_id} Q{hop_idx}: {e}")
            
            if r_answer and r_answer.get("answer") == "yes":
                # Track regex resolution
                seg_ctx._resolved_by_regex_this_hop = True  # type: ignore[attr-defined]
                
                # Update token accumulator
                if _re_eng._FORCE_SHADOW:
                    token_accumulator['regex_hit_shadow'] += 1
                else:
                    token_accumulator['regex_yes'] += 1
                
                # Create trace entry
                trace_entry = {
                    "Q": hop_idx,
                    "answer": r_answer["answer"],
                    "rationale": r_answer.get("rationale", "Regex match"),
                    "method": "regex",
                    "regex": r_answer.get("regex", {}),
                    "statement_text": seg_ctx.segment_text,
                    "article_id": seg_ctx.article_id,
                }
                write_trace_log(trace_dir, seg_ctx.statement_id, trace_entry)
                
                seg_ctx.analysis_history.append(f"Q{hop_idx}: yes (regex)")
                seg_ctx.reasoning_trace.append(trace_entry)
                
                # Set final frame if this is a frame-determining hop
                if hop_idx in Q_TO_FRAME:
                    seg_ctx.final_frame = r_answer.get("frame") or Q_TO_FRAME[hop_idx]
                    seg_ctx.is_concluded = True
                
                regex_resolved_segments.append(seg_ctx)
            else:
                # No regex match → needs LLM processing
                llm_pending_segments.append(seg_ctx)

        # Print REGEX banner immediately after regex processing
        regex_count = len(regex_resolved_segments)
        if regex_count > 0:
            print(f"*** REGEX HIT Hop {hop_idx:02} → regex:{regex_count} ***", flush=True)
        else:
            print(f"*** REGEX MISS Hop {hop_idx:02} ***", flush=True)

        # PHASE 2: Process LLM batches only for unresolved segments
        llm_resolved_count = 0
        
        if llm_pending_segments:
            # Build batches from LLM-pending segments only
            batches: List[List[HopContext]] = [
                llm_pending_segments[i : i + batch_size] for i in range(0, len(llm_pending_segments), batch_size)
            ]

            logging.info(
                f"Hop {hop_idx}: {regex_count} resolved by regex, processing {len(batches)} LLM batches (size={batch_size}) with concurrency={concurrency}"
            )

            def _process_llm_batch(batch_segments: List[HopContext]):
                """Process a batch for LLM only (regex already done)."""
                nonlocal llm_resolved_count
                
                batch_id = f"batch_{hop_idx}_{uuid.uuid4().hex[:6]}"
                
                # Assign batch metadata to each segment
                for i, seg in enumerate(batch_segments):
                    seg.batch_id = batch_id  # type: ignore[attr-defined]
                    seg.batch_size = len(batch_segments)  # type: ignore[attr-defined] 
                    seg.batch_pos = i  # type: ignore[attr-defined]
                    seg._resolved_by_llm_this_hop = False  # type: ignore[attr-defined]

                # Track LLM calls for token accumulator
                token_accumulator['llm_calls'] += 1

                # Create batch context (all segments need LLM)
                batch_ctx = BatchHopContext(batch_id=batch_id, hop_idx=hop_idx, segments=batch_segments)
                
                # Call LLM for the batch
                provider_inst = _provider_factory()
                batch_responses = _call_llm_batch(batch_ctx, provider_inst, model, temperature)

                # Token accounting
                usage = provider_inst.get_last_usage()
                if usage and token_lock:
                    with token_lock:
                        token_accumulator['prompt_tokens'] += usage.get('prompt_tokens', 0)
                        token_accumulator['response_tokens'] += usage.get('response_tokens', 0)
                        token_accumulator['thought_tokens'] += usage.get('thought_tokens', 0)
                        token_accumulator['total_tokens'] += usage.get('total_tokens', 0)
                
                # Process responses
                sid_to_ctx = {c.statement_id: c for c in batch_segments}
                
                for resp_obj in batch_responses:
                    sid = str(resp_obj.get("segment_id", "")).strip()
                    ctx = sid_to_ctx.get(sid)
                    if ctx is None:
                        continue
                    
                    answer = str(resp_obj.get("answer", "uncertain")).lower().strip()
                    rationale = str(resp_obj.get("rationale", "No rationale provided"))
                    
                    trace_entry = {
                        "Q": hop_idx,
                        "answer": answer,
                        "rationale": rationale,
                        "method": "llm_batch",
                        "batch_id": batch_id,
                        "batch_size": ctx.batch_size,
                        "batch_pos": ctx.batch_pos,
                        "statement_text": ctx.segment_text,
                        "article_id": ctx.article_id,
                    }
                    write_trace_log(trace_dir, ctx.statement_id, trace_entry)
                    
                    ctx.analysis_history.append(f"Q{hop_idx}: {answer}")
                    ctx.reasoning_trace.append(trace_entry)
                    
                    # Check for early termination
                    if answer == "uncertain":
                        ctx.uncertain_count += 1
                        if ctx.uncertain_count >= 3:
                            ctx.final_frame = "LABEL_UNCERTAIN"
                            ctx.final_justification = "Three consecutive uncertain responses"
                            continue
                    
                    # ──────────────────────────────────────────────────────────
                    # HOP‑11 SPECIAL‑CASE — token‑guard then safe fallback
                    # ──────────────────────────────────────────────────────────
                    if answer == "yes" and hop_idx == 11:
                        token = re.search(r"\|\|FRAME=(Alarmist|Reassuring)\b", rationale)
                        if token:
                            ctx.final_frame = token.group(1)
                            ctx.final_justification = (
                                f"Frame determined by Q11 explicit token {token.group(0)}."
                            )
                        else:
                            logging.warning(
                                "Q11 answered 'yes' but missing ||FRAME token; "
                                "defaulting to Neutral  (SID=%s)", ctx.statement_id
                            )
                            ctx.final_frame = "Neutral"
                            ctx.final_justification = (
                                "Hop 11 'yes' without explicit token – forced Neutral."
                            )
                        ctx.is_concluded = True
                        ctx._resolved_by_llm_this_hop = True
                        with token_lock:
                            llm_resolved_count += 1

                    # All other hops use the static map
                    elif answer == "yes" and hop_idx in Q_TO_FRAME:
                        ctx.final_frame = Q_TO_FRAME[hop_idx]
                        ctx.final_justification = f"Frame determined by Q{hop_idx} trigger. Rationale: {rationale}"
                        ctx.is_concluded = True
                        ctx._resolved_by_llm_this_hop = True
                        with token_lock:
                            llm_resolved_count += 1

                return batch_segments

            # Process LLM batches
        if concurrency == 1:
            for batch in batches:
                _process_llm_batch(batch)
        else:
            with ThreadPoolExecutor(max_workers=concurrency) as pool:
                futs = [pool.submit(_process_llm_batch, batch) for batch in batches]
                for fut in as_completed(futs):
                    try:
                        fut.result()
                    except Exception as exc:
                        logging.error(f"LLM batch processing error in hop {hop_idx}: {exc}")

        # Count remaining active contexts after this hop
        remaining_contexts = [c for c in active_contexts if not c.is_concluded]
        
        # Print FINISH banner
        print(f"*** FINISH Hop {hop_idx:02} → start:{len(active_contexts)} regex:{regex_count} llm:{llm_resolved_count} remain:{len(remaining_contexts)} ***", flush=True)

    # Final neutral assignment for any still-active contexts
    for ctx in contexts:
        if not ctx.is_concluded:
            ctx.final_frame = "Neutral"
            ctx.final_justification = "Default to Neutral: No specific framing cue triggered in Q1-Q12."
            ctx.is_concluded = True

    # NEW: write consolidated failure summary to main output folder
    if failed_batches:
        _fail_path = trace_dir.parent / "batch_failures.jsonl"
        try:
            import json as _json
            with _fail_path.open("w", encoding="utf-8") as _fh:
                for _rec in failed_batches:
                    _json.dump(_rec, _fh, ensure_ascii=False)
                    _fh.write("\n")
            logging.info("Batch failure summary written → %s", _fail_path)
        except Exception as _e:
            logging.warning("Could not write batch failure summary (%s): %s", _fail_path, _e)

    # NEW: write summary of batches that required >3 retries
    if long_retry_batches:
        _retry_path = trace_dir.parent / "batch_retries_over3.jsonl"
        try:
            import json as _json
            with _retry_path.open("w", encoding="utf-8") as _fh:
                for _rec in long_retry_batches:
                    _json.dump(_rec, _fh, ensure_ascii=False)
                    _fh.write("\n")
            logging.info("Long-retry batch summary written → %s", _retry_path)
        except Exception as _e:
            logging.warning("Could not write long-retry summary (%s): %s", _retry_path, _e)

    return contexts

# --- Main Entry Point for `main.py` ---

def run_coding_step_tot(config: Dict, input_csv_path: Path, output_dir: Path, limit: Optional[int] = None, start: Optional[int] = None, end: Optional[int] = None, concurrency: int = 1, model: str = "models/gemini-2.5-flash-preview-04-17", provider: str = "gemini", batch_size: int = 1, regex_mode: str = "live", shuffle_batches: bool = False, skip_eval: bool = False, only_hop: Optional[int] = None, gold_standard_file: Optional[str] = None, *, print_summary: bool = True) -> Tuple[None, Path]:
    """
    Main function to run the ToT pipeline on an input CSV and save results.
    Matches the expected return signature for a coding step in main.py.
    """
    if not _load_global_header():
        logging.critical("ToT pipeline cannot run because GLOBAL_HEADER.txt is missing or empty.")
        raise FileNotFoundError("prompts/GLOBAL_HEADER.txt is missing.")

    # --- Configure regex layer mode ---
    if regex_mode == "off":
        _re_eng.set_global_enabled(False)
        logging.info("Regex layer DISABLED via --regex-mode off")
    else:
        _re_eng.set_global_enabled(True)
        if regex_mode == "shadow":
            _re_eng.set_force_shadow(True)
            logging.info("Regex layer set to SHADOW mode: rules will not short-circuit")
        else:
            logging.info("Regex layer in LIVE mode (default)")

    df = pd.read_csv(input_csv_path, dtype={'StatementID': str})
    
    # ------------------------------------------------------------------
    # Load gold standard from separate file if provided
    # ------------------------------------------------------------------
    if gold_standard_file and not skip_eval:
        try:
            gold_standard_path = Path(gold_standard_file)
            if not gold_standard_path.exists():
                logging.error(f"Gold standard file not found: {gold_standard_file}")
                raise FileNotFoundError(f"Gold standard file not found: {gold_standard_file}")
            
            logging.info(f"Loading gold standard from: {gold_standard_file}")
            df_gold = pd.read_csv(gold_standard_path, dtype={'StatementID': str})
            
            # Verify gold standard file has required columns
            if 'StatementID' not in df_gold.columns:
                raise ValueError("Gold standard file must contain 'StatementID' column")
            if 'Gold Standard' not in df_gold.columns:
                raise ValueError("Gold standard file must contain 'Gold Standard' column")
            
            # Merge gold standard with input data based on StatementID
            df = df.merge(df_gold[['StatementID', 'Gold Standard']], on='StatementID', how='left')
            
            # Log merge statistics
            total_input = len(df)
            has_gold = df['Gold Standard'].notna().sum()
            missing_gold = total_input - has_gold
            logging.info(f"Gold standard merge: {has_gold}/{total_input} statements have gold labels, {missing_gold} missing")
            
            if missing_gold > 0:
                missing_ids = df[df['Gold Standard'].isna()]['StatementID'].tolist()
                logging.warning(f"Statements missing gold standard labels: {missing_ids[:10]}{'...' if len(missing_ids) > 10 else ''}")
                
        except Exception as e:
            logging.error(f"Error loading gold standard file: {e}")
            raise
    
    # ------------------------------------------------------------------
    # Evaluation control: if the caller explicitly requests evaluation to
    # be skipped, we *pretend* the gold column is not present to ensure
    # all downstream evaluation logic is bypassed cleanly.
    # ------------------------------------------------------------------
    if skip_eval and 'Gold Standard' in df.columns:
        df = df.drop(columns=['Gold Standard'])

    # Evaluation is enabled only when the column exists *and* skip_eval is False
    has_gold_standard = (not skip_eval) and ('Gold Standard' in df.columns)
    
    # Store original dataframe size for logging
    original_size = len(df)
    
    # Apply range filtering if start/end specified
    if start is not None or end is not None:
        # Convert to 0-based indexing for pandas
        start_idx = (start - 1) if start is not None else 0
        end_idx = end if end is not None else len(df)
        
        # Validate range
        if start_idx < 0:
            logging.warning(f"Start index {start} is less than 1, using 1 instead")
            start_idx = 0
        if end_idx > len(df):
            logging.warning(f"End index {end} exceeds dataset size {len(df)}, using {len(df)} instead")
            end_idx = len(df)
        if start_idx >= end_idx:
            logging.error(f"Invalid range: start {start} >= end {end}")
            raise ValueError(f"Start index must be less than end index")
        
        # Apply range slice
        df = df.iloc[start_idx:end_idx].copy()
        logging.info(f"Applied range filter: processing rows {start_idx + 1}-{end_idx} ({len(df)} statements from original {original_size})")
        
    # Apply limit if specified (after range filtering)
    elif limit is not None:
        df = df.head(limit)
        logging.info(f"Applied limit: processing {len(df)} statements (limited from {original_size})")
    else:
        logging.info(f"Processing all {len(df)} statements")
    
    # Select and initialize provider
    provider_name = config.get("runtime_provider", provider)  # Use runtime config if available
    
    if provider_name == "openrouter":
        llm_provider = OpenRouterProvider()
        logging.info("Initialized OpenRouter provider")
    else:
        llm_provider = GeminiProvider()
        logging.info("Initialized Gemini provider")

    results = []
    # --- Token accounting ---
    token_accumulator = {
        'prompt_tokens': 0,
        'response_tokens': 0,
        'thought_tokens': 0,
        'total_tokens': 0,
        # Regex vs LLM utilisation counters
        'total_hops': 0,
        'regex_yes': 0,   # times regex produced a definitive yes
        'regex_hit_shadow': 0,  # regex fired in shadow mode (does not short-circuit)
        'llm_calls': 0,   # times we hit the LLM
        'segments_regex_ids': set(),  # unique statement IDs resolved by regex at least once
    }
    token_lock = threading.Lock()

    trace_dir = output_dir / "traces_tot"
    trace_dir.mkdir(parents=True, exist_ok=True)
    logging.info(f"ToT trace files will be saved in: {trace_dir}")

    # ------------------------------------------------------------------
    # 🛠️  Attach file-handler so the full terminal log is captured next to
    #      the other artefacts (requested by user).
    # ------------------------------------------------------------------
    log_file_path = output_dir / "run.log"
    try:
        _fh = logging.FileHandler(log_file_path, encoding="utf-8")
        _fh.setLevel(logging.INFO)
        _fh.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logging.getLogger().addHandler(_fh)
        logging.info("File logging enabled → %s", log_file_path)
    except Exception as e:
        logging.warning("Could not attach file handler %s: %s", log_file_path, e)

    # Path for false-negative corpus (regex miss + LLM yes)
    miss_path = output_dir / "regex_miss_llm_yes.jsonl"
    global _MISS_PATH
    _MISS_PATH = miss_path  # make accessible to inner functions
    # ensure empty file
    open(miss_path, 'w', encoding='utf-8').close()

    # Path for regex *hits* that short-circuited the hop deterministically
    hit_path = output_dir / "regex_hits.jsonl"
    open(hit_path, 'w', encoding='utf-8').close()

    # Register hit logger with regex_engine so every deterministic match is captured
    try:
        import multi_coder_analysis.regex_engine as _re  # package context
    except ImportError:
        import regex_engine as _re  # standalone script

    def _log_regex_hit(payload: dict) -> None:  # noqa: D401
        # payload contains statement_id, hop, segment, rule, frame, mode, span
        try:
            with token_lock:
                with open(hit_path, 'a', encoding='utf-8') as _f:
                    _f.write(json.dumps(payload, ensure_ascii=False) + "\n")
                # Record segment-level utilisation
                token_accumulator['segments_regex_ids'].add(payload.get('statement_id'))
        except Exception as _e:
            logging.debug("Could not write regex hit log: %s", _e)

    _re.set_hit_logger(_log_regex_hit)

    # helper function to log miss safely
    def _log_regex_miss(statement_id: str, hop: int, segment: str, rationale: str, token_lock: threading.Lock, miss_path: Path):
        payload = {
            "statement_id": statement_id,
            "hop": hop,
            "segment": segment,
            "rationale": rationale,
        }
        with token_lock:
            with open(miss_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(payload, ensure_ascii=False) + "\n")

    # ------------------------------------------------------------------
    # Default counters – these will be overwritten when a gold standard
    # file is available. Initialising them prevents UnboundLocalError when
    # the evaluation branch is skipped (e.g., production runs without gold
    # labels). 2025-06-18.
    # ------------------------------------------------------------------
    initial_mismatch_count: int = 0
    fixed_by_fallback: int = 0
    final_mismatch_count: int = 0
    regex_mismatch_count: int = 0
    llm_mismatch_count: int = 0

    # --- Processing Path Selection ---
    if batch_size > 1:
        logging.info(f"Processing with batch size = {batch_size} and concurrency={concurrency}")
        hop_range = [only_hop] if only_hop else None
        final_contexts = run_tot_chain_batch(
            df,
            provider_name,
            trace_dir,
            model,
            batch_size=batch_size,
            concurrency=concurrency,
            token_accumulator=token_accumulator,
            token_lock=token_lock,
            shuffle_batches=shuffle_batches,
            hop_range=hop_range,
        )
        for ctx in final_contexts:
            final_json = {
                "StatementID": ctx.statement_id,
                "Pipeline_Result": ctx.dim1_frame,
                "Pipeline_Justification": ctx.final_justification,
                "Full_Reasoning_Trace": json.dumps(ctx.reasoning_trace)
            }
            results.append(final_json)
    else:
        # Existing single-segment path
        if concurrency == 1:
            # Disable tqdm progress bar for cleaner console output
            for _, row in tqdm(
                df.iterrows(),
                total=df.shape[0],
                desc="Processing Statements (ToT)",
                disable=True,
            ):
                final_context = run_tot_chain(row, llm_provider, trace_dir, model, token_accumulator, token_lock, TEMPERATURE)
                final_json = {
                    "StatementID": final_context.statement_id,
                    "Pipeline_Result": final_context.dim1_frame,
                    "Pipeline_Justification": final_context.final_justification,
                    "Full_Reasoning_Trace": json.dumps(final_context.reasoning_trace)
                }
                results.append(final_json)
        else:
            # Concurrent processing path as previously implemented
            logging.info(f"Using concurrent processing with {concurrency} workers")
            def process_single_statement(row_tuple):
                _, row = row_tuple
                if provider_name == "openrouter":
                    thread_provider = OpenRouterProvider()
                else:
                    thread_provider = GeminiProvider()
                final_context = run_tot_chain(row, thread_provider, trace_dir, model, token_accumulator, token_lock, TEMPERATURE)
                final_json = {
                    "StatementID": final_context.statement_id,
                    "Pipeline_Result": final_context.dim1_frame,
                    "Pipeline_Justification": final_context.final_justification,
                    "Full_Reasoning_Trace": json.dumps(final_context.reasoning_trace)
                }
                return final_json
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                future_to_row = {executor.submit(process_single_statement, row_tuple): row_tuple[1]['StatementID'] for row_tuple in df.iterrows()}
                # Disable tqdm progress bar for cleaner console output
                for future in tqdm(
                    as_completed(future_to_row),
                    total=len(future_to_row),
                    desc="Processing Statements (ToT)",
                    disable=True,
                ):
                    statement_id = future_to_row[future]
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as exc:
                        logging.error(f"Statement {statement_id} generated an exception: {exc}")
                        results.append({
                            "StatementID": statement_id,
                            "Pipeline_Result": "LABEL_UNCERTAIN",
                            "Pipeline_Justification": f"Processing failed: {exc}",
                            "Full_Reasoning_Trace": "[]"
                        })

    # Save final labels to CSV
    df_results = pd.DataFrame(results)
    # This filename should match the pattern expected by the merge step
    # Using a simple filename for now - this should be made configurable
    majority_labels_path = output_dir / f"model_labels_tot.csv"
    df_results.to_csv(majority_labels_path, index=False)
    
    # In this deterministic (VOTES=1) setup, there is no separate raw votes file.
    # The trace files serve as the detailed record.
    raw_votes_path = None

    logging.info(f"ToT processing complete. Labels saved to: {majority_labels_path}")
    
    # --- Evaluation Logic (if gold standard available) ---
    if has_gold_standard:
        # Create comparison CSV first to ensure proper alignment
        comparison_path = create_comparison_csv(df, results, output_dir)
        df_comparison = pd.read_csv(comparison_path)
        
        # Reorganize trace files by match/mismatch status
        reorganize_traces_by_match_status(trace_dir, df_comparison)
        
        # Record initial mismatch count before any fallback corrections
        initial_mismatch_count = int(df_comparison["Mismatch"].sum())

        # Base predictions/actuals — ensure they exist even when no fallback is run
        predictions = df_comparison['Pipeline_Result'].tolist()
        actuals = df_comparison['Gold_Standard'].tolist()

        # --- Mismatch attribution (regex vs. LLM) -----------------------
        seg_regex_ids = token_accumulator.get('segments_regex_ids', set())
        mismatch_ids = set(df_comparison[df_comparison["Mismatch"]].StatementID)
        regex_mismatch_count = len(seg_regex_ids & mismatch_ids)
        llm_mismatch_count = initial_mismatch_count - regex_mismatch_count

        # --- NEW: evaluate and print metrics BEFORE individual fallback ----
        predictions_pre = df_comparison['Pipeline_Result'].tolist()
        actuals_pre = df_comparison['Gold_Standard'].tolist()
        metrics_pre = calculate_metrics(predictions_pre, actuals_pre)

        print("\n🧮  Evaluation BEFORE individual fallback")
        print(f"Regex-driven mismatches : {regex_mismatch_count}")
        print(f"LLM-driven mismatches   : {llm_mismatch_count}")

        print_evaluation_report(metrics_pre, input_csv_path, output_dir, concurrency, limit, start, end)

        # --- Optional: individual fallback rerun for mismatches (batch-sensitive check) ---
        if config.get("individual_fallback", False):
            logging.info("🔄 Running individual fallback for batched mismatches …")

            # Prepare directories
            indiv_root = trace_dir / "traces_tot_individual"
            match_dir = indiv_root / "traces_tot_individual_match"
            mismatch_dir = indiv_root / "traces_tot_individual_mismatch"
            match_dir.mkdir(parents=True, exist_ok=True)
            mismatch_dir.mkdir(parents=True, exist_ok=True)

            indiv_match_entries = []
            indiv_mismatch_entries = []

            # We will update df_comparison in-place if a batch-sensitive fix occurs
            def _run_single(row_tuple):
                idx, row = row_tuple
                statement_id = row['StatementID']
                segment_text = row['Statement Text']
                gold_label = row['Gold_Standard']

                # Build minimal Series for run_tot_chain
                single_series = pd.Series({
                    'StatementID': statement_id,
                    'Statement Text': segment_text,
                    'ArticleID': row.get('ArticleID', ''),
                })

                provider_obj = OpenRouterProvider() if provider_name == "openrouter" else GeminiProvider()

                single_ctx = run_tot_chain(
                    single_series,
                    provider_obj,
                    indiv_root,
                    model,
                    token_accumulator,
                    token_lock,
                    TEMPERATURE,
                )

                single_label = single_ctx.dim1_frame

                trace_file_path = indiv_root / f"{statement_id}.jsonl"
                try:
                    with open(trace_file_path, 'r', encoding='utf-8') as tf:
                        trace_entries = [json.loads(l.strip()) for l in tf if l.strip()]
                except FileNotFoundError:
                    trace_entries = []

                entry_payload = {
                    "statement_id": statement_id,
                    "expected": gold_label,
                    "batched_result": row['Pipeline_Result'],
                    "single_result": single_label,
                    "statement_text": segment_text,
                    "trace_count": len(trace_entries),
                    "full_trace": trace_entries,
                }

                return idx, single_label, entry_payload

            mismatch_rows = list(df_comparison[df_comparison['Mismatch'] == True].iterrows())

            if mismatch_rows:
                with ThreadPoolExecutor(max_workers=concurrency) as pool:
                    futures = [pool.submit(_run_single, rt) for rt in mismatch_rows]
                    for fut in as_completed(futures):
                        idx, single_label, entry_payload = fut.result()

                        if single_label == entry_payload['expected']:
                            indiv_match_entries.append(entry_payload)
                            df_comparison.at[idx, 'Pipeline_Result'] = single_label
                            df_comparison.at[idx, 'Mismatch'] = False
                        else:
                            indiv_mismatch_entries.append(entry_payload)

            # Write consolidated files
            if indiv_match_entries:
                with open(match_dir / "consolidated_individual_match_traces.jsonl", 'w', encoding='utf-8') as f:
                    for e in indiv_match_entries:
                        f.write(json.dumps(e, ensure_ascii=False) + "\n")

            if indiv_mismatch_entries:
                with open(mismatch_dir / "consolidated_individual_mismatch_traces.jsonl", 'w', encoding='utf-8') as f:
                    for e in indiv_mismatch_entries:
                        f.write(json.dumps(e, ensure_ascii=False) + "\n")

            fixed_by_fallback = len(indiv_match_entries)
            final_mismatch_count = len(indiv_mismatch_entries)
            logging.info(f"🗂️  Individual fallback complete. Fixed: {fixed_by_fallback}, Still mismatched: {final_mismatch_count}")

            # ------------------------------------------------------------------
            # Print concise summaries for fixed and remaining mismatches
            # ------------------------------------------------------------------
            if indiv_match_entries:
                print("\n✅ Fixed mismatches (batch → single):")
                for e in indiv_match_entries:
                    print(
                        f"  • {e['statement_id']}: batched={e['batched_result']} » single={e['single_result']} (expected={e['expected']})"
                    )

                # Tally by hop where the corrected 'yes' fired
                hop_tally: dict[int, int] = {}
                for e in indiv_match_entries:
                    for tr in e.get('full_trace', []):
                        if tr.get('answer') == 'yes':
                            hop = int(tr.get('Q', 0))
                            hop_tally[hop] = hop_tally.get(hop, 0) + 1
                            break
                if hop_tally:
                    print("\n🔢  Hop tally for fixed mismatches (first YES hop):")
                    for h, cnt in sorted(hop_tally.items()):
                        print(f"    Q{h:02}: {cnt}")

            if indiv_mismatch_entries:
                print("\n❌ Still mismatched after fallback:")
                for e in indiv_mismatch_entries:
                    print(
                        f"  • {e['statement_id']}: expected={e['expected']} but got={e['single_result']}"
                    )

            # Final metrics (after any fallback) and reporting
            if has_gold_standard:
                # Recompute mismatch attribution post-fallback
                mismatch_ids_post = set(df_comparison[df_comparison["Mismatch"]].StatementID)
                regex_mismatch_post = len(seg_regex_ids & mismatch_ids_post)
                llm_mismatch_post = len(mismatch_ids_post) - regex_mismatch_post

                predictions = df_comparison['Pipeline_Result'].tolist()
                actuals = df_comparison['Gold_Standard'].tolist()
                metrics = calculate_metrics(predictions, actuals)

                print("\n🧮  Evaluation AFTER individual fallback")
                print(f"Regex-driven mismatches : {regex_mismatch_post}")
                print(f"LLM-driven mismatches   : {llm_mismatch_post}")

                print_evaluation_report(metrics, input_csv_path, output_dir, concurrency, limit, start, end)
                print(f"✍️  All evaluation data written to {comparison_path}")
                print_mismatches(df_comparison)
                print(f"\n✅ Evaluation complete. Full telemetry in {output_dir}")
        
        # If fallback was not run, set mismatch stats accordingly
        if not config.get("individual_fallback", False):
            fixed_by_fallback = 0
            final_mismatch_count = initial_mismatch_count
        
        # Calculate metrics
        metrics = calculate_metrics(predictions, actuals)
        
        # Print evaluation report
        print_evaluation_report(metrics, input_csv_path, output_dir, concurrency, limit, start, end)
        
        print(f"✍️  All evaluation data written to {comparison_path}")
        
        # Print mismatches
        print_mismatches(df_comparison)
        
        print(f"\n✅ Evaluation complete. Full telemetry in {output_dir}")
    
    # --- Token usage summary ---
    # Downgrade duplicate token usage logs to DEBUG to avoid redundant console output
    logging.debug("=== TOKEN USAGE SUMMARY ===")
    logging.debug(f"Prompt tokens   : {token_accumulator['prompt_tokens']}")
    logging.debug(f"Response tokens : {token_accumulator['response_tokens']}")
    logging.debug(f"Thought tokens  : {token_accumulator['thought_tokens']}")
    logging.debug(f"Total tokens    : {token_accumulator['total_tokens']}")
    if print_summary:
        print("\n📏 Token usage:")
        print(f"Prompt  : {token_accumulator['prompt_tokens']}")
        print(f"Response: {token_accumulator['response_tokens']}")
        print(f"Thought : {token_accumulator['thought_tokens']}")
        print(f"Total   : {token_accumulator['total_tokens']}")

    # --- Regex vs LLM usage summary ---
    # Recompute shadow-hit tally based on per-rule statistics so that all shadow
    # matches are counted even when no live rules exist (post-v2.20 change).
    stats_snapshot = _re_eng.get_rule_stats()
    rules_index_snapshot = {r.name: r for r in _re_eng.RAW_RULES}

    # In SHADOW pipeline mode we want to know **all** regex hits because
    # short-circuiting was disabled globally.  Counting only the rules whose
    # YAML mode is already "shadow" hides hits from the main "live" rules and
    # produces near-zero coverage numbers.  Instead:
    #   • if the run was invoked with --regex-mode shadow → count every rule hit
    #   • else (live/off)        → keep the original behaviour (shadow-rules only)

    if regex_mode == "shadow":
        shadow_total = sum(counter.get("hit", 0) for counter in stats_snapshot.values())
    else:
        shadow_total = sum(
            counter.get("hit", 0)
            for name, counter in stats_snapshot.items()
            if rules_index_snapshot.get(name) and rules_index_snapshot[name].mode == "shadow"
        )

    # Store aggregate hits. In SHADOW mode we switch from per-rule hit counts
    # to *unique segments* that triggered at least one regex rule so the
    # metric is not inflated when a segment matches multiple rules/hops.
    if regex_mode == "shadow":
        token_accumulator['regex_hit_shadow'] = len(token_accumulator.get('segments_regex_ids', set()))
    else:
        token_accumulator['regex_hit_shadow'] = shadow_total

    regex_yes = token_accumulator.get('regex_yes', 0)
    regex_hit_shadow = token_accumulator.get('regex_hit_shadow', 0)
    llm_calls = token_accumulator.get('llm_calls', 0)
    total_hops = token_accumulator.get('total_hops', 0)

    # Downgrade duplicate regex/LLM utilisation logs to DEBUG
    logging.debug("=== REGEX / LLM UTILISATION ===")
    logging.debug(f"Total hops          : {total_hops}")
    logging.debug(f"Regex definitive YES : {regex_yes}")
    logging.debug(f"Regex hits (shadow) : {regex_hit_shadow}")
    logging.debug(f"LLM calls made       : {llm_calls}")
    logging.debug(f"Regex coverage       : {regex_yes / total_hops:.2%}" if total_hops else "Regex coverage: n/a")

    if print_summary:
        print("\n⚡ Hybrid stats:")
        print(f"Total hops          : {total_hops}")
        print(f"Regex definitive YES: {regex_yes}")
        print(f"Regex hits (shadow) : {regex_hit_shadow}")
        print(f"LLM calls made      : {llm_calls}")
        if total_hops:
            print(f"Regex coverage      : {regex_yes / total_hops:.2%}")

        # Segment-level utilisation
        total_segments = len(df)
        segments_regex = len(token_accumulator.get('segments_regex_ids', set()))
        segments_llm = total_segments - segments_regex

        print(f"Segments total      : {total_segments}")
        print(f"Segments regex      : {segments_regex}  ({segments_regex / total_segments:.2%})")
        print(f"Segments LLM        : {segments_llm}  ({segments_llm / total_segments:.2%})")

    summary_path = output_dir / "token_usage_summary.json"
    try:
        # Convert non-serialisable objects (like sets) to serialisable forms
        _safe_token_acc = {k: (list(v) if isinstance(v, set) else v) for k, v in token_accumulator.items()}
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(_safe_token_acc, f, indent=2)
        logging.info(f"Token summary written to {summary_path}")
    except Exception as e:
        logging.error(f"Failed to write token summary: {e}")

    # --- Regex per-rule stats CSV ---
    import csv

    stats = _re_eng.get_rule_stats()
    rules_index = {r.name: r for r in _re_eng.RAW_RULES}

    stats_path = output_dir / "regex_rule_stats.csv"
    try:
        with open(stats_path, 'w', newline='', encoding='utf-8') as f:
            w = csv.writer(f)
            w.writerow(["rule", "hop", "mode", "hit", "total", "coverage"])
            for name, counter in sorted(stats.items()):
                rule = rules_index.get(name)
                hop = rule.hop if rule else "?"
                mode = rule.mode if rule else "?"
                hit = counter.get("hit", 0)
                total = counter.get("total", 0)
                cov = f"{hit/total:.2%}" if total else "0%"
                w.writerow([name, hop, mode, hit, total, cov])
        logging.info(f"Regex rule stats written to {stats_path}")
    except Exception as e:
        logging.error(f"Failed to write regex rule stats: {e}")

    # --- NEW: export full regex rule definitions (useful for debugging) ---
    import json as _json

    rules_snapshot_path = output_dir / "regex_rules_snapshot.jsonl"

    try:
        with open(rules_snapshot_path, "w", encoding="utf-8") as fp:
            for r in _re_eng.RAW_RULES:
                # yes_regex may be a compiled pattern or raw string depending on origin
                pattern_str = (
                    r.yes_regex.pattern if hasattr(r.yes_regex, "pattern") else str(r.yes_regex)
                )
                _json.dump(
                    {
                        "name": r.name,
                        "hop": r.hop,
                        "mode": r.mode,
                        "frame": r.yes_frame,
                        "pattern": pattern_str,
                    },
                    fp,
                    ensure_ascii=False,
                )
                fp.write("\n")
        logging.info(f"Regex rules snapshot written to {rules_snapshot_path}")
    except Exception as e:
        logging.error(f"Failed to export regex rules snapshot: {e}")

    # --- Run parameters summary ---
    params_summary = {
        "timestamp": datetime.now().isoformat(timespec="seconds"),
        "input_file": str(input_csv_path),
        "total_statements": len(df),
        "provider": provider_name,
        "model": model,
        "temperature": TEMPERATURE,
        "top_p": 0.1,
        "top_k": 1 if provider_name != "openrouter" else None,
        "batch_size": batch_size,
        "concurrency": concurrency,
        "individual_fallback_enabled": bool(config.get("individual_fallback", False)),
        "individual_fallback_note": "--individual-fallback flag WAS used" if config.get("individual_fallback", False) else "--individual-fallback flag NOT used",
        "token_usage": _safe_token_acc,
        "regex_yes": regex_yes,
        "regex_hit_shadow": regex_hit_shadow,
        "llm_calls": llm_calls,
        "regex_coverage": (regex_yes / total_hops) if total_hops else None,
        "initial_mismatch_count": initial_mismatch_count,
        "fixed_by_individual_fallback": fixed_by_fallback,
        "final_mismatch_count": final_mismatch_count,
        "regex_mismatch_count": regex_mismatch_count,
        "llm_mismatch_count": llm_mismatch_count,
    }
    if has_gold_standard:
        params_summary.update({
            "accuracy": metrics.get("accuracy"),
            "mismatch_count": int(df_comparison["Mismatch"].sum()),
        })

    params_file = output_dir / "run_parameters_summary.json"
    try:
        with open(params_file, "w", encoding="utf-8") as f:
            json.dump(params_summary, f, indent=2)
        logging.info(f"Run parameter summary written to {params_file}")
    except Exception as e:
        logging.error(f"Failed to write run parameters summary: {e}")

    # ------------------------------------------------------------------
    # 📊  RUN-LEVEL SUMMARY DOCUMENT
    # ------------------------------------------------------------------
    try:
        summary_lines: list[str] = []
        summary_lines.append("=== RUN SUMMARY ===")
        summary_lines.append(f"Total statements          : {len(df)}")
        summary_lines.append(f"Total hops processed      : {token_accumulator.get('total_hops', 0)}")
        summary_lines.append(f"Regex deterministic hits  : {token_accumulator.get('regex_yes', 0)}")
        summary_lines.append(f"Regex hits in shadow mode : {token_accumulator.get('regex_hit_shadow', 0)}")
        summary_lines.append(f"LLM batch calls           : {token_accumulator.get('llm_calls', 0)}")
        summary_lines.append("-- Token usage --")
        summary_lines.append(f"  Prompt tokens   : {token_accumulator.get('prompt_tokens', 0)}")
        summary_lines.append(f"  Response tokens : {token_accumulator.get('response_tokens', 0)}")
        summary_lines.append(f"  Thought tokens  : {token_accumulator.get('thought_tokens', 0)}")
        summary_lines.append(f"  Total tokens    : {token_accumulator.get('total_tokens', 0)}")

        # Aggregate batch-level numbers by reading *_summary.txt inside batch_traces
        batch_tr_dir = trace_dir / "batch_traces"
        dup_total = 0
        residual_total = 0
        missing_total = 0
        if batch_tr_dir.exists():
            for txt_path in batch_tr_dir.glob("*_summary.txt"):
                try:
                    with open(txt_path, 'r', encoding='utf-8') as fh:
                        for line in fh:
                            if line.startswith("Duplicate conflicts"):
                                dup_total += int(line.split(":", 1)[1].strip())
                            elif line.startswith("Residual (extra)"):
                                residual_total += int(line.split(":", 1)[1].strip())
                            elif line.startswith("Missing in reply"):
                                missing_total += int(line.split(":", 1)[1].strip())
                except Exception:
                    continue

        summary_lines.append("-- Batch anomalies --")
        summary_lines.append(f"  Duplicate conflicts : {dup_total}")
        summary_lines.append(f"  Residual extras     : {residual_total}")
        summary_lines.append(f"  Missing in replies  : {missing_total}")

        run_summary_path = output_dir / "run_summary.txt"
        with open(run_summary_path, 'w', encoding='utf-8') as sf:
            sf.write("\n".join(summary_lines))
        logging.info(f"Run-level summary written to: {run_summary_path}")
    except Exception as e:
        logging.warning("Could not write run-level summary: %s", e)

    # ------------------------------------------------------------------
    # 🧹  Detach file handler to avoid duplicate logs if function is called
    #      again within the same Python process.
    # ------------------------------------------------------------------
    try:
        if '_fh' in locals():
            logging.getLogger().removeHandler(_fh)
            _fh.close()
    except Exception:
        pass

    return raw_votes_path, majority_labels_path

# --- Evaluation Functions ---

def calculate_metrics(predictions: List[str], actuals: List[str]) -> Dict[str, Any]:
    """Calculate precision, recall, F1 for each frame and overall accuracy."""
    # Filter out "Unknown" predictions from evaluation (v2.16 upgrade)
    filtered_pairs = [(p, a) for p, a in zip(predictions, actuals) if p.lower() != "unknown"]
    
    if not filtered_pairs:
        # All predictions were "Unknown" - return empty metrics
        return {
            'accuracy': 0.0,
            'frame_metrics': {},
            'total_samples': len(predictions),
            'correct_samples': 0,
            'excluded_unknown': len(predictions)
        }
    
    filtered_predictions, filtered_actuals = zip(*filtered_pairs)
    
    # Get unique labels (excluding Unknown)
    all_labels = sorted(set(filtered_predictions + filtered_actuals))
    
    # Calculate per-frame metrics
    frame_metrics = {}
    for label in all_labels:
        tp = sum(1 for p, a in zip(filtered_predictions, filtered_actuals) if p == label and a == label)
        fp = sum(1 for p, a in zip(filtered_predictions, filtered_actuals) if p == label and a != label)
        fn = sum(1 for p, a in zip(filtered_predictions, filtered_actuals) if p != label and a == label)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        frame_metrics[label] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'tp': tp,
            'fp': fp,
            'fn': fn
        }
    
    # Overall accuracy (excluding Unknown predictions)
    correct = sum(1 for p, a in zip(filtered_predictions, filtered_actuals) if p == a)
    accuracy = correct / len(filtered_predictions) if len(filtered_predictions) > 0 else 0.0
    
    return {
        'accuracy': accuracy,
        'frame_metrics': frame_metrics,
        'total_samples': len(predictions),
        'correct_samples': correct,
        'excluded_unknown': len(predictions) - len(filtered_pairs)
    }

def print_evaluation_report(metrics: Dict[str, Any], input_file: Path, output_dir: Path, 
                          concurrency: int, limit: Optional[int] = None, start: Optional[int] = None, end: Optional[int] = None):
    """Print formatted evaluation report to terminal."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\n📊 Reports → {output_dir}")
    print(f"📂 Loading data from CSV: {input_file}")
    
    # Show processing range info
    if start is not None or end is not None:
        range_desc = f"rows {start if start else 1}-{end if end else 'end'}"
        print(f"✅ Loaded {metrics['total_samples']} examples ({range_desc}).")
    elif limit:
        print(f"✅ Loaded {metrics['total_samples']} examples (segments 1-{limit}).")
    else:
        print(f"✅ Loaded {metrics['total_samples']} examples.")
    
    print(f"🔄 Running evaluation with {concurrency} concurrent threads...")
    
    # Show Unknown exclusion info (v2.16 upgrade)
    excluded_count = metrics.get('excluded_unknown', 0)
    if excluded_count > 0:
        evaluated_count = metrics['total_samples'] - excluded_count
        print(f"🔍 Excluded {excluded_count} 'Unknown' labels from evaluation")
        print(f"📊 Evaluating {evaluated_count}/{metrics['total_samples']} samples")
    
    print(f"\n🎯 OVERALL ACCURACY: {metrics['accuracy']:.2%}")
    print(f"\n=== Per-Frame Precision / Recall ===")
    
    for frame, stats in metrics['frame_metrics'].items():
        if stats['tp'] + stats['fp'] + stats['fn'] == 0:
            continue  # Skip frames not present in the data
            
        p_str = f"{stats['precision']:.2%}" if stats['precision'] > 0 else "nan%"
        r_str = f"{stats['recall']:.2%}" if stats['recall'] > 0 else "0.00%"
        f1_str = f"{stats['f1']:.2%}" if stats['f1'] > 0 else "nan%"
        
        print(f"{frame:<12} P={p_str:<8} R={r_str:<8} F1={f1_str:<8} "
              f"(tp={stats['tp']}, fp={stats['fp']}, fn={stats['fn']})")

def create_comparison_csv(df_original: pd.DataFrame, results: List[Dict], 
                         output_dir: Path) -> Path:
    """Create CSV comparing gold standard to pipeline results."""
    # Convert results to DataFrame for easier merging
    df_results = pd.DataFrame(results)
    
    # Merge with original data
    df_comparison = df_original.merge(
        df_results[['StatementID', 'Pipeline_Result']], 
        on='StatementID', 
        how='inner'
    )
    
    # Rename columns for clarity
    df_comparison = df_comparison.rename(columns={
        'Gold Standard': 'Gold_Standard'
    })
    
    # Add mismatch column
    df_comparison['Mismatch'] = df_comparison['Gold_Standard'] != df_comparison['Pipeline_Result']
    
    # Save comparison CSV
    comparison_path = output_dir / "comparison_with_gold_standard.csv"
    df_comparison.to_csv(comparison_path, index=False)
    
    return comparison_path

def print_mismatches(df_comparison: pd.DataFrame):
    """Print detailed mismatch information."""
    mismatches = df_comparison[df_comparison['Mismatch'] == True]
    
    if len(mismatches) == 0:
        print(f"🎉 Perfect match! All {len(df_comparison)} statements consistent with gold standard.")
        return
    
    print(f"\n❌ INCONSISTENT STATEMENTS ({len(mismatches)}/{len(df_comparison)}):")
    print("=" * 80)
    
    for _, row in mismatches.iterrows():
        print(f"StatementID: {row['StatementID']}")
        print(f"Text: {row['Statement Text']}")
        print(f"Gold Standard: {row['Gold_Standard']}")
        print(f"Pipeline Result: {row['Pipeline_Result']}")
        print(f"Inconsistency: Expected '{row['Gold_Standard']}' but got '{row['Pipeline_Result']}'")
        print("-" * 80)

def reorganize_traces_by_match_status(trace_dir: Path, df_comparison: pd.DataFrame):
    """
    Reorganize trace files into match/mismatch subdirectories based on evaluation results.
    Also creates consolidated files for easy analysis.
    
    Args:
        trace_dir: Directory containing the original trace files
        df_comparison: DataFrame with comparison results including 'Mismatch' column
    """
    # Create subdirectories
    match_dir = trace_dir / "traces_tot_match"
    mismatch_dir = trace_dir / "traces_tot_mismatch"
    match_dir.mkdir(exist_ok=True)
    mismatch_dir.mkdir(exist_ok=True)
    
    moved_files = {"match": 0, "mismatch": 0}
    mismatch_traces = []  # For consolidation
    match_traces = []     # For consolidation
    
    for _, row in df_comparison.iterrows():
        statement_id = row['StatementID']
        is_mismatch = row['Mismatch']
        
        # Find the original trace file
        original_file = trace_dir / f"{statement_id}.jsonl"
        
        if original_file.exists():
            # Read trace entries for consolidation
            trace_entries = []
            try:
                with open(original_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            trace_entries.append(json.loads(line))
            except Exception as e:
                logging.warning(f"Error reading trace file {original_file}: {e}")
                trace_entries = []
            
            if is_mismatch:
                # Move to mismatch directory
                dest_file = mismatch_dir / f"{statement_id}.jsonl"
                shutil.move(str(original_file), str(dest_file))
                moved_files["mismatch"] += 1
                
                # Add to mismatch consolidation
                mismatch_traces.append({
                    "statement_id": statement_id,
                    "expected": row['Gold_Standard'],
                    "predicted": row['Pipeline_Result'],
                    "statement_text": row.get('Statement Text', ''),
                    "trace_count": len(trace_entries),
                    "full_trace": trace_entries
                })
            else:
                # Move to match directory
                dest_file = match_dir / f"{statement_id}.jsonl"
                shutil.move(str(original_file), str(dest_file))
                moved_files["match"] += 1
                
                # Add to match consolidation
                match_traces.append({
                    "statement_id": statement_id,
                    "expected": row['Gold_Standard'],
                    "predicted": row['Pipeline_Result'],
                    "statement_text": row.get('Statement Text', ''),
                    "trace_count": len(trace_entries),
                    "full_trace": trace_entries
                })
    
    # Create consolidated files
    if mismatch_traces:
        mismatch_consolidated_path = mismatch_dir / "consolidated_mismatch_traces.jsonl"
        with open(mismatch_consolidated_path, 'w', encoding='utf-8') as f:
            for entry in mismatch_traces:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        logging.info(f"📋 Created consolidated mismatch file: {mismatch_consolidated_path} ({len(mismatch_traces)} entries)")
    
    if match_traces:
        match_consolidated_path = match_dir / "consolidated_match_traces.jsonl"
        with open(match_consolidated_path, 'w', encoding='utf-8') as f:
            for entry in match_traces:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        logging.info(f"📋 Created consolidated match file: {match_consolidated_path} ({len(match_traces)} entries)")
    
    logging.info(f"📁 Reorganized traces: {moved_files['match']} matches → {match_dir}")
    logging.info(f"📁 Reorganized traces: {moved_files['mismatch']} mismatches → {mismatch_dir}")
    
    return moved_files

START_TIME = time.perf_counter()

# Helper to log hop progress
def _log_hop(
    hop_idx: int,
    start_active: int,
    regex_yes: int,
    llm_yes: int = 0,
    end_active: int | None = None,
):
    """Progress logger used by both legacy and pipeline code.
 
    start_active – distinct unresolved StatementIDs entering the hop
    regex_yes – number concluded by regex layer for this hop
    llm_yes   – number concluded by LLM responses for this hop
    end_active – optional: unresolved StatementIDs after this hop (if provided)
    """
    elapsed = time.perf_counter() - START_TIME
    if end_active is None:
        end_active = max(0, start_active - regex_yes - llm_yes)

    msg_plain = (
        f"Hop {hop_idx:02} → "
        f"start:{start_active:<3} "
        f"regex:{regex_yes:<2} "
        f"llm:{llm_yes:<2} "
        f"remain:{end_active:<3} "
        f"({elapsed:5.1f}s)"
    )

    # keep existing INFO log for file handlers
    logging.info(msg_plain)

    # Also emit human-readable banner with asterisks once to stdout
    try:
        print(f"*** {msg_plain} ***", flush=True)
    except Exception:
        pass

# ---------------------------------------------------------------------------
# Parsing helpers
# ---------------------------------------------------------------------------
_RANK_RE = re.compile(r"ranking\s*[:\-]\s*(.*)", re.I | re.S)

def _extract_frame_and_ranking(text: str) -> tuple[str | None, list[str] | None]:
    """Extract (top_frame, ranking_list) from LLM *text*.

    The helper supports both legacy single-answer responses and the new
    ranked-list format of the form::

        Ranking: A > B > C

    Returns
    -------
    tuple
        (top_frame, ranking_list) where *ranking_list* may be ``None`` if no
        ranking pattern is detected.
    """
    if not text:
        return None, None

    m = _RANK_RE.search(text)
    if not m:
        # fallback: legacy answer extraction (e.g., "answer: <frame>")
        m_ans = re.search(r"answer\s*[:\-]\s*([\w\s]+)", text, re.I)
        if m_ans:
            return m_ans.group(1).strip(), None
        return None, None

    seq = [s.strip(" .") for s in re.split(r"[>]", m.group(1))]
    seq = [s for s in seq if s]
    top = seq[0] if seq else None
    return top, seq or None

# ---------------------------------------------
# Ranked-list parsing helpers (early definition)
# ---------------------------------------------

_RANK_RE = re.compile(r"ranking\s*[:\-]\s*(.*)", re.I | re.S)


def _extract_frame_and_ranking(text: str) -> tuple[str | None, list[str] | None]:  # noqa: D401
    """Return (top_frame, ranking list) parsed from *text*."""
    if not text:
        return None, None

    m = _RANK_RE.search(text)
    if not m:
        m_ans = re.search(r"answer\s*[:\-]\s*([\w\s]+)", text, re.I)
        return (m_ans.group(1).strip(), None) if m_ans else (None, None)

    seq = [s.strip(" .") for s in re.split(r"[>]", m.group(1)) if s.strip()]
    return (seq[0] if seq else None), (seq or None)

## 0028. multi_coder_analysis\runtime\cli.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Command-line interface entry point (Phase 7)."""

import typer
from pathlib import Path
from typing import Optional

from multi_coder_analysis.config.run_config import RunConfig
from multi_coder_analysis.runtime.tot_runner import execute
from multi_coder_analysis.providers.base import get_cost_accumulator

app = typer.Typer(help="Multi-Coder Analysis toolkit (ToT refactor)")


@app.command()
def run(
    input_csv: Path = typer.Argument(..., help="CSV file with statements"),
    output_dir: Path = typer.Argument(..., help="Directory for outputs"),
    provider: str = typer.Option("gemini", help="LLM provider: gemini|openrouter"),
    model: str = typer.Option("models/gemini-2.5-flash-preview-04-17", help="Model identifier"),
    batch_size: int = typer.Option(
        1,
        min=1,
        help="(DEPRECATED: pipeline mode is non-batching) Batch size for legacy LLM calls",
    ),
    concurrency: int = typer.Option(1, min=1, help="Thread pool size"),
    regex_mode: str = typer.Option("live", help="Regex mode: live|shadow|off"),
    shuffle_batches: bool = typer.Option(False, help="Randomise batch order"),
    phase: str = typer.Option(
        "pipeline",
        "--phase",
        help="Execution mode: legacy | pipeline",
        rich_help_panel="Execution mode",
    ),
    consensus: str = typer.Option(
        "final",
        help="Consensus mode: 'hop' for per-hop majority, 'final' for end-of-tree",
        rich_help_panel="Consensus",
    ),
    # ---- self-consistency decoding flags ----
    decode_mode: str = typer.Option(
        "normal",
        "--decode-mode",
        "-m",
        help="normal | self-consistency",
    ),
    votes: int = typer.Option(1, "--votes", "-n", help="# paths/votes for self-consistency"),
    sc_rule: str = typer.Option(
        "majority",
        "--sc-rule",
        help="majority | ranked | ranked-raw | irv | borda | mrr",
    ),
    sc_temperature: float = typer.Option(0.7, "--sc-temperature", help="Sampling temperature"),
    sc_top_k: int = typer.Option(40, "--sc-top-k", help="top-k sampling cutoff (0 disables)"),
    sc_top_p: float = typer.Option(0.95, "--sc-top-p", help="nucleus sampling p value"),
    print_cost: bool = typer.Option(
        False,
        "--print-cost",
        help="Print total USD cost when the run finishes",
        rich_help_panel="Cost",
    ),
    # ---- ranked-list flags ----
    ranked_list: bool = typer.Option(
        False,
        "--ranked-list",
        help="Prompt LLM to emit an ordered list of answers. "
              "Valid --sc-rule options in this mode: irv | borda | mrr.",
    ),
    max_candidates: int = typer.Option(
        5,
        "--max-candidates",
        help="How many candidates to retain from each ranked list (1 keeps only the top choice).",
    ),
):
    """Run the deterministic Tree-of-Thought coder."""

    cfg = RunConfig(
        input_csv=input_csv,
        output_dir=output_dir,
        provider=provider,
        model=model,
        batch_size=batch_size,
        concurrency=concurrency,
        regex_mode=regex_mode,
        shuffle_batches=shuffle_batches,
        phase=phase,
        consensus_mode=consensus,
        decode_mode=decode_mode,
        sc_votes=votes,
        sc_rule=sc_rule,
        sc_temperature=sc_temperature,
        sc_top_k=sc_top_k,
        sc_top_p=sc_top_p,
        ranked_list=ranked_list,
        max_candidates=max_candidates,
    )
    out_path = execute(cfg)
    if print_cost:
        typer.echo(f"\n💰  Run cost = ${get_cost_accumulator():.4f}\n")
    return out_path


if __name__ == "__main__":
    app() 

## 0029. multi_coder_analysis\runtime\tot_runner.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Runtime orchestrator for the Tree-of-Thought pipeline.

This thin wrapper bridges the *runtime* layer (CLI / env / I/O) with the
*core* pipeline logic implemented in :pyfunc:`multi_coder_analysis.run_multi_coder_tot`.
"""

import logging
from pathlib import Path
import threading
import sys
import json
import uuid

from multi_coder_analysis.config.run_config import RunConfig
from multi_coder_analysis import run_multi_coder_tot as tot
from multi_coder_analysis.config import load_settings
from multi_coder_analysis.core.pipeline.tot import build_tot_pipeline
from multi_coder_analysis.core.pipeline.consensus_tot import build_consensus_pipeline
from multi_coder_analysis.core.regex import Engine
from multi_coder_analysis.providers import get_provider
from multi_coder_analysis.providers.base import get_usage_accumulator
from multi_coder_analysis.models import HopContext
from multi_coder_analysis.utils.tie import is_perfect_tie
from datetime import datetime

__all__ = ["execute"]


def execute(cfg: RunConfig) -> Path:
    """Execute the ToT pipeline according to *cfg* and return the output CSV path.

    The function merges any values present in legacy `config.yaml` (loaded via
    `load_settings()`) but **explicit CLI/RunConfig values win**.
    """

    # Merge deprecated YAML settings for backwards compatibility
    legacy = load_settings().dict()
    merged_data = {**legacy, **cfg.dict(exclude_unset=True)}  # CLI overrides YAML
    cfg = RunConfig(**merged_data)

    from uuid import uuid4
    run_id_base = datetime.now().strftime("%Y%m%d_%H%M%S") + "-" + uuid4().hex[:6]
    run_id = f"{run_id_base}_{cfg.archive_tag or 'main'}"

    logging.info(
        "Starting ToT run: provider=%s, model=%s, batch=%s, concurrency=%s",
        cfg.provider,
        cfg.model,
        cfg.batch_size,
        cfg.concurrency,
    )

    cfg.output_dir.mkdir(parents=True, exist_ok=True)

    # --------------------------------------------------
    # 📂  Copy prompt folder and dump concatenated prompts (guarded by copy_prompts)
    # --------------------------------------------------
    if cfg.copy_prompts and cfg.archive_tag in (None, "main"):
        try:
            from multi_coder_analysis.concat_prompts import concatenate_prompts
            import shutil as _shutil

            # Source prompt directory – use legacy PROMPTS_DIR so behaviour matches
            _src_prompts = Path(tot.PROMPTS_DIR).resolve()
            _dst_prompts = cfg.output_dir / "prompts"

            # Copy the full folder (idempotent: dirs_exist_ok)
            _shutil.copytree(_src_prompts, _dst_prompts, dirs_exist_ok=True)
            logging.info("Copied prompt folder ➜ %s", _dst_prompts)

            # Concatenate prompts into single file in run folder
            _concat_name = "concatenated_prompts.txt"
            concatenate_prompts(_src_prompts, _concat_name, cfg.output_dir)
        except Exception as _e:
            logging.warning("Could not export prompts catalogue: %s", _e)

    # regex counters collected via Regex Engine; usage stats via providers.base.track_usage
    token_accumulator = get_usage_accumulator()
    token_lock = threading.Lock()

    if cfg.phase == "pipeline":
        provider = cfg.provider
        provider_inst = get_provider(provider)

        # --------------------------------------------------
        # Configure regex engine:** live / shadow / off **
        # --------------------------------------------------
        engine = Engine.default()
        mode = cfg.regex_mode.lower()
        if mode == "off":
            engine.set_global_enabled(False)
        elif mode == "shadow":
            engine.set_global_enabled(True)
            engine.set_force_shadow(True)
        else:  # "live" (default)
            engine.set_global_enabled(True)
            engine.set_force_shadow(False)

        # ---------- Self-consistency mode -----------
        if cfg.decode_mode == "self-consistency":
            from multi_coder_analysis.core.self_consistency import decode_paths, aggregate

            import pandas as pd

            df = pd.read_csv(cfg.input_csv, dtype={"StatementID": str})

            from concurrent.futures import ThreadPoolExecutor, as_completed

            results = []

            def _process_row(row_data):
                local_provider = get_provider(cfg.provider)
                base_ctx = HopContext(
                    statement_id=row_data["StatementID"],
                    segment_text=row_data["Statement Text"],
                    article_id=row_data.get("ArticleID", ""),
                )

                pairs = decode_paths(
                    base_ctx,
                    local_provider,
                    cfg.model,
                    votes=cfg.sc_votes,
                    temperature=cfg.sc_temperature,
                    top_k=cfg.sc_top_k,
                    top_p=cfg.sc_top_p,
                    ranked_list=cfg.ranked_list,
                    max_candidates=cfg.max_candidates,
                )

                frame, conf = aggregate(pairs, rule=cfg.sc_rule)

                # Usage already counted via @track_usage decorator

                return {
                    "StatementID": base_ctx.statement_id,
                    "Frame": frame,
                    "Consistency": f"{conf:.2f}",
                }

            with ThreadPoolExecutor(max_workers=cfg.concurrency) as exe:
                future_to_row = {
                    exe.submit(_process_row, row): row for _, row in df.iterrows()
                }

                for fut in as_completed(future_to_row):
                    results.append(fut.result())

            out_df = pd.DataFrame(results)
            out_path = cfg.output_dir / f"sc_results_{datetime.now():%Y%m%d_%H%M%S}.csv"
            out_df.to_csv(out_path, index=False)

            logging.info("Self-consistency run completed ➜ %s", out_path)

            # --- parameter summary ---
            _write_param_summary(cfg, cfg.output_dir)
            from multi_coder_analysis.runtime.tracing import TraceWriter
            TraceWriter(cfg.output_dir / "traces").write_run_summary(token_accumulator)
            return out_path

        # 'permute' mode deprecated until implemented; behaves like 'normal'

        # ---------- Normal / permute path (default) -----------

        tie_records: list = []
        decision_records: list = []

        # --------------------------------------------------------------
        # Build pipeline (consensus-aware or vanilla) only ONCE
        # --------------------------------------------------------------
        if cfg.consensus_mode == "hop":
            pipeline, hop_var = build_consensus_pipeline(
                provider_inst,
                cfg.model,
                batch_size=cfg.batch_size,
                top_k=(None if cfg.sc_top_k == 0 else cfg.sc_top_k),
                top_p=cfg.sc_top_p,
                tie_collector=tie_records,
                concurrency=cfg.concurrency,
                run_id=run_id,
                archive_dir=legacy.get("archive_dir", Path("output/archive")) if legacy.get("archive_enable", True) else Path(),
                tag=cfg.archive_tag or "main",
                ranked_list=cfg.ranked_list,
                max_candidates=cfg.max_candidates,
                decision_collector=decision_records,
            )
        else:
            pipeline = build_tot_pipeline(
                provider_inst,
                cfg.model,
                top_k=(None if cfg.sc_top_k == 0 else cfg.sc_top_k),
                top_p=cfg.sc_top_p,
                ranked_list=cfg.ranked_list,
                max_candidates=cfg.max_candidates,
            )
            hop_var = {}

        # --------------------------------------------------------------
        # Build ONE big list of contexts holding the entire dataset
        # --------------------------------------------------------------
        import pandas as pd

        df = pd.read_csv(cfg.input_csv, dtype={"StatementID": str})

        all_ctxs: list[HopContext] = []
        if cfg.consensus_mode == "hop":
            # ----------------------------------------------------------
            # Canonical eight permutations  (AB, BA, ArBr, …) identical
            # to the outer permutation suite so that hop-consensus and
            # final-consensus runs are directly comparable.
            # ----------------------------------------------------------

            mid = len(df) // 2
            A = df.iloc[:mid].copy().reset_index(drop=True)
            B = df.iloc[mid:].copy().reset_index(drop=True)

            Ar = A.iloc[::-1].copy().reset_index(drop=True)
            Br = B.iloc[::-1].copy().reset_index(drop=True)

            permuted_dfs = [
                pd.concat([A, B], ignore_index=True),      # P1_AB
                pd.concat([B, A], ignore_index=True),      # P2_BA
                pd.concat([Ar, Br], ignore_index=True),    # P3_ArBr
                pd.concat([Br, Ar], ignore_index=True),    # P4_BrAr
                pd.concat([Ar, B], ignore_index=True),     # P5_ArB
                pd.concat([Br, A], ignore_index=True),     # P6_BrA
                pd.concat([A, Br], ignore_index=True),     # P7_ABr
                pd.concat([B, Ar], ignore_index=True),     # P8_BAr
            ]

            for i, _df_perm in enumerate(permuted_dfs):
                for _, row in _df_perm.iterrows():
                    all_ctxs.append(
                        HopContext(
                            statement_id=row["StatementID"],
                            segment_text=row["Statement Text"],
                            article_id=row.get("ArticleID", ""),
                            permutation_idx=i,
                        )
                    )
        else:
            for _, row in df.iterrows():
                all_ctxs.append(
                    HopContext(
                        statement_id=row["StatementID"],
                        segment_text=row["Statement Text"],
                        article_id=row.get("ArticleID", ""),
                    )
                )

        # ---------------- Run the pipeline ONCE -----------------------
        pipeline.run(all_ctxs)

        # --------------------------------------------------------------
        # Collapse back to one representative context per StatementID
        # --------------------------------------------------------------
        from collections import defaultdict

        grouped: defaultdict[str, list[HopContext]] = defaultdict(list)
        for c in all_ctxs:
            grouped[c.statement_id].append(c)

        contexts: list[HopContext] = []
        for sid, ctx_list in grouped.items():
            # Prefer a concluded context; if multiple concluded, pick first
            rep = next((c for c in ctx_list if c.is_concluded), ctx_list[0])
            # If consensus mode, also ensure we capture tie placeholders
            if rep.final_frame is None and ctx_list and cfg.consensus_mode == "hop":
                # no concluded permutation survived (perfect tie) → mark tie
                rep.final_frame = "tie"
                rep.is_concluded = True
            contexts.append(rep)

        # Usage already counted by decorator

        # --- Persist tie traces if any ---
        if tie_records:
            traces_dir = cfg.output_dir / "traces"
            traces_dir.mkdir(parents=True, exist_ok=True)
            tie_out = traces_dir / f"tie_traces_{datetime.now():%Y%m%d_%H%M%S}.jsonl"
            with tie_out.open("w", encoding="utf-8") as f:
                for rec in tie_records:
                    json.dump(rec, f, ensure_ascii=False)
                    f.write("\n")

            logging.info("Wrote %s tie trace(s) ➜ %s", len(tie_records), tie_out)

        # Convert to DataFrame
        out_rows = [
            {
                "StatementID": c.statement_id,
                "Frame": c.final_frame,
                "Justification": c.final_justification,
            }
            for c in contexts
        ]
        out_df = pd.DataFrame(out_rows)
        out_path = cfg.output_dir / f"tot_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        out_df.to_csv(out_path, index=False)

        # Save variability logs if present
        if hop_var:
            variab_path = cfg.output_dir / f"hop_variability_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            variab_path.write_text(json.dumps(hop_var, indent=2, ensure_ascii=False))

            # also save tie segments
            ties_csv = cfg.output_dir / f"tie_segments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            import csv
            with ties_csv.open("w", newline="", encoding="utf-8") as fh:
                w = csv.writer(fh)
                w.writerow(["StatementID", "Hop", "Distribution"])
                for hop, rows_ in hop_var.items():
                    for sid, dist in rows_:
                        if is_perfect_tie(dist):
                            w.writerow([sid, hop, json.dumps(dist)])

        # ---------------- Determinative votes summary ------------------
        if decision_records:
            import pandas as _pd
            _dec_df = _pd.DataFrame(decision_records)
            _dec_df.to_csv(cfg.output_dir / f"determinative_votes_{datetime.now():%Y%m%d_%H%M%S}.csv", index=False)

        logging.info(
            "LLM stats – calls=%s prompt=%s response=%s total=%s",
            token_accumulator.get("llm_calls", 0),
            token_accumulator.get("prompt_tokens", 0),
            token_accumulator.get("response_tokens", 0),
            token_accumulator.get("total_tokens", 0),
        )

        # --- parameter summary ---
        _write_param_summary(cfg, cfg.output_dir)

        # ---------------- Segment trace export ------------------------
        try:
            import json as _json
            _trace_path = cfg.output_dir / f"segment_traces_{datetime.now():%Y%m%d_%H%M%S}.jsonl"
            with _trace_path.open("w", encoding="utf-8") as _fh:
                for _ctx in all_ctxs:
                    if not _ctx.is_concluded:
                        continue  # skip ongoing

                    _entry = {
                        "statement_id": _ctx.statement_id,
                        "statement_text": _ctx.segment_text,
                        "permutation_idx": getattr(_ctx, "permutation_idx", None),
                        "final_frame": _ctx.final_frame,
                        "hop_decision": _ctx.q_idx if hasattr(_ctx, "q_idx") else None,
                        "raw_llm_responses": _ctx.raw_llm_responses,
                        "analysis_history": _ctx.analysis_history,
                        "reasoning_trace": _ctx.reasoning_trace,
                    }
                    _fh.write(_json.dumps(_entry, ensure_ascii=False) + "\n")
            logging.info("Per-segment traces written ➜ %s", _trace_path)
        except Exception as _e:
            logging.warning("Could not write segment traces: %s", _e)

        # ---------------- Determinative-hop trace export -------------
        try:
            _det_trace_path = cfg.output_dir / f"determinative_traces_{datetime.now():%Y%m%d_%H%M%S}.jsonl"
            with _det_trace_path.open("w", encoding="utf-8") as _fh:
                for _rec in decision_records:
                    _fh.write(_json.dumps(_rec, ensure_ascii=False) + "\n")
            logging.info("Determinative hop traces written ➜ %s", _det_trace_path)
        except Exception as _e:
            logging.warning("Could not write determinative traces: %s", _e)

        return out_path

    # --- Legacy path (default) ---
    _, output_csv = tot.run_coding_step_tot(
        config={},  # legacy param kept for compatibility
        input_csv_path=cfg.input_csv,
        output_dir=cfg.output_dir,
        concurrency=cfg.concurrency,
        model=cfg.model,
        provider=cfg.provider,
        batch_size=cfg.batch_size,
        regex_mode=cfg.regex_mode,
        shuffle_batches=cfg.shuffle_batches,
        token_accumulator=token_accumulator,  # type: ignore[arg-type]
        token_lock=token_lock,  # type: ignore[arg-type]
    )

    logging.info("ToT run completed ➜ %s", output_csv)
    return Path(output_csv)


# ------------------------------------------------------------
# Helper: write comprehensive parameters summary for the run
# ------------------------------------------------------------


def _write_param_summary(cfg: RunConfig, output_dir: Path) -> None:
    """Dump command-line and full RunConfig to JSON in *output_dir*."""

    summary = {
        "timestamp": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "command_line": " ".join(sys.argv),
        "parameters": cfg.dict(),
    }

    out_file = output_dir / "parameters_summary.json"
    out_file.write_text(json.dumps(summary, indent=2, ensure_ascii=False, default=str), encoding="utf-8")

## 0030. multi_coder_analysis\runtime\tracing.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

"""Structured logging and tracing utilities (Phase 8)."""

import json
import logging
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

import structlog

__all__ = ["setup_logging", "get_logger", "TraceWriter"]

# Global run ID for this process
_RUN_ID = str(uuid.uuid4())


def setup_logging(level: str = "INFO", json_logs: bool = False) -> None:
    """Configure structured logging for the application.
    
    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR)
        json_logs: If True, emit JSON-formatted logs
    """
    
    # Configure standard library logging
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s" if not json_logs else None,
    )
    
    # Configure structlog
    processors = [
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
    ]
    
    if json_logs:
        processors.append(structlog.processors.JSONRenderer())
    else:
        processors.extend([
            structlog.dev.ConsoleRenderer(),
        ])
    
    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(
            getattr(logging, level.upper())
        ),
        logger_factory=structlog.WriteLoggerFactory(),
        cache_logger_on_first_use=True,
    )


def get_logger(name: str) -> structlog.BoundLogger:
    """Get a structured logger instance.
    
    Args:
        name: Logger name (typically __name__)
        
    Returns:
        Configured structlog logger
    """
    return structlog.get_logger(name).bind(run_id=_RUN_ID)


class TraceWriter:
    """NDJSON trace writer with envelope metadata."""
    
    def __init__(self, trace_dir: Path):
        self.trace_dir = Path(trace_dir)
        self.trace_dir.mkdir(parents=True, exist_ok=True)
        self._run_id = _RUN_ID
        
    def write_trace(self, statement_id: str, trace_data: Dict[str, Any]) -> None:
        """Write a single trace entry.
        
        Args:
            statement_id: Unique identifier for the statement
            trace_data: Trace payload data
        """
        envelope = {
            "run_id": self._run_id,
            "statement_id": statement_id,
            "timestamp": datetime.now().isoformat(),
            "trace_data": trace_data,
        }
        
        trace_file = self.trace_dir / f"{statement_id}.ndjson"
        with trace_file.open("a", encoding="utf-8") as f:
            f.write(json.dumps(envelope, ensure_ascii=False) + "\n")
    
    def write_batch_trace(self, batch_id: str, hop_idx: int, batch_data: Dict[str, Any]) -> None:
        """Write a batch trace entry.
        
        Args:
            batch_id: Unique identifier for the batch
            hop_idx: Hop number (1-12)
            batch_data: Batch processing data
        """
        envelope = {
            "run_id": self._run_id,
            "batch_id": batch_id,
            "hop_idx": hop_idx,
            "timestamp": datetime.now().isoformat(),
            "batch_data": batch_data,
        }
        
        batch_dir = self.trace_dir / "batches"
        batch_dir.mkdir(exist_ok=True)
        batch_file = batch_dir / f"{batch_id}_Q{hop_idx:02d}.ndjson"
        
        with batch_file.open("a", encoding="utf-8") as f:
            f.write(json.dumps(envelope, ensure_ascii=False) + "\n")
    
    def write_run_summary(self, summary_data: Dict[str, Any]) -> None:
        """Write a run-level summary.
        
        Args:
            summary_data: Summary statistics and metadata
        """
        envelope = {
            "run_id": self._run_id,
            "timestamp": datetime.now().isoformat(),
            "summary_data": summary_data,
        }
        
        summary_file = self.trace_dir / f"run_summary_{self._run_id}.ndjson"
        with summary_file.open("w", encoding="utf-8") as f:
            f.write(json.dumps(envelope, ensure_ascii=False) + "\n")


# Legacy compatibility adapters
def write_trace_log(trace_dir: Path, statement_id: str, trace_entry: Dict[str, Any]) -> None:
    """Legacy adapter for existing trace writing code."""
    writer = TraceWriter(trace_dir)
    writer.write_trace(statement_id, trace_entry)


def write_batch_trace(trace_dir: Path, batch_id: str, hop_idx: int, batch_payload: Dict[str, Any]) -> None:
    """Legacy adapter for existing batch trace writing code."""
    writer = TraceWriter(trace_dir)
    writer.write_batch_trace(batch_id, hop_idx, batch_payload) 

## 0031. multi_coder_analysis\utils\__init__.py
----------------------------------------------------------------------------------------------------
"""Utility sub-package.

This package exposes helper functions that are shared across the codebase.

Public re-exports
-----------------
archive_resolved
    Stream concluded :class:`~multi_coder_analysis.models.HopContext` objects to a
    worker-local JSON Lines archive file.  The function lives in
    :pymod:`multi_coder_analysis.utils.archiver` but is re-exported here for
    convenience so callers can simply write::

        from multi_coder_analysis.utils import archive_resolved

    instead of remembering the full sub-module path.
"""

from .archiver import archive_resolved  # noqa: F401 

## 0032. multi_coder_analysis\utils\archiver.py
----------------------------------------------------------------------------------------------------
from __future__ import annotations

import json, gzip
from pathlib import Path
from typing import Iterable

from multi_coder_analysis.models import HopContext

# ------------------------------------------------------------------
# Public helper – *process-local* (no cross-process locks required)
# ------------------------------------------------------------------
def archive_resolved(
    ctxs: Iterable[HopContext],
    *,
    run_id: str,
    tag: str,
    archive_dir: Path,
) -> None:
    """
    Append concluded segments to a worker-local JSONL (.gz optional) file.

    File name:  <run_id>_<tag>.jsonl[.gz]
    """
    if not archive_dir:
        return

    archive_dir.mkdir(parents=True, exist_ok=True)
    file_path = archive_dir / f"{run_id}_{tag}.jsonl"

    # Transparent compression when suffix = .gz
    opener = gzip.open if file_path.suffix == ".gz" else open   # type: ignore[assignment]

    with opener(file_path, "at", encoding="utf-8") as fh:       # type: ignore[arg-type]
        for ctx in ctxs:
            if not ctx.is_concluded:
                continue

            fh.write(
                json.dumps(
                    {
                        "statement_id": ctx.statement_id,
                        "permutation": ctx.permutation_idx,
                        "hop": ctx.q_idx,
                        "frame": ctx.final_frame,
                    },
                    ensure_ascii=False,
                )
                + "\n"
            ) 

====================================================================================================
# End of ToT pipeline snapshot — 32 files
